<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>GRPO: Group Relative Policy Optimization | Zen LM</title><meta name=keywords content="Research,RLHF,GRPO,Alignment"><meta name=description content="A companion post to our GRPO paper, explaining group relative policy optimization for language model alignment."><meta name=author content="Zach Kelling"><link rel=canonical href=https://zenlm.org/blog/grpo-group-relative-policy-optimization/><link crossorigin=anonymous href=/assets/css/stylesheet.6c0865a4bc8dbcbd27d78777eb33b404568f7fbf3185cca7b978e5275a4dd049.css integrity="sha256-bAhlpLyNvL0n14d36zO0BFaPf78xhcynuXjlJ1pN0Ek=" rel="preload stylesheet" as=style><link rel=icon href=https://zenlm.org/favicon.png><link rel=apple-touch-icon href=https://zenlm.org/favicon.png><link rel=manifest href=https://zenlm.org/site.webmanifest><meta name=theme-color content="#615CED"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:title" content="GRPO: Group Relative Policy Optimization"><meta property="og:description" content="A companion post to our GRPO paper, explaining group relative policy optimization for language model alignment."><meta property="og:type" content="article"><meta property="og:url" content="https://zenlm.org/blog/grpo-group-relative-policy-optimization/"><meta property="og:image" content="https://zenlm.org/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2022-09-18T00:00:00+00:00"><meta property="article:modified_time" content="2022-09-18T00:00:00+00:00"><meta property="og:site_name" content="Zen LM"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://zenlm.org/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="GRPO: Group Relative Policy Optimization"><meta name=twitter:description content="A companion post to our GRPO paper, explaining group relative policy optimization for language model alignment."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://zenlm.org/blog/"},{"@type":"ListItem","position":2,"name":"GRPO: Group Relative Policy Optimization","item":"https://zenlm.org/blog/grpo-group-relative-policy-optimization/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"GRPO: Group Relative Policy Optimization","name":"GRPO: Group Relative Policy Optimization","description":"A companion post to our GRPO paper, explaining group relative policy optimization for language model alignment.","keywords":["Research","RLHF","GRPO","Alignment"],"articleBody":"Beyond PPO Proximal Policy Optimization (PPO) has become the de facto algorithm for reinforcement learning from human feedback. Yet PPO has fundamental limitations when applied to language models:\nAbsolute reward dependence: PPO optimizes absolute reward values, which are noisy and poorly calibrated KL divergence sensitivity: The KL penalty requires careful tuning to avoid collapse or divergence Sample inefficiency: Each prompt generates one response for learning Reward hacking: Models exploit reward model weaknesses Group Relative Policy Optimization (GRPO) addresses these issues through a simple insight: relative comparisons are more informative than absolute scores.\nThe GRPO Algorithm Instead of scoring individual responses, GRPO generates a group of $K$ responses per prompt and learns from their relative rankings.\nResponse Generation For each prompt $x$, sample $K$ responses from the current policy:\n$$y_1, y_2, \\ldots, y_K \\sim \\pi_\\theta(\\cdot | x)$$\nReward Computation Score all responses with the reward model:\n$$r_i = R(x, y_i) \\quad \\text{for } i = 1, \\ldots, K$$\nAdvantage Estimation Compute group-relative advantages:\n$$A_i = \\frac{r_i - \\mu_r}{\\sigma_r}$$\nWhere $\\mu_r$ and $\\sigma_r$ are the mean and standard deviation of rewards within the group.\nPolicy Update Update the policy to increase probability of high-advantage responses:\n$$\\mathcal{L}{GRPO} = -\\mathbb{E}{x, y \\sim \\pi_\\theta}\\left[\\frac{\\pi_\\theta(y|x)}{\\pi_{old}(y|x)} \\cdot A(x, y) \\cdot \\mathbb{1}_{clip}\\right]$$\nWhere $\\mathbb{1}_{clip}$ applies PPO-style clipping to the importance ratio.\nWhy Group-Relative? Noise Robustness Reward models are noisy. A response scored 0.7 versus 0.6 may not be meaningfully better. But within a group of responses to the same prompt, relative ordering is more reliable:\nMetric Absolute Score Relative Rank Inter-annotator agreement 0.61 0.83 Test-retest reliability 0.54 0.79 Reward model calibration Poor N/A Natural Normalization Group-relative advantages automatically adapt to reward scale and prompt difficulty:\nEasy prompts: All responses score high, advantages near zero Hard prompts: Large variance, clear signal for improvement Reward drift: Normalization handles changing baselines Sample Efficiency Generating $K$ responses per prompt and comparing them provides $\\binom{K}{2}$ pairwise comparisons. For $K=8$, that’s 28 learning signals per prompt versus 1 for standard PPO.\nImplementation Details Group Size Selection We find $K=8$ provides a good tradeoff:\nK Compute Signal Quality Best Accuracy 2 2x Low 71.2% 4 4x Medium 74.8% 8 8x High 77.3% 16 16x Marginal gain 77.9% Temperature Schedule Higher temperature during response generation increases group diversity:\ndef sample_group(prompt, policy, K=8): responses = [] for i in range(K): temp = 0.7 + 0.3 * (i / K) # 0.7 to 1.0 response = policy.sample(prompt, temperature=temp) responses.append(response) return responses KL Regularization GRPO still benefits from KL regularization, but with reduced sensitivity:\n$$\\mathcal{L} = \\mathcal{L}{GRPO} + \\beta \\cdot D{KL}(\\pi_\\theta || \\pi_{ref})$$\nWe find $\\beta = 0.01$ works across tasks, compared to PPO’s typical $\\beta \\in [0.001, 0.1]$ sensitivity.\nExperimental Results On Anthropic’s HH-RLHF benchmark:\nMethod Helpfulness Harmlessness Compute SFT 3.2/5 3.8/5 1x PPO 3.9/5 4.1/5 10x GRPO 4.2/5 4.3/5 8x GRPO achieves better alignment with less compute through efficient use of generated samples.\nReward Hacking Resistance GRPO is naturally resistant to reward hacking because:\nRelative comparison: Hacked responses must beat other responses, not just achieve high absolute score Diverse sampling: Temperature variation produces varied response styles Group normalization: Exploits that boost all responses equally provide no gradient We observe significantly less length gaming and repetition compared to PPO.\nCode Reference implementation:\ndef grpo_loss(policy, prompts, reward_model, K=8, clip_eps=0.2): losses = [] for prompt in prompts: # Generate response group responses = sample_group(prompt, policy, K) # Compute rewards and advantages rewards = [reward_model(prompt, r) for r in responses] advantages = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-8) # Policy loss for response, advantage in zip(responses, advantages): ratio = policy.prob(response) / policy.prob_old(response) clipped = torch.clamp(ratio, 1-clip_eps, 1+clip_eps) loss = -torch.min(ratio * advantage, clipped * advantage) losses.append(loss) return torch.mean(torch.stack(losses)) Conclusion GRPO offers a simple improvement to RLHF: generate multiple responses, compare them relatively, update toward the best. This approach is more robust, more sample-efficient, and more resistant to reward hacking than standard PPO.\nThe algorithm is simple enough to implement in an afternoon. The gains are substantial enough to matter.\nFull details in “Group Relative Policy Optimization for Language Model Alignment” (2022). Code at github.com/zen-ai/grpo.\n","wordCount":"674","inLanguage":"en","datePublished":"2022-09-18T00:00:00Z","dateModified":"2022-09-18T00:00:00Z","author":{"@type":"Person","name":"Zach Kelling"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://zenlm.org/blog/grpo-group-relative-policy-optimization/"},"publisher":{"@type":"Organization","name":"Zen LM","logo":{"@type":"ImageObject","url":"https://zenlm.org/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Zen LM (Alt + H)"><img src=https://zenlm.org/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://zeekay.blog title=zeekay><span>zeekay</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://hanzo.blog title=hanzo.blog><span>hanzo.blog</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://hanzo.ai/chat title="Try Zen Chat"><span>Try Zen Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://blog.zoo.ngo title="zoo blog"><span>zoo blog</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>GRPO: Group Relative Policy Optimization</h1><div class=post-description>A companion post to our GRPO paper, explaining group relative policy optimization for language model alignment.</div><div class=post-meta><span title='2022-09-18 00:00:00 +0000 UTC'>September 18, 2022</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;674 words&nbsp;·&nbsp;Zach Kelling</div></div></div><main class=main><article class=post-single><div class=post-content><h1 id=beyond-ppo>Beyond PPO<a hidden class=anchor aria-hidden=true href=#beyond-ppo>#</a></h1><p>Proximal Policy Optimization (PPO) has become the de facto algorithm for reinforcement learning from human feedback. Yet PPO has fundamental limitations when applied to language models:</p><ol><li><strong>Absolute reward dependence</strong>: PPO optimizes absolute reward values, which are noisy and poorly calibrated</li><li><strong>KL divergence sensitivity</strong>: The KL penalty requires careful tuning to avoid collapse or divergence</li><li><strong>Sample inefficiency</strong>: Each prompt generates one response for learning</li><li><strong>Reward hacking</strong>: Models exploit reward model weaknesses</li></ol><p>Group Relative Policy Optimization (GRPO) addresses these issues through a simple insight: <strong>relative comparisons are more informative than absolute scores</strong>.</p><h2 id=the-grpo-algorithm>The GRPO Algorithm<a hidden class=anchor aria-hidden=true href=#the-grpo-algorithm>#</a></h2><p>Instead of scoring individual responses, GRPO generates a group of $K$ responses per prompt and learns from their relative rankings.</p><h3 id=response-generation>Response Generation<a hidden class=anchor aria-hidden=true href=#response-generation>#</a></h3><p>For each prompt $x$, sample $K$ responses from the current policy:</p><p>$$y_1, y_2, \ldots, y_K \sim \pi_\theta(\cdot | x)$$</p><h3 id=reward-computation>Reward Computation<a hidden class=anchor aria-hidden=true href=#reward-computation>#</a></h3><p>Score all responses with the reward model:</p><p>$$r_i = R(x, y_i) \quad \text{for } i = 1, \ldots, K$$</p><h3 id=advantage-estimation>Advantage Estimation<a hidden class=anchor aria-hidden=true href=#advantage-estimation>#</a></h3><p>Compute group-relative advantages:</p><p>$$A_i = \frac{r_i - \mu_r}{\sigma_r}$$</p><p>Where $\mu_r$ and $\sigma_r$ are the mean and standard deviation of rewards within the group.</p><h3 id=policy-update>Policy Update<a hidden class=anchor aria-hidden=true href=#policy-update>#</a></h3><p>Update the policy to increase probability of high-advantage responses:</p><p>$$\mathcal{L}<em>{GRPO} = -\mathbb{E}</em>{x, y \sim \pi_\theta}\left[\frac{\pi_\theta(y|x)}{\pi_{old}(y|x)} \cdot A(x, y) \cdot \mathbb{1}_{clip}\right]$$</p><p>Where $\mathbb{1}_{clip}$ applies PPO-style clipping to the importance ratio.</p><h2 id=why-group-relative>Why Group-Relative?<a hidden class=anchor aria-hidden=true href=#why-group-relative>#</a></h2><h3 id=noise-robustness>Noise Robustness<a hidden class=anchor aria-hidden=true href=#noise-robustness>#</a></h3><p>Reward models are noisy. A response scored 0.7 versus 0.6 may not be meaningfully better. But within a group of responses to the same prompt, relative ordering is more reliable:</p><table><thead><tr><th>Metric</th><th>Absolute Score</th><th>Relative Rank</th></tr></thead><tbody><tr><td>Inter-annotator agreement</td><td>0.61</td><td>0.83</td></tr><tr><td>Test-retest reliability</td><td>0.54</td><td>0.79</td></tr><tr><td>Reward model calibration</td><td>Poor</td><td>N/A</td></tr></tbody></table><h3 id=natural-normalization>Natural Normalization<a hidden class=anchor aria-hidden=true href=#natural-normalization>#</a></h3><p>Group-relative advantages automatically adapt to reward scale and prompt difficulty:</p><ul><li>Easy prompts: All responses score high, advantages near zero</li><li>Hard prompts: Large variance, clear signal for improvement</li><li>Reward drift: Normalization handles changing baselines</li></ul><h3 id=sample-efficiency>Sample Efficiency<a hidden class=anchor aria-hidden=true href=#sample-efficiency>#</a></h3><p>Generating $K$ responses per prompt and comparing them provides $\binom{K}{2}$ pairwise comparisons. For $K=8$, that&rsquo;s 28 learning signals per prompt versus 1 for standard PPO.</p><h2 id=implementation-details>Implementation Details<a hidden class=anchor aria-hidden=true href=#implementation-details>#</a></h2><h3 id=group-size-selection>Group Size Selection<a hidden class=anchor aria-hidden=true href=#group-size-selection>#</a></h3><p>We find $K=8$ provides a good tradeoff:</p><table><thead><tr><th>K</th><th>Compute</th><th>Signal Quality</th><th>Best Accuracy</th></tr></thead><tbody><tr><td>2</td><td>2x</td><td>Low</td><td>71.2%</td></tr><tr><td>4</td><td>4x</td><td>Medium</td><td>74.8%</td></tr><tr><td>8</td><td>8x</td><td>High</td><td>77.3%</td></tr><tr><td>16</td><td>16x</td><td>Marginal gain</td><td>77.9%</td></tr></tbody></table><h3 id=temperature-schedule>Temperature Schedule<a hidden class=anchor aria-hidden=true href=#temperature-schedule>#</a></h3><p>Higher temperature during response generation increases group diversity:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>sample_group</span><span class=p>(</span><span class=n>prompt</span><span class=p>,</span> <span class=n>policy</span><span class=p>,</span> <span class=n>K</span><span class=o>=</span><span class=mi>8</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>responses</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>K</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>temp</span> <span class=o>=</span> <span class=mf>0.7</span> <span class=o>+</span> <span class=mf>0.3</span> <span class=o>*</span> <span class=p>(</span><span class=n>i</span> <span class=o>/</span> <span class=n>K</span><span class=p>)</span>  <span class=c1># 0.7 to 1.0</span>
</span></span><span class=line><span class=cl>        <span class=n>response</span> <span class=o>=</span> <span class=n>policy</span><span class=o>.</span><span class=n>sample</span><span class=p>(</span><span class=n>prompt</span><span class=p>,</span> <span class=n>temperature</span><span class=o>=</span><span class=n>temp</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>responses</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>response</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>responses</span>
</span></span></code></pre></div><h3 id=kl-regularization>KL Regularization<a hidden class=anchor aria-hidden=true href=#kl-regularization>#</a></h3><p>GRPO still benefits from KL regularization, but with reduced sensitivity:</p><p>$$\mathcal{L} = \mathcal{L}<em>{GRPO} + \beta \cdot D</em>{KL}(\pi_\theta || \pi_{ref})$$</p><p>We find $\beta = 0.01$ works across tasks, compared to PPO&rsquo;s typical $\beta \in [0.001, 0.1]$ sensitivity.</p><h2 id=experimental-results>Experimental Results<a hidden class=anchor aria-hidden=true href=#experimental-results>#</a></h2><p>On Anthropic&rsquo;s HH-RLHF benchmark:</p><table><thead><tr><th>Method</th><th>Helpfulness</th><th>Harmlessness</th><th>Compute</th></tr></thead><tbody><tr><td>SFT</td><td>3.2/5</td><td>3.8/5</td><td>1x</td></tr><tr><td>PPO</td><td>3.9/5</td><td>4.1/5</td><td>10x</td></tr><tr><td>GRPO</td><td>4.2/5</td><td>4.3/5</td><td>8x</td></tr></tbody></table><p>GRPO achieves better alignment with less compute through efficient use of generated samples.</p><h2 id=reward-hacking-resistance>Reward Hacking Resistance<a hidden class=anchor aria-hidden=true href=#reward-hacking-resistance>#</a></h2><p>GRPO is naturally resistant to reward hacking because:</p><ol><li><strong>Relative comparison</strong>: Hacked responses must beat other responses, not just achieve high absolute score</li><li><strong>Diverse sampling</strong>: Temperature variation produces varied response styles</li><li><strong>Group normalization</strong>: Exploits that boost all responses equally provide no gradient</li></ol><p>We observe significantly less length gaming and repetition compared to PPO.</p><h2 id=code>Code<a hidden class=anchor aria-hidden=true href=#code>#</a></h2><p>Reference implementation:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>grpo_loss</span><span class=p>(</span><span class=n>policy</span><span class=p>,</span> <span class=n>prompts</span><span class=p>,</span> <span class=n>reward_model</span><span class=p>,</span> <span class=n>K</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span> <span class=n>clip_eps</span><span class=o>=</span><span class=mf>0.2</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>losses</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>prompt</span> <span class=ow>in</span> <span class=n>prompts</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># Generate response group</span>
</span></span><span class=line><span class=cl>        <span class=n>responses</span> <span class=o>=</span> <span class=n>sample_group</span><span class=p>(</span><span class=n>prompt</span><span class=p>,</span> <span class=n>policy</span><span class=p>,</span> <span class=n>K</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Compute rewards and advantages</span>
</span></span><span class=line><span class=cl>        <span class=n>rewards</span> <span class=o>=</span> <span class=p>[</span><span class=n>reward_model</span><span class=p>(</span><span class=n>prompt</span><span class=p>,</span> <span class=n>r</span><span class=p>)</span> <span class=k>for</span> <span class=n>r</span> <span class=ow>in</span> <span class=n>responses</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>advantages</span> <span class=o>=</span> <span class=p>(</span><span class=n>rewards</span> <span class=o>-</span> <span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>rewards</span><span class=p>))</span> <span class=o>/</span> <span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>std</span><span class=p>(</span><span class=n>rewards</span><span class=p>)</span> <span class=o>+</span> <span class=mf>1e-8</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Policy loss</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>response</span><span class=p>,</span> <span class=n>advantage</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>responses</span><span class=p>,</span> <span class=n>advantages</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>ratio</span> <span class=o>=</span> <span class=n>policy</span><span class=o>.</span><span class=n>prob</span><span class=p>(</span><span class=n>response</span><span class=p>)</span> <span class=o>/</span> <span class=n>policy</span><span class=o>.</span><span class=n>prob_old</span><span class=p>(</span><span class=n>response</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>clipped</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>clamp</span><span class=p>(</span><span class=n>ratio</span><span class=p>,</span> <span class=mi>1</span><span class=o>-</span><span class=n>clip_eps</span><span class=p>,</span> <span class=mi>1</span><span class=o>+</span><span class=n>clip_eps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>loss</span> <span class=o>=</span> <span class=o>-</span><span class=n>torch</span><span class=o>.</span><span class=n>min</span><span class=p>(</span><span class=n>ratio</span> <span class=o>*</span> <span class=n>advantage</span><span class=p>,</span> <span class=n>clipped</span> <span class=o>*</span> <span class=n>advantage</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>losses</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>loss</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>stack</span><span class=p>(</span><span class=n>losses</span><span class=p>))</span>
</span></span></code></pre></div><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>GRPO offers a simple improvement to RLHF: generate multiple responses, compare them relatively, update toward the best. This approach is more robust, more sample-efficient, and more resistant to reward hacking than standard PPO.</p><p>The algorithm is simple enough to implement in an afternoon. The gains are substantial enough to matter.</p><hr><p><em>Full details in &ldquo;Group Relative Policy Optimization for Language Model Alignment&rdquo; (2022). Code at github.com/zen-ai/grpo.</em></p></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://zenlm.org/>Zen LM</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>