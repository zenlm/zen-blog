1:"$Sreact.fragment"
2:I[10086,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/36bfed0236ce2cf2.js","/_next/static/chunks/e62b91212ee7f8ff.js","/_next/static/chunks/2a98816c7d26bf58.js","/_next/static/chunks/cb0a883bafeb6805.js"],""]
3:I[48068,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/36bfed0236ce2cf2.js","/_next/static/chunks/e62b91212ee7f8ff.js","/_next/static/chunks/2a98816c7d26bf58.js","/_next/static/chunks/cb0a883bafeb6805.js"],"default"]
1d:I[51504,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/36bfed0236ce2cf2.js","/_next/static/chunks/e62b91212ee7f8ff.js","/_next/static/chunks/2a98816c7d26bf58.js","/_next/static/chunks/cb0a883bafeb6805.js"],"CodeBlock"]
1e:I[51504,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/36bfed0236ce2cf2.js","/_next/static/chunks/e62b91212ee7f8ff.js","/_next/static/chunks/2a98816c7d26bf58.js","/_next/static/chunks/cb0a883bafeb6805.js"],"Pre"]
1f:I[89923,["/_next/static/chunks/4d80e004cf4896dd.js","/_next/static/chunks/350ee4303b732916.js"],"OutletBoundary"]
20:"$Sreact.suspense"
0:{"buildId":"qMVpAZcUAPsCZEMM19Q64","rsc":["$","$1","c",{"children":[["$","main",null,{"className":"mx-auto w-full max-w-2xl px-4 py-16","children":[["$","$L2",null,{"href":"/blog","className":"inline-flex items-center gap-1.5 text-sm text-fd-muted-foreground hover:text-fd-foreground mb-10 transition-colors","children":"← Back to Blog"}],["$","div",null,{"className":"mb-8","children":[["$","time",null,{"className":"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider","children":"April 1, 2024"}],["$","h1",null,{"className":"text-3xl font-bold mt-2 mb-3","children":"Qwen1.5-32B: Fitting the Capstone of the Qwen1.5 Language Model Series"}],["$","p",null,{"className":"text-fd-muted-foreground text-lg mb-4","children":"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD"}],["$","div",null,{"className":"flex items-center gap-3 pt-4 border-t border-fd-border","children":[["$","span",null,{"className":"text-sm text-fd-muted-foreground","children":["By ","Zen LM Team"]}],["$","div",null,{"className":"flex gap-1.5 ml-auto","children":[]}]]}]]}],["$","div",null,{"className":"prose dark:prose-invert max-w-none","children":[["$","p",null,{"children":[["$","$L3",null,{"href":"https://github.com/QwenLM/Qwen1.5","children":"GITHUB"}]," ",["$","$L3",null,{"href":"https://huggingface.co/Qwen","children":"HUGGING FACE"}]," ",["$","$L3",null,{"href":"https://modelscope.cn/organization/qwen","children":"MODELSCOPE"}]," ",["$","$L3",null,{"href":"https://huggingface.co/spaces/Qwen/Qwen1.5-72B-Chat","children":"DEMO"}]," ",["$","$L3",null,{"href":"https://discord.gg/yPEP2vHTu4","children":"DISCORD"}]]}],"\n",["$","h1",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"introduction","children":[["$","a",null,{"data-card":"","href":"#introduction","className":"peer","children":"Introduction"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}],"\n",["$","p",null,{"children":"The open-source community has long sought a model that strikes an ideal balance between performance, efficiency, and memory footprint. Despite the emergence of cutting-edge models like Qwen1.5-72B and DBRX, the models have faced persistent challenges such as large memory consumption, slow inference speed, and substantial finetuning costs."}],"\n",["$","p",null,{"children":"A growing consensus within the field now points to a model with approximately 30 billion parameters as the optimal “sweet spot” for achieving both strong performance and manageable resource requirements. In response to this trend, we are proud to unveil the latest additions to our Qwen1.5 language model series: Qwen1.5-32B and Qwen1.5-32B-Chat."}],"\n",["$","p",null,{"children":"Over the past months, we have meticulously developed the Qwen1.5-32B base model, striving to match or even surpass the performance benchmarks set by state-of-the-art 30B models. Simultaneously, we have made advancements in our post-training techniques, particularly in RLHF, to elevate the conversational capabilities of Qwen1.5-32B-Chat."}],"\n",["$","h1",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"model-quality","children":[["$","a",null,{"data-card":"","href":"#model-quality","className":"peer","children":"Model Quality"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":["$L4","$L5","$undefined"]}]]}],"\n","$L6","\n","$L7","\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","$L8","\n","$L9","\n","$La","\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","$Lb","\n","$Lc","\n","$Ld","\n","$Le","\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","$Lf","\n","$L10","\n","$L11","\n","$L12","\n","$L13","\n","$L14","\n","$L15","\n","$L16","\n","$L17"]}]]}],["$L18","$L19","$L1a","$L1b"],"$L1c"]}],"loading":null,"isPartial":false}
4:["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}]
5:["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}]
6:["$","p",null,{"children":"Qwen1.5-32B is a new member of the Qwen1.5 language model series, and besides model sizes, there is almost nothing different in model architecture except for the inclusion grouped query attention (GQA). Thus it has better potential of more efficient inference performance in model serving."}]
7:["$","p",null,{"children":"Here we provide the performance comparison with the SOTA of around 30B parameters or larger model sizes, in terms of the base capability evaluation, chat evaluation, and multilingual evaluation. Below, we report the evaluation of capabilities of base language models:"}]
8:["$","div",null,{"className":"relative overflow-auto prose-no-margin my-6","children":["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"Model"}],["$","th",null,{"children":"MMLU"}],["$","th",null,{"children":"C-Eval"}],["$","th",null,{"children":"GSM8K"}],["$","th",null,{"children":"MATH"}],["$","th",null,{"children":"HumanEval"}],["$","th",null,{"children":"MBPP"}],["$","th",null,{"children":"BBH"}],["$","th",null,{"children":"CMMLU"}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":"Llama2-34B"}],["$","td",null,{"children":"62.6"}],["$","td",null,{"children":"-"}],["$","td",null,{"children":"42.2"}],["$","td",null,{"children":"6.2"}],["$","td",null,{"children":"22.6"}],["$","td",null,{"children":"33.0"}],["$","td",null,{"children":"44.1"}],["$","td",null,{"children":"-"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"Yi-34B"}],["$","td",null,{"children":"76.3"}],["$","td",null,{"children":"81.4"}],["$","td",null,{"children":"67.2"}],["$","td",null,{"children":"14.4"}],["$","td",null,{"children":"23.2"}],["$","td",null,{"children":"41.0"}],["$","td",null,{"children":"54.3"}],["$","td",null,{"children":"83.7"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"Mixtral-8x7B"}],["$","td",null,{"children":"70.6"}],["$","td",null,{"children":"-"}],["$","td",null,{"children":"74.4"}],["$","td",null,{"children":"28.4"}],["$","td",null,{"children":"40.2"}],["$","td",null,{"children":"60.7"}],["$","td",null,{"children":"-"}],["$","td",null,{"children":"-"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"Qwen1.5-72B"}],["$","td",null,{"children":"77.5"}],["$","td",null,{"children":"84.1"}],["$","td",null,{"children":"79.5"}],["$","td",null,{"children":"34.1"}],["$","td",null,{"children":"41.5"}],["$","td",null,{"children":"53.4"}],["$","td",null,{"children":"65.5"}],["$","td",null,{"children":"83.5"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"Qwen1.5-32B"}],["$","td",null,{"children":"73.4"}],["$","td",null,{"children":"83.5"}],["$","td",null,{"children":"77.4"}],["$","td",null,{"children":"36.1"}],["$","td",null,{"children":"37.2"}],["$","td",null,{"children":"49.4"}],["$","td",null,{"children":"66.8"}],["$","td",null,{"children":"82.3"}]]}]]}]]}]}]
9:["$","p",null,{"children":"Our 32B model demonstrates competitive performance across a variety of tasks, including MMLU, GSM8K, HumanEval, and BBH. Compared with the 72B parameter model, Qwen1.5-32B exhibits a slight decrease in performance, yet it still outperforms other 30B models, such as Llama2-34B and Mixtral-8x7B, in most tasks."}]
a:["$","p",null,{"children":"In terms of the chat models, we follow the evaluation recipe of Qwen1.5 to test their performance on MT-Bench and Alpaca-Eval 2.0. The results are shown below:"}]
b:["$","div",null,{"className":"relative overflow-auto prose-no-margin my-6","children":["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"Models"}],["$","th",null,{"children":"MT-Bench"}],["$","th",null,{"children":"AlpacaEval 2.0"}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":"Avg. Score"}],["$","td",null,{"children":"LC Win Rate"}],["$","td",null,{}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"Qwen1.5-72B-Chat"}],["$","td",null,{"children":"8.61"}],["$","td",null,{"children":"36.60"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"Qwen1.5-32B-Chat"}],["$","td",null,{"children":"8.30"}],["$","td",null,{"children":"27.49"}]]}]]}]]}]}]
c:["$","p",null,{"children":"Significantly, Qwen1.5-32B-Chat achieves a score of over 8 points, and the gap between Qwen1.5-32B-Chat and Qwen1.5-72B-Chat is relatively small. This result indicates that the 32B model is a viable alternative for users who require a more efficient and cost-effective solution for chat applications."}]
d:["$","p",null,{"children":"We also test the multilingual capabilities of Qwen1.5-32B on a diverse set of 12 languages, including Arabic, Spanish, French, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, and Indonesian, covering domains including exams, understanding, math, and translation. Results are shown below:"}]
e:["$","p",null,{"children":"The detailed results are demonstrated below:"}]
f:["$","div",null,{"className":"relative overflow-auto prose-no-margin my-6","children":["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"Models"}],["$","th",null,{"children":"Exams"}],["$","th",null,{"children":"Understanding"}],["$","th",null,{"children":"Math"}],["$","th",null,{"children":"Translation"}],["$","th",null,{"children":"Average"}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":"Mixtral-8x7B"}],["$","td",null,{"children":"56.08"}],["$","td",null,{"children":"70.70"}],["$","td",null,{"children":"45.00"}],["$","td",null,{"children":"29.78"}],["$","td",null,{"children":"50.39"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"Qwen1.5-72B"}],["$","td",null,{"children":"66.35"}],["$","td",null,{"children":"78.16"}],["$","td",null,{"children":"61.67"}],["$","td",null,{"children":"35.57"}],["$","td",null,{"children":"60.44"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"Qwen1.5-32B"}],["$","td",null,{"children":"61.57"}],["$","td",null,{"children":"76.48"}],["$","td",null,{"children":"56.13"}],["$","td",null,{"children":"33.46"}],["$","td",null,{"children":"56.91"}]]}]]}]]}]}]
10:["$","p",null,{"children":"Similar to other Qwen1.5 models, the 32B one also has decent multiplingual capabilities and it is also slightly behind the 72B model."}]
11:["$","p",null,{"children":"Finally we come to take a look at its performance in the long-context evaluation, Needle in a Haystack. We are happy to see that it is able to achieve a top-level performance in the context of 32K tokens."}]
12:["$","h1",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"develop-with-qwen15-32b","children":[["$","a",null,{"data-card":"","href":"#develop-with-qwen15-32b","className":"peer","children":"Develop with Qwen1.5-32B"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
13:["$","p",null,{"children":["We advise you to read our blog for ",["$","$L3",null,{"href":"https://qwenlm.github.io/blog/qwen1.5/","children":"Qwen1.5"}]," to figure out the usages with Transformers, vLLM, llama.cpp, Ollama, etc."]}]
14:["$","h1",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"conclusion","children":[["$","a",null,{"data-card":"","href":"#conclusion","className":"peer","children":"Conclusion"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
15:["$","p",null,{"children":"We release the medium-size model Qwen1.5-32B as well as its chat counterpart. The models require much less memory footprint and run significantly faster than the 72B model. We hope that this release can help our users to figure out a better solution for their downstream application to tackle the problems of weak capabilities of 14B models (especially in agent playing scenarios) and high inference costs of 72B models."}]
16:["$","h1",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"citation","children":[["$","a",null,{"data-card":"","href":"#citation","className":"peer","children":"Citation"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
17:["$","$L1d",null,{"className":"shiki shiki-themes github-light github-dark","style":{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},"tabIndex":"0","icon":"<svg viewBox=\"0 0 24 24\"><path d=\"M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z\" fill=\"currentColor\" /></svg>","children":["$","$L1e",null,{"children":["$","code",null,{"children":[["$","span",null,{"className":"line","children":["$","span",null,{}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"    @misc{qwen1.5,"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"        title = {Introducing Qwen1.5},"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"        url = {https://qwenlm.github.io/blog/qwen1.5/},"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"        author = {Qwen Team},"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"        month = {February},"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"        year = {2024}"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"    }"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"    "}]}]]}]}]}]
18:["$","script","script-0",{"src":"/_next/static/chunks/36bfed0236ce2cf2.js","async":true}]
19:["$","script","script-1",{"src":"/_next/static/chunks/e62b91212ee7f8ff.js","async":true}]
1a:["$","script","script-2",{"src":"/_next/static/chunks/2a98816c7d26bf58.js","async":true}]
1b:["$","script","script-3",{"src":"/_next/static/chunks/cb0a883bafeb6805.js","async":true}]
1c:["$","$L1f",null,{"children":["$","$20",null,{"name":"Next.MetadataOutlet","children":"$@21"}]}]
21:null
