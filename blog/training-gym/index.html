<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Training Gym: A Platform for Open Model Development | Zen LM</title><meta name=keywords content="Infrastructure,Training,Open Source"><meta name=description content="Announcing Training Gym, our open platform for collaborative large model training."><meta name=author content="Zach Kelling"><link rel=canonical href=https://zenlm.org/blog/training-gym/><link crossorigin=anonymous href=/assets/css/stylesheet.6c0865a4bc8dbcbd27d78777eb33b404568f7fbf3185cca7b978e5275a4dd049.css integrity="sha256-bAhlpLyNvL0n14d36zO0BFaPf78xhcynuXjlJ1pN0Ek=" rel="preload stylesheet" as=style><link rel=icon href=https://zenlm.org/favicon.png><link rel=apple-touch-icon href=https://zenlm.org/favicon.png><link rel=manifest href=https://zenlm.org/site.webmanifest><meta name=theme-color content="#615CED"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script><meta property="og:title" content="Training Gym: A Platform for Open Model Development"><meta property="og:description" content="Announcing Training Gym, our open platform for collaborative large model training."><meta property="og:type" content="article"><meta property="og:url" content="https://zenlm.org/blog/training-gym/"><meta property="og:image" content="https://zenlm.org/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2023-09-11T10:00:00-08:00"><meta property="article:modified_time" content="2023-09-11T10:00:00-08:00"><meta property="og:site_name" content="Zen LM"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://zenlm.org/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Training Gym: A Platform for Open Model Development"><meta name=twitter:description content="Announcing Training Gym, our open platform for collaborative large model training."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://zenlm.org/blog/"},{"@type":"ListItem","position":2,"name":"Training Gym: A Platform for Open Model Development","item":"https://zenlm.org/blog/training-gym/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Training Gym: A Platform for Open Model Development","name":"Training Gym: A Platform for Open Model Development","description":"Announcing Training Gym, our open platform for collaborative large model training.","keywords":["Infrastructure","Training","Open Source"],"articleBody":"Training large language models requires more than algorithms. It requires infrastructure: distributed training frameworks, data pipelines, experiment tracking, and evaluation harnesses. Today we open source Training Gym, our complete platform for model development.\nWhy Training Gym? Open AI development faces an infrastructure gap. Publishing model weights is valuable, but it’s not enough. Researchers need:\nReproducible training pipelines Scalable distributed training Standardized evaluation Experiment management Data processing tools Training Gym provides all of this in an integrated, open source package.\nArchitecture +------------------+ +------------------+ +------------------+ | Data Pipeline | --\u003e | Training Loop | --\u003e | Evaluation | +------------------+ +------------------+ +------------------+ | | | v v v +------------------+ +------------------+ +------------------+ | Data Registry | | Checkpoint Store | | Metrics DB | +------------------+ +------------------+ +------------------+ | v +------------------+ | Experiment | | Tracker | +------------------+ Data Pipeline The data pipeline handles:\nIngestion: Load from local files, cloud storage, or streaming sources Processing: Tokenization, filtering, deduplication Mixing: Combine multiple data sources with configurable ratios Streaming: Memory-efficient data loading for large corpora from training_gym.data import DataPipeline, MixedDataset pipeline = DataPipeline( sources=[ (\"s3://data/books\", 0.3), (\"s3://data/web\", 0.5), (\"s3://data/code\", 0.2), ], tokenizer=\"zoo-labs/zen-tokenizer\", sequence_length=2048, ) dataset = pipeline.build() Distributed Training Training Gym supports multiple distributed training strategies:\nData Parallel: Simple replication across devices Tensor Parallel: Split layers across devices Pipeline Parallel: Split model stages across devices ZeRO: Memory-efficient data parallelism FSDP: Fully sharded data parallel (PyTorch native) Configuration is declarative:\ndistributed: strategy: fsdp world_size: 64 sharding_strategy: full_shard mixed_precision: bf16 gradient_checkpointing: true Training Loop The training loop is modular and extensible:\nfrom training_gym import Trainer, TrainingConfig config = TrainingConfig( model=\"zen-7b\", optimizer=\"adamw\", learning_rate=1e-4, batch_size=2048, max_steps=100000, warmup_steps=2000, weight_decay=0.1, ) trainer = Trainer(config) trainer.fit(dataset) Built-in features:\nLearning rate scheduling Gradient clipping Mixed precision training Automatic checkpointing Loss spike detection and recovery Evaluation Standardized evaluation across common benchmarks:\nfrom training_gym.eval import Evaluator evaluator = Evaluator( benchmarks=[\"mmlu\", \"hellaswag\", \"winogrande\", \"arc\"], model=model, ) results = evaluator.run() Supported benchmarks:\nMMLU (multitask language understanding) HellaSwag (commonsense reasoning) WinoGrande (coreference resolution) ARC (science questions) TruthfulQA (truthfulness) HumanEval (code generation) GSM8K (math reasoning) Experiment Tracking Every training run is tracked:\nfrom training_gym import Experiment with Experiment(\"zen-7b-v2\") as exp: exp.log_config(config) trainer.fit(dataset) exp.log_metrics(results) exp.log_artifacts([\"model.pt\", \"tokenizer/\"]) The experiment tracker records:\nHyperparameters Training metrics (loss, gradient norms, learning rates) Evaluation results System metrics (GPU utilization, memory) Artifacts (checkpoints, configs) Reproducibility Training Gym emphasizes reproducibility:\nDeterministic Training reproducibility: seed: 42 deterministic_algorithms: true cublas_workspace_config: \":4096:8\" Same seed, same results (within floating point precision).\nEnvironment Capture Every experiment records:\nGit commit hash Package versions Hardware configuration CUDA/cuDNN versions Configuration as Code All configs are versioned YAML:\n# experiments/zen-7b-v2.yaml model: architecture: llama hidden_size: 4096 num_layers: 32 num_heads: 32 vocab_size: 32000 training: batch_size: 2048 learning_rate: 3e-4 max_steps: 150000 Community Features Training Gym includes tools for collaborative development:\nModel Registry Share and discover models:\nfrom training_gym.registry import ModelRegistry registry = ModelRegistry() # Publish a model registry.push(\"my-org/my-model\", model, config) # Load a model model = registry.pull(\"zoo-labs/zen-7b\") Leaderboards Automatic benchmark submission:\nevaluator.submit_to_leaderboard( model_name=\"zen-7b-v2\", organization=\"zoo-labs\", ) Dataset Sharing from training_gym.data import DatasetRegistry # Share processed datasets DatasetRegistry.push(\"my-corpus\", dataset, license=\"cc-by-4.0\") # Load shared datasets dataset = DatasetRegistry.pull(\"zoo-labs/zen-pretrain-v1\") Getting Started Installation pip install training-gym # For distributed training pip install training-gym[distributed] # For evaluation suite pip install training-gym[eval] Quick Start from training_gym import quickstart # Train a small model to verify setup quickstart.train_tiny_model() # Run evaluation suite quickstart.evaluate_model(\"my-model\") Documentation Full documentation at docs.training-gym.ai:\nGetting started guide Architecture overview API reference Example configurations Troubleshooting Roadmap Q4 2023: Multi-modal training support Q1 2024: Reinforcement learning from human feedback (RLHF) integration Q2 2024: Federated training support Q3 2024: Automated hyperparameter optimization\nConclusion Open AI development needs open infrastructure. Training Gym provides the tools to train, evaluate, and share models. Join us in building the future of open AI.\nRepository: github.com/zoo-labs/training-gym\nZach Kelling is a co-founder of Zoo Labs Foundation.\n","wordCount":"622","inLanguage":"en","datePublished":"2023-09-11T10:00:00-08:00","dateModified":"2023-09-11T10:00:00-08:00","author":{"@type":"Person","name":"Zach Kelling"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://zenlm.org/blog/training-gym/"},"publisher":{"@type":"Organization","name":"Zen LM","logo":{"@type":"ImageObject","url":"https://zenlm.org/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Zen LM (Alt + H)"><img src=https://zenlm.org/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://zeekay.blog title=zeekay><span>zeekay</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://hanzo.blog title=hanzo.blog><span>hanzo.blog</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://hanzo.ai/chat title="Try Zen Chat"><span>Try Zen Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://blog.zoo.ngo title="zoo blog"><span>zoo blog</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Training Gym: A Platform for Open Model Development</h1><div class=post-description>Announcing Training Gym, our open platform for collaborative large model training.</div><div class=post-meta><span title='2023-09-11 10:00:00 -0800 -0800'>September 11, 2023</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;622 words&nbsp;·&nbsp;Zach Kelling</div></div></div><main class=main><article class=post-single><div class=post-content><p>Training large language models requires more than algorithms. It requires infrastructure: distributed training frameworks, data pipelines, experiment tracking, and evaluation harnesses. Today we open source Training Gym, our complete platform for model development.</p><h2 id=why-training-gym>Why Training Gym?<a hidden class=anchor aria-hidden=true href=#why-training-gym>#</a></h2><p>Open AI development faces an infrastructure gap. Publishing model weights is valuable, but it&rsquo;s not enough. Researchers need:</p><ul><li>Reproducible training pipelines</li><li>Scalable distributed training</li><li>Standardized evaluation</li><li>Experiment management</li><li>Data processing tools</li></ul><p>Training Gym provides all of this in an integrated, open source package.</p><h2 id=architecture>Architecture<a hidden class=anchor aria-hidden=true href=#architecture>#</a></h2><pre tabindex=0><code>+------------------+     +------------------+     +------------------+
|   Data Pipeline  | --&gt; |  Training Loop   | --&gt; |   Evaluation     |
+------------------+     +------------------+     +------------------+
         |                       |                        |
         v                       v                        v
+------------------+     +------------------+     +------------------+
|   Data Registry  |     | Checkpoint Store |     |   Metrics DB     |
+------------------+     +------------------+     +------------------+
                                |
                                v
                    +------------------+
                    |   Experiment     |
                    |   Tracker        |
                    +------------------+
</code></pre><h3 id=data-pipeline>Data Pipeline<a hidden class=anchor aria-hidden=true href=#data-pipeline>#</a></h3><p>The data pipeline handles:</p><ul><li><strong>Ingestion</strong>: Load from local files, cloud storage, or streaming sources</li><li><strong>Processing</strong>: Tokenization, filtering, deduplication</li><li><strong>Mixing</strong>: Combine multiple data sources with configurable ratios</li><li><strong>Streaming</strong>: Memory-efficient data loading for large corpora</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>training_gym.data</span> <span class=kn>import</span> <span class=n>DataPipeline</span><span class=p>,</span> <span class=n>MixedDataset</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>pipeline</span> <span class=o>=</span> <span class=n>DataPipeline</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>sources</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=p>(</span><span class=s2>&#34;s3://data/books&#34;</span><span class=p>,</span> <span class=mf>0.3</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=p>(</span><span class=s2>&#34;s3://data/web&#34;</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=p>(</span><span class=s2>&#34;s3://data/code&#34;</span><span class=p>,</span> <span class=mf>0.2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>tokenizer</span><span class=o>=</span><span class=s2>&#34;zoo-labs/zen-tokenizer&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>sequence_length</span><span class=o>=</span><span class=mi>2048</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>dataset</span> <span class=o>=</span> <span class=n>pipeline</span><span class=o>.</span><span class=n>build</span><span class=p>()</span>
</span></span></code></pre></div><h3 id=distributed-training>Distributed Training<a hidden class=anchor aria-hidden=true href=#distributed-training>#</a></h3><p>Training Gym supports multiple distributed training strategies:</p><ul><li><strong>Data Parallel</strong>: Simple replication across devices</li><li><strong>Tensor Parallel</strong>: Split layers across devices</li><li><strong>Pipeline Parallel</strong>: Split model stages across devices</li><li><strong>ZeRO</strong>: Memory-efficient data parallelism</li><li><strong>FSDP</strong>: Fully sharded data parallel (PyTorch native)</li></ul><p>Configuration is declarative:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>distributed</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>strategy</span><span class=p>:</span><span class=w> </span><span class=l>fsdp</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>world_size</span><span class=p>:</span><span class=w> </span><span class=m>64</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>sharding_strategy</span><span class=p>:</span><span class=w> </span><span class=l>full_shard</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>mixed_precision</span><span class=p>:</span><span class=w> </span><span class=l>bf16</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>gradient_checkpointing</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span></span></span></code></pre></div><h3 id=training-loop>Training Loop<a hidden class=anchor aria-hidden=true href=#training-loop>#</a></h3><p>The training loop is modular and extensible:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>training_gym</span> <span class=kn>import</span> <span class=n>Trainer</span><span class=p>,</span> <span class=n>TrainingConfig</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>config</span> <span class=o>=</span> <span class=n>TrainingConfig</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>=</span><span class=s2>&#34;zen-7b&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=o>=</span><span class=s2>&#34;adamw&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>learning_rate</span><span class=o>=</span><span class=mf>1e-4</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>batch_size</span><span class=o>=</span><span class=mi>2048</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_steps</span><span class=o>=</span><span class=mi>100000</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>warmup_steps</span><span class=o>=</span><span class=mi>2000</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>weight_decay</span><span class=o>=</span><span class=mf>0.1</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>trainer</span> <span class=o>=</span> <span class=n>Trainer</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>trainer</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>dataset</span><span class=p>)</span>
</span></span></code></pre></div><p>Built-in features:</p><ul><li>Learning rate scheduling</li><li>Gradient clipping</li><li>Mixed precision training</li><li>Automatic checkpointing</li><li>Loss spike detection and recovery</li></ul><h3 id=evaluation>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation>#</a></h3><p>Standardized evaluation across common benchmarks:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>training_gym.eval</span> <span class=kn>import</span> <span class=n>Evaluator</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>evaluator</span> <span class=o>=</span> <span class=n>Evaluator</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>benchmarks</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;mmlu&#34;</span><span class=p>,</span> <span class=s2>&#34;hellaswag&#34;</span><span class=p>,</span> <span class=s2>&#34;winogrande&#34;</span><span class=p>,</span> <span class=s2>&#34;arc&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>results</span> <span class=o>=</span> <span class=n>evaluator</span><span class=o>.</span><span class=n>run</span><span class=p>()</span>
</span></span></code></pre></div><p>Supported benchmarks:</p><ul><li>MMLU (multitask language understanding)</li><li>HellaSwag (commonsense reasoning)</li><li>WinoGrande (coreference resolution)</li><li>ARC (science questions)</li><li>TruthfulQA (truthfulness)</li><li>HumanEval (code generation)</li><li>GSM8K (math reasoning)</li></ul><h3 id=experiment-tracking>Experiment Tracking<a hidden class=anchor aria-hidden=true href=#experiment-tracking>#</a></h3><p>Every training run is tracked:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>training_gym</span> <span class=kn>import</span> <span class=n>Experiment</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>Experiment</span><span class=p>(</span><span class=s2>&#34;zen-7b-v2&#34;</span><span class=p>)</span> <span class=k>as</span> <span class=n>exp</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>exp</span><span class=o>.</span><span class=n>log_config</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>trainer</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>dataset</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>exp</span><span class=o>.</span><span class=n>log_metrics</span><span class=p>(</span><span class=n>results</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>exp</span><span class=o>.</span><span class=n>log_artifacts</span><span class=p>([</span><span class=s2>&#34;model.pt&#34;</span><span class=p>,</span> <span class=s2>&#34;tokenizer/&#34;</span><span class=p>])</span>
</span></span></code></pre></div><p>The experiment tracker records:</p><ul><li>Hyperparameters</li><li>Training metrics (loss, gradient norms, learning rates)</li><li>Evaluation results</li><li>System metrics (GPU utilization, memory)</li><li>Artifacts (checkpoints, configs)</li></ul><h2 id=reproducibility>Reproducibility<a hidden class=anchor aria-hidden=true href=#reproducibility>#</a></h2><p>Training Gym emphasizes reproducibility:</p><h3 id=deterministic-training>Deterministic Training<a hidden class=anchor aria-hidden=true href=#deterministic-training>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=nt>reproducibility</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>seed</span><span class=p>:</span><span class=w> </span><span class=m>42</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>deterministic_algorithms</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>cublas_workspace_config</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;:4096:8&#34;</span><span class=w>
</span></span></span></code></pre></div><p>Same seed, same results (within floating point precision).</p><h3 id=environment-capture>Environment Capture<a hidden class=anchor aria-hidden=true href=#environment-capture>#</a></h3><p>Every experiment records:</p><ul><li>Git commit hash</li><li>Package versions</li><li>Hardware configuration</li><li>CUDA/cuDNN versions</li></ul><h3 id=configuration-as-code>Configuration as Code<a hidden class=anchor aria-hidden=true href=#configuration-as-code>#</a></h3><p>All configs are versioned YAML:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yaml data-lang=yaml><span class=line><span class=cl><span class=c># experiments/zen-7b-v2.yaml</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>model</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>architecture</span><span class=p>:</span><span class=w> </span><span class=l>llama</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>hidden_size</span><span class=p>:</span><span class=w> </span><span class=m>4096</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>num_layers</span><span class=p>:</span><span class=w> </span><span class=m>32</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>num_heads</span><span class=p>:</span><span class=w> </span><span class=m>32</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>vocab_size</span><span class=p>:</span><span class=w> </span><span class=m>32000</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>training</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>batch_size</span><span class=p>:</span><span class=w> </span><span class=m>2048</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>learning_rate</span><span class=p>:</span><span class=w> </span><span class=m>3e-4</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>max_steps</span><span class=p>:</span><span class=w> </span><span class=m>150000</span><span class=w>
</span></span></span></code></pre></div><h2 id=community-features>Community Features<a hidden class=anchor aria-hidden=true href=#community-features>#</a></h2><p>Training Gym includes tools for collaborative development:</p><h3 id=model-registry>Model Registry<a hidden class=anchor aria-hidden=true href=#model-registry>#</a></h3><p>Share and discover models:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>training_gym.registry</span> <span class=kn>import</span> <span class=n>ModelRegistry</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>registry</span> <span class=o>=</span> <span class=n>ModelRegistry</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Publish a model</span>
</span></span><span class=line><span class=cl><span class=n>registry</span><span class=o>.</span><span class=n>push</span><span class=p>(</span><span class=s2>&#34;my-org/my-model&#34;</span><span class=p>,</span> <span class=n>model</span><span class=p>,</span> <span class=n>config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Load a model</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>registry</span><span class=o>.</span><span class=n>pull</span><span class=p>(</span><span class=s2>&#34;zoo-labs/zen-7b&#34;</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=leaderboards>Leaderboards<a hidden class=anchor aria-hidden=true href=#leaderboards>#</a></h3><p>Automatic benchmark submission:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>evaluator</span><span class=o>.</span><span class=n>submit_to_leaderboard</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model_name</span><span class=o>=</span><span class=s2>&#34;zen-7b-v2&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>organization</span><span class=o>=</span><span class=s2>&#34;zoo-labs&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><h3 id=dataset-sharing>Dataset Sharing<a hidden class=anchor aria-hidden=true href=#dataset-sharing>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>training_gym.data</span> <span class=kn>import</span> <span class=n>DatasetRegistry</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Share processed datasets</span>
</span></span><span class=line><span class=cl><span class=n>DatasetRegistry</span><span class=o>.</span><span class=n>push</span><span class=p>(</span><span class=s2>&#34;my-corpus&#34;</span><span class=p>,</span> <span class=n>dataset</span><span class=p>,</span> <span class=n>license</span><span class=o>=</span><span class=s2>&#34;cc-by-4.0&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Load shared datasets</span>
</span></span><span class=line><span class=cl><span class=n>dataset</span> <span class=o>=</span> <span class=n>DatasetRegistry</span><span class=o>.</span><span class=n>pull</span><span class=p>(</span><span class=s2>&#34;zoo-labs/zen-pretrain-v1&#34;</span><span class=p>)</span>
</span></span></code></pre></div><h2 id=getting-started>Getting Started<a hidden class=anchor aria-hidden=true href=#getting-started>#</a></h2><h3 id=installation>Installation<a hidden class=anchor aria-hidden=true href=#installation>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install training-gym
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># For distributed training</span>
</span></span><span class=line><span class=cl>pip install training-gym<span class=o>[</span>distributed<span class=o>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># For evaluation suite</span>
</span></span><span class=line><span class=cl>pip install training-gym<span class=o>[</span>eval<span class=o>]</span>
</span></span></code></pre></div><h3 id=quick-start>Quick Start<a hidden class=anchor aria-hidden=true href=#quick-start>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>training_gym</span> <span class=kn>import</span> <span class=n>quickstart</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Train a small model to verify setup</span>
</span></span><span class=line><span class=cl><span class=n>quickstart</span><span class=o>.</span><span class=n>train_tiny_model</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Run evaluation suite</span>
</span></span><span class=line><span class=cl><span class=n>quickstart</span><span class=o>.</span><span class=n>evaluate_model</span><span class=p>(</span><span class=s2>&#34;my-model&#34;</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=documentation>Documentation<a hidden class=anchor aria-hidden=true href=#documentation>#</a></h3><p>Full documentation at docs.training-gym.ai:</p><ul><li>Getting started guide</li><li>Architecture overview</li><li>API reference</li><li>Example configurations</li><li>Troubleshooting</li></ul><h2 id=roadmap>Roadmap<a hidden class=anchor aria-hidden=true href=#roadmap>#</a></h2><p><strong>Q4 2023</strong>: Multi-modal training support
<strong>Q1 2024</strong>: Reinforcement learning from human feedback (RLHF) integration
<strong>Q2 2024</strong>: Federated training support
<strong>Q3 2024</strong>: Automated hyperparameter optimization</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Open AI development needs open infrastructure. Training Gym provides the tools to train, evaluate, and share models. Join us in building the future of open AI.</p><p>Repository: github.com/zoo-labs/training-gym</p><hr><p><em>Zach Kelling is a co-founder of Zoo Labs Foundation.</em></p></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://zenlm.org/>Zen LM</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>