<!DOCTYPE html><!--i_dnJM_MIpJSOCQWNJVMq--><html lang="en" class="dark"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/chunks/f2332aac77592f9d.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/e83606e8fa9cc796.js"/><script src="/_next/static/chunks/36d6595f0156cd7e.js" async=""></script><script src="/_next/static/chunks/040e9cea20a8d9c7.js" async=""></script><script src="/_next/static/chunks/d4dffb5a0973d49c.js" async=""></script><script src="/_next/static/chunks/turbopack-967f27a9fd33556d.js" async=""></script><script src="/_next/static/chunks/59d0ad1b64f8544e.js" async=""></script><script src="/_next/static/chunks/4d80e004cf4896dd.js" async=""></script><script src="/_next/static/chunks/350ee4303b732916.js" async=""></script><script src="/_next/static/chunks/8de849ca74fc071f.js" async=""></script><script src="/_next/static/chunks/e62b91212ee7f8ff.js" async=""></script><script src="/_next/static/chunks/f19fe44237e54646.js" async=""></script><script src="/_next/static/chunks/cb0a883bafeb6805.js" async=""></script><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#0A0A0A"/><meta name="theme-color" media="(prefers-color-scheme: light)" content="#fff"/><title>Introducing Zen LM: Open Frontier Models from Hanzo AI and Zoo Labs — Zen LM Blog | Zen LM</title><meta name="description" content="Announcing the Zen model family: 94+ open models built on Zen MoDE architecture, co-developed by Hanzo AI and Zoo Labs Foundation."/><meta property="og:title" content="Zen LM - Open Foundation Models"/><meta property="og:description" content="Zen AI model family by Hanzo AI. 14 frontier models from 4B to 1T+ parameters for code, reasoning, vision, and multimodal tasks."/><meta property="og:url" content="https://zenlm.org"/><meta property="og:site_name" content="Zen LM"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="@zenlmorg"/><meta name="twitter:creator" content="@zenlmorg"/><meta name="twitter:title" content="Zen LM - Open Foundation Models"/><meta name="twitter:description" content="Zen AI model family by Hanzo AI. 14 frontier models from 4B to 1T+ parameters for code, reasoning, vision, and multimodal tasks."/><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="antialiased"><div hidden=""><!--$--><!--/$--></div><script>((a,b,c,d,e,f,g,h)=>{let i=document.documentElement,j=["light","dark"];function k(b){var c;(Array.isArray(a)?a:[a]).forEach(a=>{let c="class"===a,d=c&&f?e.map(a=>f[a]||a):e;c?(i.classList.remove(...d),i.classList.add(f&&f[b]?f[b]:b)):i.setAttribute(a,b)}),c=b,h&&j.includes(c)&&(i.style.colorScheme=c)}if(d)k(d);else try{let a=localStorage.getItem(b)||c,d=g&&"system"===a?window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":a;k(d)}catch(a){}})("class","theme","system",null,["light","dark"],null,true,true)</script><!--$--><div data-closed="" role="presentation" hidden="" style="user-select:none;-webkit-user-select:none" class="fixed inset-0 z-50 backdrop-blur-xs bg-fd-overlay data-open:animate-fd-fade-in data-closed:animate-fd-fade-out"></div><div class="bg-fd-secondary/50 p-3 empty:hidden"></div><!--/$--><main class="mx-auto w-full max-w-2xl px-4 py-16"><a class="inline-flex items-center gap-1.5 text-sm text-fd-muted-foreground hover:text-fd-foreground mb-10 transition-colors" href="/blog">← Back to Blog</a><div class="mb-8"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">January 14, 2026</time><h1 class="text-3xl font-bold mt-2 mb-3">Introducing Zen LM: Open Frontier Models from Hanzo AI and Zoo Labs</h1><p class="text-fd-muted-foreground text-lg mb-4">Announcing the Zen model family: 94+ open models built on Zen MoDE architecture, co-developed by Hanzo AI and Zoo Labs Foundation.</p><div class="flex items-center gap-3 pt-4 border-t border-fd-border"><span class="text-sm text-fd-muted-foreground">By <!-- -->Zen LM Team</span><div class="flex gap-1.5 ml-auto"><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Announcement</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Models</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Zen</span></div></div></div><div class="prose dark:prose-invert max-w-none"><p><a href="https://github.com/hanzoai" rel="noreferrer noopener" target="_blank">GITHUB</a> <a href="https://huggingface.co/hanzoai" rel="noreferrer noopener" target="_blank">HUGGING FACE</a> <a href="https://hanzo.ai/chat" rel="noreferrer noopener" target="_blank">TRY ZEN CHAT</a></p>
<p>Today we are announcing <strong>Zen LM</strong> — a family of open frontier models co-developed by Hanzo AI and Zoo Labs Foundation. This release marks the public launch of the Zen model catalog: 94+ models spanning text, vision, audio, and code, all built on our <strong>Zen MoDE</strong> (Mixture of Distilled Experts) architecture.</p>
<h2 class="flex scroll-m-28 flex-row items-center gap-2" id="why-we-built-it"><a data-card="" href="#why-we-built-it" class="peer">Why We Built It</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>Modern AI infrastructure concentrates capability in a small number of proprietary systems. The models that power the most demanding production workloads are closed, expensive, and opaque. We believe frontier capability should be accessible — not as a service you pay per token, but as open weights you can run, inspect, fine-tune, and deploy on your own infrastructure.</p>
<p>Hanzo AI has spent years building AI infrastructure used by thousands of developers. Zoo Labs Foundation has led decentralized AI research through ZIPs (Zoo Improvement Proposals) and the PoAI (Proof of AI) protocol. Zen LM is the convergence of that work: production-grade models, open weights, governed by the community.</p>
<h2 class="flex scroll-m-28 flex-row items-center gap-2" id="the-zen-mode-architecture"><a data-card="" href="#the-zen-mode-architecture" class="peer">The Zen MoDE Architecture</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>Zen MoDE stands for <strong>Mixture of Distilled Experts</strong>. Every Zen model is built on this architecture:</p>
<ul>
<li><strong>Expert routing</strong> : Each token is processed by a small subset of specialized expert networks</li>
<li><strong>Distillation from large teachers</strong> : Experts are initialized from, and continuously refined against, larger teacher models</li>
<li><strong>Efficient inference</strong> : Only a fraction of total parameters activate per token, making large models economically viable</li>
</ul>
<p>The result: Zen models deliver capability competitive with much larger dense models at a fraction of the inference cost.</p>
<p>For a deep technical treatment see our architecture post: <a href="/blog/zen-mode-architecture">Zen MoDE Architecture</a>.</p>
<h2 class="flex scroll-m-28 flex-row items-center gap-2" id="the-catalog"><a data-card="" href="#the-catalog" class="peer">The Catalog</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>The Zen family covers every major use case:</p>




























































<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Model</th><th>Parameters</th><th>Use Case</th></tr></thead><tbody><tr><td>Zen4 Ultra</td><td>480B (35B active)</td><td>Maximum capability, frontier tasks</td></tr><tr><td>Zen Max</td><td>72B</td><td>General enterprise use</td></tr><tr><td>Zen4 Pro</td><td>32B (22B active)</td><td>Balanced capability/cost</td></tr><tr><td>Zen4 Flash</td><td>7B (3B active)</td><td>Low-latency production</td></tr><tr><td>Zen4 Coder</td><td>480B (35B active)</td><td>Code generation, agentic coding</td></tr><tr><td>Zen Omni</td><td>32B</td><td>Vision + text + audio</td></tr><tr><td>Zen VL</td><td>72B</td><td>Image understanding, OCR</td></tr><tr><td>Zen Nano</td><td>0.6B</td><td>On-device inference</td></tr><tr><td>Zen Embedding</td><td>7680-dim</td><td>Semantic search, RAG</td></tr><tr><td>Zen Guard</td><td>3B</td><td>Safety classification</td></tr></tbody></table></div>
<p>All general-purpose models are released under <strong>Apache-2.0</strong>. Safety models use a more restrictive license to prevent adversarial use.</p>
<h2 class="flex scroll-m-28 flex-row items-center gap-2" id="how-we-train"><a data-card="" href="#how-we-train" class="peer">How We Train</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>Zen models are trained on the Zoo Compute Network — a decentralized GPU cluster funded through the Zoo Labs Foundation treasury and governed by ZIPs. Training details:</p>
<ul>
<li><strong>Data</strong> : 15T+ tokens, high-quality filtered corpus with documented provenance</li>
<li><strong>Compute</strong> : Distributed across heterogeneous H100/A100 clusters</li>
<li><strong>Alignment</strong> : GRPO (Group Relative Policy Optimization) for instruction following</li>
<li><strong>Continuous improvement</strong> : ASO (Active Semantic Optimization, HIP-002) feeds production signals back into training</li>
</ul>
<h2 class="flex scroll-m-28 flex-row items-center gap-2" id="open-weights-commitment"><a data-card="" href="#open-weights-commitment" class="peer">Open Weights Commitment</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>We release weights, not just APIs. Every Zen model is available on Hugging Face under <code>hanzoai/</code>. You can:</p>
<ul>
<li>Download and run locally with <code>transformers</code>, <code>vLLM</code>, or <code>llama.cpp</code></li>
<li>Fine-tune on your own data</li>
<li>Deploy on any infrastructure</li>
<li>Inspect weights, tokenizer, and training configuration</li>
</ul>
<p>We believe open weights is the only credible commitment to openness. APIs can be taken down. Weights are permanent.</p>
<h2 class="flex scroll-m-28 flex-row items-center gap-2" id="whats-next"><a data-card="" href="#whats-next" class="peer">What’s Next</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>The Zen4 generation is live today. We are already training Zen5, with improvements to:</p>
<ul>
<li>Long-context reasoning (targeting 2M token context)</li>
<li>Multimodal integration (tighter vision-text-audio coupling)</li>
<li>Agentic reliability (reduced hallucination in tool-use chains)</li>
<li>Distillation efficiency (more capability per active parameter)</li>
</ul>
<p>Follow along at <a href="https://zenlm.org" rel="noreferrer noopener" target="_blank">zenlm.org</a>, on <a href="https://github.com/hanzoai" rel="noreferrer noopener" target="_blank">GitHub</a>, and on <a href="https://huggingface.co/hanzoai" rel="noreferrer noopener" target="_blank">Hugging Face</a>.</p>
<hr/>
<p><em>Zen LM is a joint initiative of Hanzo AI Inc. (Techstars ‘17) and Zoo Labs Foundation (501c3).</em></p></div></main><!--$--><!--/$--><script src="/_next/static/chunks/e83606e8fa9cc796.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[106,[\"/_next/static/chunks/59d0ad1b64f8544e.js\"],\"RootProvider\"]\n3:I[53113,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"default\"]\n4:I[73211,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"default\"]\n5:I[10086,[\"/_next/static/chunks/59d0ad1b64f8544e.js\",\"/_next/static/chunks/8de849ca74fc071f.js\",\"/_next/static/chunks/e62b91212ee7f8ff.js\",\"/_next/static/chunks/f19fe44237e54646.js\",\"/_next/static/chunks/cb0a883bafeb6805.js\"],\"\"]\n7:I[89923,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"OutletBoundary\"]\n8:\"$Sreact.suspense\"\na:I[89923,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"ViewportBoundary\"]\nc:I[89923,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"MetadataBoundary\"]\ne:I[6998,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"default\"]\n:HL[\"/_next/static/chunks/f2332aac77592f9d.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"i-dnJM_MIpJSOCQWNJVMq\",\"c\":[\"\",\"blog\",\"zen-lm-launch\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"zen-lm-launch\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/f2332aac77592f9d.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/59d0ad1b64f8544e.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"dark\",\"suppressHydrationWarning\":true,\"children\":[\"$\",\"body\",null,{\"className\":\"antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"main\",null,{\"className\":\"flex min-h-screen flex-col items-center justify-center px-4 text-center\",\"children\":[[\"$\",\"div\",null,{\"className\":\"mb-8 opacity-20\",\"children\":[\"$\",\"svg\",null,{\"width\":\"120\",\"height\":\"120\",\"viewBox\":\"0 0 120 120\",\"fill\":\"none\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"circle\",null,{\"cx\":\"60\",\"cy\":\"60\",\"r\":\"50\",\"stroke\":\"currentColor\",\"strokeWidth\":\"3\",\"strokeLinecap\":\"round\",\"strokeDasharray\":\"280 40\"}]}]}],[\"$\",\"p\",null,{\"className\":\"text-xs font-mono uppercase tracking-[0.3em] text-fd-muted-foreground mb-4\",\"children\":\"404\"}],[\"$\",\"h1\",null,{\"className\":\"text-3xl font-semibold mb-3\",\"children\":\"Page not found\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground max-w-sm mb-10\",\"children\":\"This page doesn't exist, or it may have moved. Try the documentation or head home.\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-3 justify-center\",\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/\",\"className\":\"inline-flex items-center gap-2 rounded-lg bg-fd-primary text-fd-primary-foreground px-4 py-2 text-sm font-medium hover:opacity-90 transition\",\"children\":\"Go home\"}],[\"$\",\"$L5\",null,{\"href\":\"/docs\",\"className\":\"inline-flex items-center gap-2 rounded-lg border border-fd-border bg-fd-background px-4 py-2 text-sm font-medium hover:bg-fd-muted transition\",\"children\":\"Documentation\"}],[\"$\",\"$L5\",null,{\"href\":\"/docs/models\",\"className\":\"inline-flex items-center gap-2 rounded-lg border border-fd-border bg-fd-background px-4 py-2 text-sm font-medium hover:bg-fd-muted transition\",\"children\":\"Browse models\"}]]}],[\"$\",\"p\",null,{\"className\":\"mt-16 text-xs font-mono text-fd-muted-foreground opacity-50\",\"children\":\"zenlm.org\"}]]}],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[\"$L6\",[[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/8de849ca74fc071f.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-1\",{\"src\":\"/_next/static/chunks/e62b91212ee7f8ff.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-2\",{\"src\":\"/_next/static/chunks/f19fe44237e54646.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-3\",{\"src\":\"/_next/static/chunks/cb0a883bafeb6805.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"$L7\",null,{\"children\":[\"$\",\"$8\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@9\"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$La\",null,{\"children\":\"$Lb\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$Lc\",null,{\"children\":[\"$\",\"$8\",null,{\"name\":\"Next.Metadata\",\"children\":\"$Ld\"}]}]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$e\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"f:I[48068,[\"/_next/static/chunks/59d0ad1b64f8544e.js\",\"/_next/static/chunks/8de849ca74fc071f.js\",\"/_next/static/chunks/e62b91212ee7f8ff.js\",\"/_next/static/chunks/f19fe44237e54646.js\",\"/_next/static/chunks/cb0a883bafeb6805.js\"],\"default\"]\n"])</script><script>self.__next_f.push([1,"6:[\"$\",\"main\",null,{\"className\":\"mx-auto w-full max-w-2xl px-4 py-16\",\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog\",\"className\":\"inline-flex items-center gap-1.5 text-sm text-fd-muted-foreground hover:text-fd-foreground mb-10 transition-colors\",\"children\":\"← Back to Blog\"}],[\"$\",\"div\",null,{\"className\":\"mb-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"January 14, 2026\"}],[\"$\",\"h1\",null,{\"className\":\"text-3xl font-bold mt-2 mb-3\",\"children\":\"Introducing Zen LM: Open Frontier Models from Hanzo AI and Zoo Labs\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-lg mb-4\",\"children\":\"Announcing the Zen model family: 94+ open models built on Zen MoDE architecture, co-developed by Hanzo AI and Zoo Labs Foundation.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3 pt-4 border-t border-fd-border\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-sm text-fd-muted-foreground\",\"children\":[\"By \",\"Zen LM Team\"]}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[[\"$\",\"span\",\"Announcement\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Announcement\"}],[\"$\",\"span\",\"Models\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Models\"}],[\"$\",\"span\",\"Zen\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Zen\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"prose dark:prose-invert max-w-none\",\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/hanzoai\",\"children\":\"GITHUB\"}],\" \",[\"$\",\"$Lf\",null,{\"href\":\"https://huggingface.co/hanzoai\",\"children\":\"HUGGING FACE\"}],\" \",[\"$\",\"$Lf\",null,{\"href\":\"https://hanzo.ai/chat\",\"children\":\"TRY ZEN CHAT\"}]]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Today we are announcing \",[\"$\",\"strong\",null,{\"children\":\"Zen LM\"}],\" — a family of open frontier models co-developed by Hanzo AI and Zoo Labs Foundation. This release marks the public launch of the Zen model catalog: 94+ models spanning text, vision, audio, and code, all built on our \",[\"$\",\"strong\",null,{\"children\":\"Zen MoDE\"}],\" (Mixture of Distilled Experts) architecture.\"]}],\"\\n\",[\"$\",\"h2\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"why-we-built-it\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#why-we-built-it\",\"className\":\"peer\",\"children\":\"Why We Built It\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Modern AI infrastructure concentrates capability in a small number of proprietary systems. The models that power the most demanding production workloads are closed, expensive, and opaque. We believe frontier capability should be accessible — not as a service you pay per token, but as open weights you can run, inspect, fine-tune, and deploy on your own infrastructure.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Hanzo AI has spent years building AI infrastructure used by thousands of developers. Zoo Labs Foundation has led decentralized AI research through ZIPs (Zoo Improvement Proposals) and the PoAI (Proof of AI) protocol. Zen LM is the convergence of that work: production-grade models, open weights, governed by the community.\"}],\"\\n\",\"$L10\",\"\\n\",\"$L11\",\"\\n\",\"$L12\",\"\\n\",\"$L13\",\"\\n\",\"$L14\",\"\\n\",\"$L15\",\"\\n\",\"$L16\",\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L17\",\"\\n\",\"$L18\",\"\\n\",\"$L19\",\"\\n\",\"$L1a\",\"\\n\",\"$L1b\",\"\\n\",\"$L1c\",\"\\n\",\"$L1d\",\"\\n\",\"$L1e\",\"\\n\",\"$L1f\",\"\\n\",\"$L20\",\"\\n\",\"$L21\",\"\\n\",\"$L22\",\"\\n\",\"$L23\",\"\\n\",\"$L24\",\"\\n\",\"$L25\"]}]]}]\n"])</script><script>self.__next_f.push([1,"10:[\"$\",\"h2\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"the-zen-mode-architecture\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#the-zen-mode-architecture\",\"className\":\"peer\",\"children\":\"The Zen MoDE Architecture\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n11:[\"$\",\"p\",null,{\"children\":[\"Zen MoDE stands for \",[\"$\",\"strong\",null,{\"children\":\"Mixture of Distilled Experts\"}],\". Every Zen model is built on this architecture:\"]}]\n12:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Expert routing\"}],\" : Each token is processed by a small subset of specialized expert networks\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Distillation from large teachers\"}],\" : Experts are initialized from, and continuously refined against, larger teacher models\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Efficient inference\"}],\" : Only a fraction of total parameters activate per token, making large models economically viable\"]}],\"\\n\"]}]\n13:[\"$\",\"p\",null,{\"children\":\"The result: Zen models deliver capability competitive with much larger dense models at a fraction of the inference cost.\"}]\n14:[\"$\",\"p\",null,{\"children\":[\"For a deep technical treatment see our architecture post: \",[\"$\",\"$Lf\",null,{\"href\":\"/blog/zen-mode-architecture/\",\"children\":\"Zen MoDE Architecture\"}],\".\"]}]\n15:[\"$\",\"h2\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"the-catalog\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#the-catalog\",\"className\":\"peer\",\"children\":\"The Catalog\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n16:[\"$\",\"p\",null,{\"children\":\"The Zen family covers every major use case:\"}]\n"])</script><script>self.__next_f.push([1,"17:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Model\"}],[\"$\",\"th\",null,{\"children\":\"Parameters\"}],[\"$\",\"th\",null,{\"children\":\"Use Case\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Zen4 Ultra\"}],[\"$\",\"td\",null,{\"children\":\"480B (35B active)\"}],[\"$\",\"td\",null,{\"children\":\"Maximum capability, frontier tasks\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Zen Max\"}],[\"$\",\"td\",null,{\"children\":\"72B\"}],[\"$\",\"td\",null,{\"children\":\"General enterprise use\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Zen4 Pro\"}],[\"$\",\"td\",null,{\"children\":\"32B (22B active)\"}],[\"$\",\"td\",null,{\"children\":\"Balanced capability/cost\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Zen4 Flash\"}],[\"$\",\"td\",null,{\"children\":\"7B (3B active)\"}],[\"$\",\"td\",null,{\"children\":\"Low-latency production\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Zen4 Coder\"}],[\"$\",\"td\",null,{\"children\":\"480B (35B active)\"}],[\"$\",\"td\",null,{\"children\":\"Code generation, agentic coding\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Zen Omni\"}],[\"$\",\"td\",null,{\"children\":\"32B\"}],[\"$\",\"td\",null,{\"children\":\"Vision + text + audio\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Zen VL\"}],[\"$\",\"td\",null,{\"children\":\"72B\"}],[\"$\",\"td\",null,{\"children\":\"Image understanding, OCR\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Zen Nano\"}],[\"$\",\"td\",null,{\"children\":\"0.6B\"}],[\"$\",\"td\",null,{\"children\":\"On-device inference\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Zen Embedding\"}],[\"$\",\"td\",null,{\"children\":\"7680-dim\"}],[\"$\",\"td\",null,{\"children\":\"Semantic search, RAG\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Zen Guard\"}],[\"$\",\"td\",null,{\"children\":\"3B\"}],[\"$\",\"td\",null,{\"children\":\"Safety classification\"}]]}]]}]]}]}]\n"])</script><script>self.__next_f.push([1,"18:[\"$\",\"p\",null,{\"children\":[\"All general-purpose models are released under \",[\"$\",\"strong\",null,{\"children\":\"Apache-2.0\"}],\". Safety models use a more restrictive license to prevent adversarial use.\"]}]\n19:[\"$\",\"h2\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"how-we-train\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#how-we-train\",\"className\":\"peer\",\"children\":\"How We Train\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n1a:[\"$\",\"p\",null,{\"children\":\"Zen models are trained on the Zoo Compute Network — a decentralized GPU cluster funded through the Zoo Labs Foundation treasury and governed by ZIPs. Training details:\"}]\n1b:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Data\"}],\" : 15T+ tokens, high-quality filtered corpus with documented provenance\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Compute\"}],\" : Distributed across heterogeneous H100/A100 clusters\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Alignment\"}],\" : GRPO (Group Relative Policy Optimization) for instruction following\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Continuous improvement\"}],\" : ASO (Active Semantic Optimization, HIP-002) feeds production signals back into training\"]}],\"\\n\"]}]\n1c:[\"$\",\"h2\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"open-weights-commitment\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#open-weights-commitment\",\"className\":\"peer\",\"children\":\"Open Weights Commitment\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n1d:[\"$\",\"p\",null,{\"children\":[\"We release weights, not just APIs. Every Zen model is available on Hugging Face under \",[\"$\",\"code\",null,{\"children\":\"hanzoai/\"}],\". You can:\"]}]\n1e:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Download and run locally with \",[\"$\",\"code\",null,{\"children\":\"transformers\"}],\", \",[\"$\",\"code\",null,{\"children\":\"vLLM\"}],\", or \",[\"$\",\"code\",null,{\"children\":\"llama.cpp\"}]]}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Fine-tune on your own data\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Deploy on any infrastructure\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Inspect weights, tokenizer, and training configuration\"}],\"\\n\"]}]\n1f:[\"$\",\"p\",null,{\"children\":\"We believe open weights is the only credible commitment to openness. APIs can be taken down. Weights are permanent.\"}]\n20:[\"$\",\"h2\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"whats-next\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#whats-next\",\"className\":\"peer\",\"children\":\"What’s Next\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M1"])</script><script>self.__next_f.push([1,"0 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n21:[\"$\",\"p\",null,{\"children\":\"The Zen4 generation is live today. We are already training Zen5, with improvements to:\"}]\n22:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Long-context reasoning (targeting 2M token context)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Multimodal integration (tighter vision-text-audio coupling)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Agentic reliability (reduced hallucination in tool-use chains)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Distillation efficiency (more capability per active parameter)\"}],\"\\n\"]}]\n23:[\"$\",\"p\",null,{\"children\":[\"Follow along at \",[\"$\",\"$Lf\",null,{\"href\":\"https://zenlm.org\",\"children\":\"zenlm.org\"}],\", on \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/hanzoai\",\"children\":\"GitHub\"}],\", and on \",[\"$\",\"$Lf\",null,{\"href\":\"https://huggingface.co/hanzoai\",\"children\":\"Hugging Face\"}],\".\"]}]\n24:[\"$\",\"hr\",null,{}]\n25:[\"$\",\"p\",null,{\"children\":[\"$\",\"em\",null,{\"children\":\"Zen LM is a joint initiative of Hanzo AI Inc. (Techstars ‘17) and Zoo Labs Foundation (501c3).\"}]}]\n"])</script><script>self.__next_f.push([1,"b:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"2\",{\"name\":\"theme-color\",\"media\":\"(prefers-color-scheme: dark)\",\"content\":\"#0A0A0A\"}],[\"$\",\"meta\",\"3\",{\"name\":\"theme-color\",\"media\":\"(prefers-color-scheme: light)\",\"content\":\"#fff\"}]]\n"])</script><script>self.__next_f.push([1,"9:null\nd:[[\"$\",\"title\",\"0\",{\"children\":\"Introducing Zen LM: Open Frontier Models from Hanzo AI and Zoo Labs — Zen LM Blog | Zen LM\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Announcing the Zen model family: 94+ open models built on Zen MoDE architecture, co-developed by Hanzo AI and Zoo Labs Foundation.\"}],[\"$\",\"meta\",\"2\",{\"property\":\"og:title\",\"content\":\"Zen LM - Open Foundation Models\"}],[\"$\",\"meta\",\"3\",{\"property\":\"og:description\",\"content\":\"Zen AI model family by Hanzo AI. 14 frontier models from 4B to 1T+ parameters for code, reasoning, vision, and multimodal tasks.\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:url\",\"content\":\"https://zenlm.org\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:site_name\",\"content\":\"Zen LM\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"7\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"8\",{\"name\":\"twitter:site\",\"content\":\"@zenlmorg\"}],[\"$\",\"meta\",\"9\",{\"name\":\"twitter:creator\",\"content\":\"@zenlmorg\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:title\",\"content\":\"Zen LM - Open Foundation Models\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:description\",\"content\":\"Zen AI model family by Hanzo AI. 14 frontier models from 4B to 1T+ parameters for code, reasoning, vision, and multimodal tasks.\"}]]\n"])</script></body></html>