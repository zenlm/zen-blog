<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Embedding Spaces at 7680 Dimensions | Zen LM</title><meta name=keywords content="Research,Embeddings,Retrieval"><meta name=description content="Exploring high-dimensional embedding spaces for semantic search and retrieval."><meta name=author content="Zach Kelling"><link rel=canonical href=https://zenlm.org/blog/embedding-spaces-7680-dimensions/><link crossorigin=anonymous href=/assets/css/stylesheet.6c0865a4bc8dbcbd27d78777eb33b404568f7fbf3185cca7b978e5275a4dd049.css integrity="sha256-bAhlpLyNvL0n14d36zO0BFaPf78xhcynuXjlJ1pN0Ek=" rel="preload stylesheet" as=style><link rel=icon href=https://zenlm.org/favicon.png><link rel=apple-touch-icon href=https://zenlm.org/favicon.png><link rel=manifest href=https://zenlm.org/site.webmanifest><meta name=theme-color content="#615CED"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:title" content="Embedding Spaces at 7680 Dimensions"><meta property="og:description" content="Exploring high-dimensional embedding spaces for semantic search and retrieval."><meta property="og:type" content="article"><meta property="og:url" content="https://zenlm.org/blog/embedding-spaces-7680-dimensions/"><meta property="og:image" content="https://zenlm.org/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2022-12-05T00:00:00+00:00"><meta property="article:modified_time" content="2022-12-05T00:00:00+00:00"><meta property="og:site_name" content="Zen LM"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://zenlm.org/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Embedding Spaces at 7680 Dimensions"><meta name=twitter:description content="Exploring high-dimensional embedding spaces for semantic search and retrieval."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://zenlm.org/blog/"},{"@type":"ListItem","position":2,"name":"Embedding Spaces at 7680 Dimensions","item":"https://zenlm.org/blog/embedding-spaces-7680-dimensions/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Embedding Spaces at 7680 Dimensions","name":"Embedding Spaces at 7680 Dimensions","description":"Exploring high-dimensional embedding spaces for semantic search and retrieval.","keywords":["Research","Embeddings","Retrieval"],"articleBody":"The Dimension Question How many dimensions does a text embedding need?\nThe field has settled on conventions: 768 for BERT-scale models, 1536 for OpenAI’s ada-002, 4096 for some recent models. But these choices reflect architectural constraints, not fundamental requirements.\nWe investigate what happens when we scale embedding dimensions to 7680—ten times the BERT baseline.\nWhy Higher Dimensions? Capacity Arguments A $d$-dimensional embedding space can represent $\\mathcal{O}(e^d)$ nearly-orthogonal vectors. For semantic search, we want documents with different meanings to map to different regions. Higher dimensions provide more room.\nInformation-Theoretic View Consider a corpus of $N$ documents, each with entropy $H$ bits. A $d$-dimensional float32 embedding stores $32d$ bits. For lossless encoding:\n$$32d \\geq N \\cdot H$$\nFor a corpus of 1 billion documents with 100 bits of effective information each, we need:\n$$d \\geq \\frac{10^9 \\cdot 100}{32} \\approx 3 \\times 10^9$$\nClearly, embeddings are lossy compression. But higher dimensions reduce the loss.\nEmpirical Observations Retrieval quality on our internal benchmarks plateaued around:\n768 dimensions: 71% recall@10 1536 dimensions: 78% recall@10 3072 dimensions: 83% recall@10 7680 dimensions: 87% recall@10 Diminishing returns set in, but gains persist well beyond conventional wisdom.\nTraining High-Dimensional Embeddings Architecture We use a standard transformer encoder with a projection head:\nInput --\u003e Transformer(L=12, H=768) --\u003e Pool --\u003e Linear(768, 7680) --\u003e Normalize The projection head maps from the transformer’s hidden dimension to the embedding space. This decouples representation capacity from compute requirements.\nContrastive Learning We train with InfoNCE loss over batches of (query, positive, negatives):\n$$\\mathcal{L} = -\\log \\frac{\\exp(q \\cdot p^+ / \\tau)}{\\sum_{i} \\exp(q \\cdot p_i / \\tau)}$$\nWith temperature $\\tau = 0.01$ for high-dimensional spaces (lower than typical).\nHard Negative Mining High-dimensional spaces require harder negatives to provide gradient signal. Our mining strategy:\nRetrieve top-100 candidates via approximate nearest neighbor Filter to exclude true positives Sample negatives weighted by similarity (harder = more likely) This curriculum focuses training on the decision boundary.\nPractical Considerations Storage 7680-dimensional float32 embeddings require 30KB per vector. For 1 billion documents:\n$$10^9 \\times 30\\text{KB} = 30\\text{TB}$$\nThis is substantial but manageable with modern storage.\nQuantization We can reduce storage through quantization:\nPrecision Bytes/Vector Recall@10 float32 30,720 87.3% float16 15,360 87.1% int8 7,680 85.9% binary 960 78.2% int8 quantization provides 4x compression with minimal quality loss.\nApproximate Search Exact nearest neighbor search in 7680 dimensions is expensive. We use hierarchical navigable small world (HNSW) graphs:\nDimensions Build Time Query Time Recall@10 768 1x 1x 99.2% 7680 3.2x 2.8x 98.7% The overhead is sublinear in dimensionality due to efficient distance computations.\nBenchmark Results MS MARCO Passage Retrieval Model Dimensions MRR@10 Recall@100 BM25 - 18.4 66.5 DPR 768 31.1 82.4 Contriever 768 32.8 84.1 Zen-Embed 7680 38.6 91.3 Natural Questions Model Dimensions Top-20 Acc Top-100 Acc DPR 768 78.4 85.4 Contriever 768 81.3 88.1 Zen-Embed 7680 86.7 93.2 High-dimensional embeddings provide substantial gains on retrieval benchmarks.\nAnalysis What Do Extra Dimensions Encode? We analyze the learned embedding space through probing tasks:\nProperty 768d Probe Acc 7680d Probe Acc Topic 84.2% 86.1% Sentiment 91.3% 92.8% Entity 67.4% 78.9% Relation 52.1% 71.3% The largest gains are in entity and relation encoding—fine-grained semantic properties that require more capacity.\nNearest Neighbor Analysis For the query “What causes inflation?”, nearest neighbors at different dimensions:\n768 dimensions:\nWhat is inflation? (similar query) How does inflation work? (similar query) Inflation rates by country (tangential) 7680 dimensions:\nInflation is caused by… (direct answer) The primary drivers of inflation include… (direct answer) Central bank policies affect inflation through… (relevant detail) Higher dimensions better distinguish queries from answers.\nRecommendations Based on our experiments:\nTry 3072+ dimensions if retrieval quality matters and storage is available Use int8 quantization for production deployments Invest in hard negative mining to realize the benefits of capacity Benchmark on your data—gains vary by domain Conventional embedding sizes are conventions, not laws. Question them.\nTechnical details in “Scaling Embedding Dimensions for Semantic Retrieval” (2022). Model weights available on Hugging Face.\n","wordCount":"647","inLanguage":"en","datePublished":"2022-12-05T00:00:00Z","dateModified":"2022-12-05T00:00:00Z","author":{"@type":"Person","name":"Zach Kelling"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://zenlm.org/blog/embedding-spaces-7680-dimensions/"},"publisher":{"@type":"Organization","name":"Zen LM","logo":{"@type":"ImageObject","url":"https://zenlm.org/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Zen LM (Alt + H)"><img src=https://zenlm.org/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://zeekay.blog title=zeekay><span>zeekay</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://hanzo.blog title=hanzo.blog><span>hanzo.blog</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://hanzo.ai/chat title="Try Zen Chat"><span>Try Zen Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://blog.zoo.ngo title="zoo blog"><span>zoo blog</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Embedding Spaces at 7680 Dimensions</h1><div class=post-description>Exploring high-dimensional embedding spaces for semantic search and retrieval.</div><div class=post-meta><span title='2022-12-05 00:00:00 +0000 UTC'>December 5, 2022</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;647 words&nbsp;·&nbsp;Zach Kelling</div></div></div><main class=main><article class=post-single><div class=post-content><h1 id=the-dimension-question>The Dimension Question<a hidden class=anchor aria-hidden=true href=#the-dimension-question>#</a></h1><p>How many dimensions does a text embedding need?</p><p>The field has settled on conventions: 768 for BERT-scale models, 1536 for OpenAI&rsquo;s ada-002, 4096 for some recent models. But these choices reflect architectural constraints, not fundamental requirements.</p><p>We investigate what happens when we scale embedding dimensions to 7680&mdash;ten times the BERT baseline.</p><h2 id=why-higher-dimensions>Why Higher Dimensions?<a hidden class=anchor aria-hidden=true href=#why-higher-dimensions>#</a></h2><h3 id=capacity-arguments>Capacity Arguments<a hidden class=anchor aria-hidden=true href=#capacity-arguments>#</a></h3><p>A $d$-dimensional embedding space can represent $\mathcal{O}(e^d)$ nearly-orthogonal vectors. For semantic search, we want documents with different meanings to map to different regions. Higher dimensions provide more room.</p><h3 id=information-theoretic-view>Information-Theoretic View<a hidden class=anchor aria-hidden=true href=#information-theoretic-view>#</a></h3><p>Consider a corpus of $N$ documents, each with entropy $H$ bits. A $d$-dimensional float32 embedding stores $32d$ bits. For lossless encoding:</p><p>$$32d \geq N \cdot H$$</p><p>For a corpus of 1 billion documents with 100 bits of effective information each, we need:</p><p>$$d \geq \frac{10^9 \cdot 100}{32} \approx 3 \times 10^9$$</p><p>Clearly, embeddings are lossy compression. But higher dimensions reduce the loss.</p><h3 id=empirical-observations>Empirical Observations<a hidden class=anchor aria-hidden=true href=#empirical-observations>#</a></h3><p>Retrieval quality on our internal benchmarks plateaued around:</p><ul><li>768 dimensions: 71% recall@10</li><li>1536 dimensions: 78% recall@10</li><li>3072 dimensions: 83% recall@10</li><li>7680 dimensions: 87% recall@10</li></ul><p>Diminishing returns set in, but gains persist well beyond conventional wisdom.</p><h2 id=training-high-dimensional-embeddings>Training High-Dimensional Embeddings<a hidden class=anchor aria-hidden=true href=#training-high-dimensional-embeddings>#</a></h2><h3 id=architecture>Architecture<a hidden class=anchor aria-hidden=true href=#architecture>#</a></h3><p>We use a standard transformer encoder with a projection head:</p><pre tabindex=0><code>Input --&gt; Transformer(L=12, H=768) --&gt; Pool --&gt; Linear(768, 7680) --&gt; Normalize
</code></pre><p>The projection head maps from the transformer&rsquo;s hidden dimension to the embedding space. This decouples representation capacity from compute requirements.</p><h3 id=contrastive-learning>Contrastive Learning<a hidden class=anchor aria-hidden=true href=#contrastive-learning>#</a></h3><p>We train with InfoNCE loss over batches of (query, positive, negatives):</p><p>$$\mathcal{L} = -\log \frac{\exp(q \cdot p^+ / \tau)}{\sum_{i} \exp(q \cdot p_i / \tau)}$$</p><p>With temperature $\tau = 0.01$ for high-dimensional spaces (lower than typical).</p><h3 id=hard-negative-mining>Hard Negative Mining<a hidden class=anchor aria-hidden=true href=#hard-negative-mining>#</a></h3><p>High-dimensional spaces require harder negatives to provide gradient signal. Our mining strategy:</p><ol><li>Retrieve top-100 candidates via approximate nearest neighbor</li><li>Filter to exclude true positives</li><li>Sample negatives weighted by similarity (harder = more likely)</li></ol><p>This curriculum focuses training on the decision boundary.</p><h2 id=practical-considerations>Practical Considerations<a hidden class=anchor aria-hidden=true href=#practical-considerations>#</a></h2><h3 id=storage>Storage<a hidden class=anchor aria-hidden=true href=#storage>#</a></h3><p>7680-dimensional float32 embeddings require 30KB per vector. For 1 billion documents:</p><p>$$10^9 \times 30\text{KB} = 30\text{TB}$$</p><p>This is substantial but manageable with modern storage.</p><h3 id=quantization>Quantization<a hidden class=anchor aria-hidden=true href=#quantization>#</a></h3><p>We can reduce storage through quantization:</p><table><thead><tr><th>Precision</th><th>Bytes/Vector</th><th>Recall@10</th></tr></thead><tbody><tr><td>float32</td><td>30,720</td><td>87.3%</td></tr><tr><td>float16</td><td>15,360</td><td>87.1%</td></tr><tr><td>int8</td><td>7,680</td><td>85.9%</td></tr><tr><td>binary</td><td>960</td><td>78.2%</td></tr></tbody></table><p>int8 quantization provides 4x compression with minimal quality loss.</p><h3 id=approximate-search>Approximate Search<a hidden class=anchor aria-hidden=true href=#approximate-search>#</a></h3><p>Exact nearest neighbor search in 7680 dimensions is expensive. We use hierarchical navigable small world (HNSW) graphs:</p><table><thead><tr><th>Dimensions</th><th>Build Time</th><th>Query Time</th><th>Recall@10</th></tr></thead><tbody><tr><td>768</td><td>1x</td><td>1x</td><td>99.2%</td></tr><tr><td>7680</td><td>3.2x</td><td>2.8x</td><td>98.7%</td></tr></tbody></table><p>The overhead is sublinear in dimensionality due to efficient distance computations.</p><h2 id=benchmark-results>Benchmark Results<a hidden class=anchor aria-hidden=true href=#benchmark-results>#</a></h2><h3 id=ms-marco-passage-retrieval>MS MARCO Passage Retrieval<a hidden class=anchor aria-hidden=true href=#ms-marco-passage-retrieval>#</a></h3><table><thead><tr><th>Model</th><th>Dimensions</th><th>MRR@10</th><th>Recall@100</th></tr></thead><tbody><tr><td>BM25</td><td>-</td><td>18.4</td><td>66.5</td></tr><tr><td>DPR</td><td>768</td><td>31.1</td><td>82.4</td></tr><tr><td>Contriever</td><td>768</td><td>32.8</td><td>84.1</td></tr><tr><td>Zen-Embed</td><td>7680</td><td>38.6</td><td>91.3</td></tr></tbody></table><h3 id=natural-questions>Natural Questions<a hidden class=anchor aria-hidden=true href=#natural-questions>#</a></h3><table><thead><tr><th>Model</th><th>Dimensions</th><th>Top-20 Acc</th><th>Top-100 Acc</th></tr></thead><tbody><tr><td>DPR</td><td>768</td><td>78.4</td><td>85.4</td></tr><tr><td>Contriever</td><td>768</td><td>81.3</td><td>88.1</td></tr><tr><td>Zen-Embed</td><td>7680</td><td>86.7</td><td>93.2</td></tr></tbody></table><p>High-dimensional embeddings provide substantial gains on retrieval benchmarks.</p><h2 id=analysis>Analysis<a hidden class=anchor aria-hidden=true href=#analysis>#</a></h2><h3 id=what-do-extra-dimensions-encode>What Do Extra Dimensions Encode?<a hidden class=anchor aria-hidden=true href=#what-do-extra-dimensions-encode>#</a></h3><p>We analyze the learned embedding space through probing tasks:</p><table><thead><tr><th>Property</th><th>768d Probe Acc</th><th>7680d Probe Acc</th></tr></thead><tbody><tr><td>Topic</td><td>84.2%</td><td>86.1%</td></tr><tr><td>Sentiment</td><td>91.3%</td><td>92.8%</td></tr><tr><td>Entity</td><td>67.4%</td><td>78.9%</td></tr><tr><td>Relation</td><td>52.1%</td><td>71.3%</td></tr></tbody></table><p>The largest gains are in entity and relation encoding&mdash;fine-grained semantic properties that require more capacity.</p><h3 id=nearest-neighbor-analysis>Nearest Neighbor Analysis<a hidden class=anchor aria-hidden=true href=#nearest-neighbor-analysis>#</a></h3><p>For the query &ldquo;What causes inflation?&rdquo;, nearest neighbors at different dimensions:</p><p><strong>768 dimensions:</strong></p><ol><li>What is inflation? (similar query)</li><li>How does inflation work? (similar query)</li><li>Inflation rates by country (tangential)</li></ol><p><strong>7680 dimensions:</strong></p><ol><li>Inflation is caused by&mldr; (direct answer)</li><li>The primary drivers of inflation include&mldr; (direct answer)</li><li>Central bank policies affect inflation through&mldr; (relevant detail)</li></ol><p>Higher dimensions better distinguish queries from answers.</p><h2 id=recommendations>Recommendations<a hidden class=anchor aria-hidden=true href=#recommendations>#</a></h2><p>Based on our experiments:</p><ol><li><strong>Try 3072+ dimensions</strong> if retrieval quality matters and storage is available</li><li><strong>Use int8 quantization</strong> for production deployments</li><li><strong>Invest in hard negative mining</strong> to realize the benefits of capacity</li><li><strong>Benchmark on your data</strong>&mdash;gains vary by domain</li></ol><p>Conventional embedding sizes are conventions, not laws. Question them.</p><hr><p><em>Technical details in &ldquo;Scaling Embedding Dimensions for Semantic Retrieval&rdquo; (2022). Model weights available on Hugging Face.</em></p></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://zenlm.org/>Zen LM</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>