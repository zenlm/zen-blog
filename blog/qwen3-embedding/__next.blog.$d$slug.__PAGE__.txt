1:"$Sreact.fragment"
2:I[10086,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/8de849ca74fc071f.js","/_next/static/chunks/e62b91212ee7f8ff.js","/_next/static/chunks/f19fe44237e54646.js","/_next/static/chunks/cb0a883bafeb6805.js"],""]
3:I[48068,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/8de849ca74fc071f.js","/_next/static/chunks/e62b91212ee7f8ff.js","/_next/static/chunks/f19fe44237e54646.js","/_next/static/chunks/cb0a883bafeb6805.js"],"default"]
1f:I[89923,["/_next/static/chunks/4d80e004cf4896dd.js","/_next/static/chunks/350ee4303b732916.js"],"OutletBoundary"]
20:"$Sreact.suspense"
0:{"buildId":"i-dnJM_MIpJSOCQWNJVMq","rsc":["$","$1","c",{"children":[["$","main",null,{"className":"mx-auto w-full max-w-2xl px-4 py-16","children":[["$","$L2",null,{"href":"/blog","className":"inline-flex items-center gap-1.5 text-sm text-fd-muted-foreground hover:text-fd-foreground mb-10 transition-colors","children":"← Back to Blog"}],["$","div",null,{"className":"mb-8","children":[["$","time",null,{"className":"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider","children":"June 4, 2025"}],["$","h1",null,{"className":"text-3xl font-bold mt-2 mb-3","children":"Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models"}],["$","p",null,{"className":"text-fd-muted-foreground text-lg mb-4","children":"GITHUB HUGGING FACE MODELSCOPE DISCORD"}],["$","div",null,{"className":"flex items-center gap-3 pt-4 border-t border-fd-border","children":[["$","span",null,{"className":"text-sm text-fd-muted-foreground","children":["By ","Zen LM Team"]}],["$","div",null,{"className":"flex gap-1.5 ml-auto","children":[]}]]}]]}],["$","div",null,{"className":"prose dark:prose-invert max-w-none","children":[["$","p",null,{"children":[["$","$L3",null,{"href":"https://github.com/QwenLM/Qwen3-Embedding","children":"GITHUB"}]," ",["$","$L3",null,{"href":"https://huggingface.co/Qwen","children":"HUGGING FACE"}]," ",["$","$L3",null,{"href":"https://modelscope.cn/organization/qwen","children":"MODELSCOPE"}]," ",["$","$L3",null,{"href":"https://discord.gg/yPEP2vHTu4","children":"DISCORD"}]]}],"\n",["$","p",null,{"children":["We release ",["$","strong",null,{"children":"Qwen3 Embedding series"}]," , a new proprietary model of the Qwen model family. These models are specifically designed for ",["$","strong",null,{"children":"text embedding"}]," , ",["$","strong",null,{"children":"retrieval"}]," , and ",["$","strong",null,{"children":"reranking"}]," tasks, built on the Qwen3 foundation model. Leveraging Qwen3’s robust multilingual text understanding capabilities, the series achieves state-of-the-art performance across multiple benchmarks for text embedding and reranking tasks. We have open-sourced this series of text embedding and reranking models under the Apache 2.0 license on Hugging Face and ModelScope, and published the technical report and related code on GitHub."]}],"\n",["$","p",null,{"children":["$","strong",null,{"children":"Evaluation results for reranking models"}]}],"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",["$","div",null,{"className":"relative overflow-auto prose-no-margin my-6","children":["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"Model"}],["$","th",null,{"children":"Param"}],["$","th",null,{"children":"MTEB-R"}],["$","th",null,{"children":"CMTEB-R"}],["$","th",null,{"children":"MMTEB-R"}],["$","th",null,{"children":"MLDR"}],["$","th",null,{"children":"MTEB-Code"}],["$","th",null,{"children":"FollowIR"}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"Qwen3-Embedding-0.6B"}]}],["$","td",null,{"children":"0.6B"}],["$","td",null,{"children":"61.82"}],["$","td",null,{"children":"71.02"}],["$","td",null,{"children":"64.64"}],["$","td",null,{"children":"50.26"}],["$","td",null,{"children":"75.41"}],["$","td",null,{"children":"5.09"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"Jina-multilingual-reranker-v2-base"}],["$","td",null,{"children":"0.3B"}],["$","td",null,{"children":"58.22"}],["$","td",null,{"children":"63.37"}],["$","td",null,{"children":"63.73"}],["$","td",null,{"children":"39.66"}],["$","td",null,{"children":"58.98"}],["$","td",null,{"children":"-0.68"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"gte-multilingual-reranker-base"}],["$","td",null,{"children":"0.3B"}],["$","td",null,{"children":"59.51"}],["$","td",null,{"children":"74.08"}],["$","td",null,{"children":"59.44"}],["$","td",null,{"children":"66.33"}],["$","td",null,{"children":"54.18"}],["$","td",null,{"children":"-1.64"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"BGE-reranker-v2-m3"}],["$","td",null,{"children":"0.6B"}],["$","td",null,{"children":"57.03"}],["$","td",null,{"children":"72.16"}],["$","td",null,{"children":"58.36"}],["$","td",null,{"children":"59.51"}],["$","td",null,{"children":"41.38"}],["$","td",null,{"children":"-0.01"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"Qwen3-Reranker-0.6B"}]}],["$","td",null,{"children":"0.6B"}],["$","td",null,{"children":"65.80"}],"$L4","$L5","$L6","$L7","$L8"]}],"$L9","$La"]}]]}]}],"\n","$Lb","\n","$Lc","\n","$Ld","\n","$Le","\n","$Lf","\n","$L10","\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","$L11","\n","$L12","\n","$L13","\n","$L14","\n","$L15","\n","$L16","\n","$L17","\n","$L18"]}]]}],["$L19","$L1a","$L1b","$L1c"],"$L1d"]}],"loading":null,"isPartial":false}
4:["$","td",null,{"children":"71.31"}]
5:["$","td",null,{"children":"66.36"}]
6:["$","td",null,{"children":"67.28"}]
7:["$","td",null,{"children":"73.42"}]
8:["$","td",null,{"children":"5.41"}]
9:["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"Qwen3-Reranker-4B"}]}],["$","td",null,{"children":"4B"}],["$","td",null,{"children":["$","strong",null,{"children":"69.76"}]}],["$","td",null,{"children":"75.94"}],["$","td",null,{"children":"72.74"}],["$","td",null,{"children":"69.97"}],["$","td",null,{"children":"81.20"}],["$","td",null,{"children":["$","strong",null,{"children":"14.84"}]}]]}]
a:["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"Qwen3-Reranker-8B"}]}],["$","td",null,{"children":"8B"}],["$","td",null,{"children":"69.02"}],["$","td",null,{"children":["$","strong",null,{"children":"77.45"}]}],["$","td",null,{"children":["$","strong",null,{"children":"72.94"}]}],["$","td",null,{"children":["$","strong",null,{"children":"70.19"}]}],["$","td",null,{"children":["$","strong",null,{"children":"81.22"}]}],["$","td",null,{"children":"8.05"}]]}]
b:["$","blockquote",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Note"}]," :"]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"We use the text retrieval subsets of MTEB(eng, v2), MTEB(cmn, v1), MTEB (Multilingual) and MTEB (Code), which are denoted as MTEB-R, CMTEB-R, MMTEB-R and MTEB-Code."}],"\n",["$","li",null,{"children":["All scores are our runs based on the top-100 candidates retrieved by dense embedding model ",["$","$L3",null,{"href":"https://huggingface.co/Qwen/Qwen3-Embedding-0.6B","children":"Qwen3-Embedding-0.6B"}],"."]}],"\n"]}],"\n"]}]
c:["$","p",null,{"children":[["$","strong",null,{"children":"Key Features"}]," :"]}]
d:["$","p",null,{"children":[["$","strong",null,{"children":"Exceptional Versatility"}]," : The embedding model has achieved state-of-the-art performance across a wide range of downstream application evaluations. The 8B size embedding model ranks No.1 in the MTEB multilingual leaderboard (as of June 5, 2025, score ",["$","strong",null,{"children":"70.58"}],"). The reranking models excel in text retrieval scenarios, significantly improving search relevance."]}]
e:["$","p",null,{"children":[["$","strong",null,{"children":"Comprehensive Flexibility"}]," : The Qwen3 Embedding series offers a diverse range of sizes (from 0.6B to 8B) for both embedding and reranking models, catering to various use cases that prioritize efficiency and effectiveness. Developers can seamlessly combine these two modules. Additionally, the embedding model allows for flexible vector definitions across all dimensions, and both embedding and reranking models support user-defined instructions to enhance performance for specific tasks, languages, or scenarios."]}]
f:["$","p",null,{"children":[["$","strong",null,{"children":"Multilingual Capability"}]," : The Qwen3 Embedding series support over 100 languages, including various programming languages, and provides robust multilingual, cross-lingual, and code retrieval capabilities."]}]
10:["$","p",null,{"children":[["$","strong",null,{"children":"Model Overview"}]," :"]}]
11:["$","div",null,{"className":"relative overflow-auto prose-no-margin my-6","children":["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"Model Type"}],["$","th",null,{"children":"Models"}],["$","th",null,{"children":"Size"}],["$","th",null,{"children":"Layers"}],["$","th",null,{"children":"Sequence Length"}],["$","th",null,{"children":"Embedding Dimension"}],["$","th",null,{"children":"MRL Support"}],["$","th",null,{"children":"Instruction Aware"}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"Text Embedding"}]}],["$","td",null,{"children":"Qwen3-Embedding-0.6B"}],["$","td",null,{"children":"0.6B"}],["$","td",null,{"children":"28"}],["$","td",null,{"children":"32K"}],["$","td",null,{"children":"1024"}],["$","td",null,{"children":"Yes"}],["$","td",null,{"children":"Yes"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"Qwen3-Embedding-4B"}],["$","td",null,{"children":"4B"}],["$","td",null,{"children":"36"}],["$","td",null,{"children":"32K"}],["$","td",null,{"children":"2560"}],["$","td",null,{"children":"Yes"}],["$","td",null,{"children":"Yes"}],["$","td",null,{}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"Qwen3-Embedding-8B"}],["$","td",null,{"children":"8B"}],["$","td",null,{"children":"36"}],["$","td",null,{"children":"32K"}],["$","td",null,{"children":"4096"}],["$","td",null,{"children":"Yes"}],["$","td",null,{"children":"Yes"}],["$","td",null,{}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"Text Reranking"}]}],["$","td",null,{"children":"Qwen3-Reranker-0.6B"}],["$","td",null,{"children":"0.6B"}],["$","td",null,{"children":"28"}],["$","td",null,{"children":"32K"}],["$","td",null,{"children":"-"}],["$","td",null,{"children":"-"}],["$","td",null,{"children":"Yes"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"Qwen3-Reranker-4B"}],["$","td",null,{"children":"4B"}],["$","td",null,{"children":"36"}],["$","td",null,{"children":"32K"}],["$","td",null,{"children":"-"}],["$","td",null,{"children":"-"}],["$","td",null,{"children":"Yes"}],["$","td",null,{}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"Qwen3-Reranker-8B"}],["$","td",null,{"children":"8B"}],["$","td",null,{"children":"36"}],["$","td",null,{"children":"32K"}],["$","td",null,{"children":"-"}],["$","td",null,{"children":"-"}],["$","td",null,{"children":"Yes"}],["$","td",null,{}]]}]]}]]}]}]
12:["$","p",null,{"children":["$","em",null,{"children":"Note: “MRL Support” indicates whether the embedding model supports custom dimensions for the final embedding. “Instruction Aware” notes whether the embedding or reranking model supports customizing the input instruction according to different tasks."}]}]
13:["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"model-architecture","children":[["$","a",null,{"data-card":"","href":"#model-architecture","className":"peer","children":"Model Architecture"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
14:["$","p",null,{"children":["Based on the Qwen3 foundation model, our Embedding and Reranking models are designed using dual-encoder and cross-encoder architectures. Through LoRA fine-tuning, we aim to fully preserve and enhance the text understanding capabilities of the base model. The Embedding model processes a single text segment as input, extracting the semantic representation by utilizing the hidden state vector corresponding to the final ",["$","code",null,{"children":"[EOS]"}]," token. In contrast, the Reranking model takes text pairs (such as user queries and candidate documents) as input, calculating and outputting a relevance score between the pairs using a cross-encoder structure."]}]
15:["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"model-training","children":[["$","a",null,{"data-card":"","href":"#model-training","className":"peer","children":"Model Training"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
1e:T561,The training framework for the Qwen3 Embedding series follows the multi-stage training paradigm established by the GTE-Qwen series. During the training of the Embedding model, we implemented a three-stage training structure: the first stage involves contrastive pre-training with a large volume of weakly supervised data; the second stage focuses on supervised training using high-quality labeled data; and the final stage integrates multiple candidate models through a merging strategy to enhance overall performance. This staged training mechanism effectively balances the model’s generalization ability and task adaptability. For the Reranking model, based on empirical validation results, we directly employed high-quality labeled data for supervised training, significantly improving training efficiency. Notably, during the first stage of weakly supervised training for the Embedding model, we developed an innovative multi-task adaptable prompt system. By leveraging the text generation capabilities of the Qwen3 foundation model, we dynamically generated weakly supervised text pairs tailored to different task types and languages. This approach addressed the limitations of traditional methods, which often relied on community forums or open-source data for text relevance pair collection, facilitating the efficient generation of large-scale weakly supervised data.16:["$","p",null,{"children":"$1e"}]
17:["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"future-work","children":[["$","a",null,{"data-card":"","href":"#future-work","className":"peer","children":"Future work"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
18:["$","p",null,{"children":"The Qwen3 Embedding series models represent a new starting point. Through ongoing optimizations of the Qwen foundation model, we will enhance the training efficiency of text embeddings and reranking models, thereby improving deployment performance across various scenarios. Additionally, we plan to expand our multimodal representation system to establish cross-modal semantic understanding capabilities. We look forward to seeing more developers explore a wider range of scenarios based on the Qwen3 Embedding series, driving deeper applications of the model across diverse contexts."}]
19:["$","script","script-0",{"src":"/_next/static/chunks/8de849ca74fc071f.js","async":true}]
1a:["$","script","script-1",{"src":"/_next/static/chunks/e62b91212ee7f8ff.js","async":true}]
1b:["$","script","script-2",{"src":"/_next/static/chunks/f19fe44237e54646.js","async":true}]
1c:["$","script","script-3",{"src":"/_next/static/chunks/cb0a883bafeb6805.js","async":true}]
1d:["$","$L1f",null,{"children":["$","$20",null,{"name":"Next.MetadataOutlet","children":"$@21"}]}]
21:null
