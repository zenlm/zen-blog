<!DOCTYPE html><!--o_J3cFnAZ1mdL_cv2bxKY--><html lang="en" class="dark"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/chunks/f2332aac77592f9d.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/e83606e8fa9cc796.js"/><script src="/_next/static/chunks/36d6595f0156cd7e.js" async=""></script><script src="/_next/static/chunks/040e9cea20a8d9c7.js" async=""></script><script src="/_next/static/chunks/c19fcbf6bf086438.js" async=""></script><script src="/_next/static/chunks/turbopack-7419f7f4f6b062de.js" async=""></script><script src="/_next/static/chunks/59d0ad1b64f8544e.js" async=""></script><script src="/_next/static/chunks/4d80e004cf4896dd.js" async=""></script><script src="/_next/static/chunks/350ee4303b732916.js" async=""></script><script src="/_next/static/chunks/36bfed0236ce2cf2.js" async=""></script><script src="/_next/static/chunks/e62b91212ee7f8ff.js" async=""></script><script src="/_next/static/chunks/2a98816c7d26bf58.js" async=""></script><script src="/_next/static/chunks/cb0a883bafeb6805.js" async=""></script><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#0A0A0A"/><meta name="theme-color" media="(prefers-color-scheme: light)" content="#fff"/><title>Hello zen — Zen LM Blog | Zen LM</title><meta name="description" content="GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD"/><meta property="og:title" content="Zen LM - Open Foundation Models"/><meta property="og:description" content="Zen AI model family by Hanzo AI. 14 frontier models from 4B to 1T+ parameters for code, reasoning, vision, and multimodal tasks."/><meta property="og:url" content="https://zenlm.org"/><meta property="og:site_name" content="Zen LM"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="@zenlmorg"/><meta name="twitter:creator" content="@zenlmorg"/><meta name="twitter:title" content="Zen LM - Open Foundation Models"/><meta name="twitter:description" content="Zen AI model family by Hanzo AI. 14 frontier models from 4B to 1T+ parameters for code, reasoning, vision, and multimodal tasks."/><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="antialiased"><div hidden=""><!--$--><!--/$--></div><script>((a,b,c,d,e,f,g,h)=>{let i=document.documentElement,j=["light","dark"];function k(b){var c;(Array.isArray(a)?a:[a]).forEach(a=>{let c="class"===a,d=c&&f?e.map(a=>f[a]||a):e;c?(i.classList.remove(...d),i.classList.add(f&&f[b]?f[b]:b)):i.setAttribute(a,b)}),c=b,h&&j.includes(c)&&(i.style.colorScheme=c)}if(d)k(d);else try{let a=localStorage.getItem(b)||c,d=g&&"system"===a?window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":a;k(d)}catch(a){}})("class","theme","system",null,["light","dark"],null,true,true)</script><!--$--><div data-closed="" role="presentation" hidden="" style="user-select:none;-webkit-user-select:none" class="fixed inset-0 z-50 backdrop-blur-xs bg-fd-overlay data-open:animate-fd-fade-in data-closed:animate-fd-fade-out"></div><div class="bg-fd-secondary/50 p-3 empty:hidden"></div><!--/$--><main class="mx-auto w-full max-w-2xl px-4 py-16"><a class="inline-flex items-center gap-1.5 text-sm text-fd-muted-foreground hover:text-fd-foreground mb-10 transition-colors" href="/blog">← Back to Blog</a><div class="mb-8"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">June 6, 2024</time><h1 class="text-3xl font-bold mt-2 mb-3">Hello zen</h1><p class="text-fd-muted-foreground text-lg mb-4">GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD</p><div class="flex items-center gap-3 pt-4 border-t border-fd-border"><span class="text-sm text-fd-muted-foreground">By <!-- -->Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></div><div class="prose dark:prose-invert max-w-none"><p><a href="https://github.com/QwenLM/zen" rel="noreferrer noopener" target="_blank">GITHUB</a> <a href="https://huggingface.co/Qwen" rel="noreferrer noopener" target="_blank">HUGGING FACE</a> <a href="https://modelscope.cn/organization/qwen" rel="noreferrer noopener" target="_blank">MODELSCOPE</a> <a href="https://huggingface.co/spaces/Qwen/zen-72B-Instruct" rel="noreferrer noopener" target="_blank">DEMO</a> <a href="https://discord.gg/yPEP2vHTu4" rel="noreferrer noopener" target="_blank">DISCORD</a></p>
<h1 class="flex scroll-m-28 flex-row items-center gap-2" id="introduction"><a data-card="" href="#introduction" class="peer">Introduction</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h1>
<p>After months of efforts, we are pleased to announce the evolution from Qwen1.5 to zen. This time, we bring to you:</p>
<ul>
<li>Pretrained and instruction-tuned models of 5 sizes, including zen-0.5B, zen-1.5B, zen-7B, zen7B-A14B, and <strong>zen-72B</strong> ;</li>
<li>Having been trained on data in <strong>27</strong> additional languages besides English and Chinese;</li>
<li>State-of-the-art performance in a large number of benchmark evaluations;</li>
<li>Significantly improved performance in coding and mathematics;</li>
<li>Extended context length support up to <strong>128K</strong> tokens with zen-7B-Instruct and zen-72B-Instruct.</li>
</ul>
<p>We have opensourced the models in Hugging Face and ModelScope to you and we are looking forward to hearing from you!</p>
<h2 class="flex scroll-m-28 flex-row items-center gap-2" id="model-information"><a data-card="" href="#model-information" class="peer">Model Information</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>The zen series include base and instruction-tuned models of 5 sizes, including zen-0.5B, zen-1.5B, zen-7B, zen7B-A14B, zen-72B. We illustrate the key information of the models in the following table:</p>











<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Models</th><th>zen-0.5B</th><th>zen-1.5B</th><th>zen-7B</th><th>zen7B-A14B</th><th>zen-72B</th></tr></thead></table></div>
<h1 class="flex scroll-m-28 flex-row items-center gap-2" id="params-049b-154b-707b-5741b-7271b"><a data-card="" href="#params-049b-154b-707b-5741b-7271b" class="peer">Params| 0.49B| 1.54B| 7.07B| 57.41B| 72.71B</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h1>
<h1 class="flex scroll-m-28 flex-row items-center gap-2" id="non-emb-params-035b-131b-598b-5632b-7021b"><a data-card="" href="#non-emb-params-035b-131b-598b-5632b-7021b" class="peer">Non-Emb Params| 0.35B| 1.31B| 5.98B| 56.32B| 70.21B</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h1>
<p>GQA| True| True| True| True| True<br/>
Tie Embedding| True| True| False| False| False<br/>
Context Length| 32K| 32K| 128K| 64K| 128K</p>
<p>Specifically, previously in Qwen1.5, only Qwen1.5-32B and Qwen1.5-110B have adopted Group Query Attention (GQA). This time, for all model sizes, we apply GQA so that they can enjoy the benefits of faster speed and less memory usage in model inference. For small models, we prefer the application of tying embedding as the large sparse embeddings take up a large proportion of the total model parameters.</p>
<p>In terms of the context length, all base language models have been pretrained on data of the context length of 32K tokens, and we observe satisfactory extrapolation capabilities up to 128K in PPL evaluation. However, for instruction-tuned models, we are not satisfied with merely PPL evaluation; we need the models to be capable of correctly understanding long context and completing tasks. In the table, we list the context length capabilities of instruction-tuned models, as assessed through the evaluation of the <a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack" rel="noreferrer noopener" target="_blank">Needle in a Haystack</a> task. Notably, when augmented with YARN, both zen-7B-Instruct and zen-72B-Instruct models demonstrate an impressive capacity to handle context lengths extending up to 128K tokens.</p>
<p>Significant efforts were directed towards augmenting both the volume and quality of pretraining and instruction-tuning datasets across a diverse linguistic spectrum, beyond English and Chinese, to bolster its multilingual competencies. Although large language models possess an inherent capacity to generalize to other languages, we explicitly highlight the inclusion of 27 additional languages in our training:</p>

































<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Regions</th><th>Languages</th></tr></thead><tbody><tr><td>Western Europe</td><td>German, French, Spanish, Portuguese, Italian, Dutch</td></tr><tr><td>Eastern &amp; Central Europe</td><td>Russian, Czech, Polish</td></tr><tr><td>Middle East</td><td>Arabic, Persian, Hebrew, Turkish</td></tr><tr><td>Eastern Asia</td><td>Japanese, Korean</td></tr><tr><td>South-Eastern Asia</td><td>Vietnamese, Thai, Indonesian, Malay, Lao, Burmese, Cebuano, Khmer, Tagalog</td></tr><tr><td>Southern Asia</td><td>Hindi, Bengali, Urdu</td></tr></tbody></table></div>
<p>Additionally, we have devoted significant effort to addressing code-switching, a frequent occurrence in multilingual evaluation. Consequently, our models’ proficiency in handling this phenomenon have notably enhanced. Evaluations using prompts that typically induce code-switching across languages confirm a substantial reduction in associated issues.</p>
<h1 class="flex scroll-m-28 flex-row items-center gap-2" id="performance"><a data-card="" href="#performance" class="peer">Performance</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h1>
<p>Comparative assessments reveal substantial enhancements in performance for large-scale models (70B+ parameters) relative to Qwen1.5. Here our evaluation centers on the large-size model zen-72B. In terms of base language models, zen-72B and state-of-the-art open models are evaluated for different capbilities including natural language understanding, knowledge acquisition, coding proficiency, mathematical skills, and multilingual abilities. Benefiting from meticulously curated datasets and optimized training methods, zen-72B exhibits superior performance compared to leading models such as Llama-3-70B. Notably, it surpasses the performance of its predecessor, Qwen1.5-110B, despite having fewer parameters.</p>
<p>After extensive large-scale pre-training, we conduct post-training to further enhance Qwen’s intelligence, bringing it closer to human. This process further improves the model’s capabilities in areas such as coding, mathematics, reasoning, instruction following, multilingual understanding, and more. Additionally, it aligns the model’s output with human values, ensuring that it is helpful, honest, and harmless. Our post-training phase is designed with the principle of scalable training with minimal human annotation. Specifically, we investigate how to obtain high-quality, reliable, diverse and creative demonstration data and preference data with various automated alignment strategies, such as <a href="https://arxiv.org/pdf/2308.01825" rel="noreferrer noopener" target="_blank">rejection sampling</a> for math, execution feedback for coding and instruction-following, back-translation for creative writing, <a href="https://arxiv.org/pdf/2401.12474" rel="noreferrer noopener" target="_blank">scalable oversight</a> for role-play, etc. As for training, we apply a combination of supervised fine-tuning, reward model training and online DPO training. We also employ a novel <a href="https://arxiv.org/pdf/2405.17931" rel="noreferrer noopener" target="_blank">Online Merging Optimizer</a> to minimize the alignment tax. These collective efforts have significantly boosted the capabilities and intelligence of our models, as illustrated in the following table.</p>
<p>We comprehensively evaluate zen-72B-Instruct on 16 benchmarks across various domains. zen-72B-Instruct strikes a balance between obtaining better capabilities and aligning well with human values. Specifically, zen-72B-Instruct significantly surpasses Qwen1.5-72B-Chat across all benchmarks, and also reaches competitive performance compared with Llama-3-70B-Instruct.1</p>
<p>In terms of smaller models, our zen models also outcompete the SOTA models of similar or even larger sizes. In comparison with the very recently released SOTA models, zen-7B-Instruct can still demonstrate advantages across benchmarks, showing specifically outstanding performance on coding and Chinese-related metrics.1</p>
<h1 class="flex scroll-m-28 flex-row items-center gap-2" id="highlights"><a data-card="" href="#highlights" class="peer">Highlights</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h1>
<h2 class="flex scroll-m-28 flex-row items-center gap-2" id="coding--mathematics"><a data-card="" href="#coding--mathematics" class="peer">Coding &amp; Mathematics</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>We have persistently dedicated our efforts to enhance the advanced capabilities of Qwen, particularly in coding and mathematics. In coding, we have successfully integrated the code training experience and data from <a href="https://qwenlm.github.io/blog/codeqwen1.5/" rel="noreferrer noopener" target="_blank">CodeQwen1.5</a>, resulting in significant improvements in zen-72B-Instruct across various programming languages. Regarding mathematics, by exploiting the extensive and high-quality datasets, zen-72B-Instruct has reflects stronger capabilities in solving mathematic problems.</p>
<h2 class="flex scroll-m-28 flex-row items-center gap-2" id="long-context-understanding"><a data-card="" href="#long-context-understanding" class="peer">Long Context Understanding</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>In zen, all instruction-tuned models have been trained on 32k length contexts, and extrapolated to longer context lengths using techniques like <a href="https://arxiv.org/abs/2309.00071" rel="noreferrer noopener" target="_blank">YARN</a> or <a href="https://arxiv.org/abs/2402.17463" rel="noreferrer noopener" target="_blank">Dual Chunk Attention</a>.</p>
<p>The figure below shows our test results on the <a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack" rel="noreferrer noopener" target="_blank">Needle in a Haystack</a>. Notably, zen-72B-Instruct is capable of flawlessly handling information extraction tasks within a 128k context. Coupled with its inherent strong performance, it becomes the preferred choice for handling long text tasks when resources are sufficient.</p>
<p>Additionally, it’s worth noting the impressive capabilities of other models in the series: zen-7B-Instruct nearly flawlessly handles contexts up to 128k in length, zen7B-A14B-Instruct manages contexts up to 64k, and the two smaller models in the lineup support contexts of 32k.</p>
<p>Alongside the long-context models, we have also open-sourced an agent solution for efficiently processing documents containing up to 1 million tokens. For more details, see <a href="https://qwenlm.github.io/blog/qwen-agent-2405/" rel="noreferrer noopener" target="_blank">our dedicated blog post on this topic</a>.</p>
<h2 class="flex scroll-m-28 flex-row items-center gap-2" id="safety-and-responsibility"><a data-card="" href="#safety-and-responsibility" class="peer">Safety and Responsibility</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>The table below presents the proportion of harmful responses generated by large models for four categories of multilingual unsafe querys(Illegal Activity, Fraud, Pornography, Privacy Violence). The test data was derived from <a href="https://github.com/verazuo/jailbreak_llms/tree/main" rel="noreferrer noopener" target="_blank">Jailbreak</a> and translated into multiple languages for evaluation. We find that Llama-3 does not effectively handle multilingual prompts, and therefore, it is not included in the comparison. Through significance testing (P_value), we found that the zen-72B-Instruct model performs comparably to GPT-4 in terms of safety, and significantly outperforms the Mistral-8x22B model.</p>
<p>Language| | Illegal Activity| | | Fraud| | | Pornography| | | Privacy Violence|<br/>
---|---|---|---|---|---|---|---|---|---|---|---|---<br/>
| GPT-4| Mistral-8x22B| zen-72B-Instruct| GPT-4| Mistral-8x22B| zen-72B-Instruct| GPT-4| Mistral-8x22B| zen-72B-Instruct| GPT-4| Mistral-8x22B| zen-72B-Instruct<br/>
zh| <strong>0%</strong>|  13%| <strong>0%</strong>| <strong>0%</strong>|  17%| <strong>0%</strong>| <strong>43%</strong>|  47%| 53%| <strong>0%</strong>|  10%| <strong>0%</strong><br/>
en| <strong>0%</strong>|  7%| <strong>0%</strong>| <strong>0%</strong>|  23%| <strong>0%</strong>| <strong>37%</strong>|  67%| 63%| <strong>0%</strong>|  27%| 3%<br/>
ar| <strong>0%</strong>|  13%| <strong>0%</strong>| <strong>0%</strong>|  7%| <strong>0%</strong>| <strong>15%</strong>|  26%| <strong>15%</strong>|  3%| 13%| <strong>0%</strong><br/>
es| <strong>0%</strong>|  7%| <strong>0%</strong>|  3%| <strong>0%</strong>| <strong>0%</strong>| <strong>48%</strong>|  64%| 50%| <strong>3%</strong>|  7%| <strong>3%</strong><br/>
fr| <strong>0%</strong>|  3%| <strong>0%</strong>| <strong>3%</strong>| <strong>3%</strong>|  7%| <strong>3%</strong>|  19%| 7%| <strong>0%</strong>|  27%| <strong>0%</strong><br/>
ko| <strong>0%</strong>|  4%| <strong>0%</strong>| <strong>3%</strong>|  8%| 4%| 17%| 29%| <strong>10%</strong>| <strong>0%</strong>|  26%| 4%<br/>
pt| <strong>0%</strong>|  7%| <strong>0%</strong>| <strong>3%</strong>|  7%| <strong>3%</strong>| <strong>47%</strong>|  57%| <strong>47%</strong>| <strong>4%</strong>|  26%| <strong>4%</strong><br/>
th| <strong>0%</strong>|  10%| <strong>0%</strong>|  7%| 23%| <strong>3%</strong>|  13%| 17%| <strong>10%</strong>|  13%| <strong>7%</strong>| <strong>7%</strong><br/>
vi| <strong>0%</strong>|  4%| <strong>0%</strong>|  4%| 11%| <strong>0%</strong>| <strong>22%</strong>|  26%| <strong>22%</strong>| <strong>0%</strong>| <strong>0%</strong>| <strong>0%</strong><br/>
Average| <strong>0%</strong>|  8%| <strong>0%</strong>|  3%| 11%| <strong>2%</strong>| <strong>27%</strong>|  39%| 31%| 3%| 16%| <strong>2%</strong></p>
<h1 class="flex scroll-m-28 flex-row items-center gap-2" id="developing-with-zen"><a data-card="" href="#developing-with-zen" class="peer">Developing with zen</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h1>
<p>Now all models have been released in Hugging Face and ModelScope. Feel free to visit the model cards for detailed usages, and learn more information about each model, including its features, performance, etc.</p>
<p>For a long time, a lot of friends have been supporting the development of Qwen, including finetuning (<a href="https://github.com/OpenAccess-AI-Collective/axolotl" rel="noreferrer noopener" target="_blank">Axolotl</a>, <a href="https://github.com/hiyouga/LLaMA-Factory" rel="noreferrer noopener" target="_blank">Llama-Factory</a>, <a href="https://github.com/yangjianxin1/Firefly" rel="noreferrer noopener" target="_blank">Firefly</a>, <a href="https://github.com/modelscope/swift" rel="noreferrer noopener" target="_blank">Swift</a>, <a href="https://github.com/InternLM/xtuner" rel="noreferrer noopener" target="_blank">XTuner</a>), quantization (<a href="https://github.com/AutoGPTQ/AutoGPTQ" rel="noreferrer noopener" target="_blank">AutoGPTQ</a>, <a href="https://github.com/casper-hansen/AutoAWQ" rel="noreferrer noopener" target="_blank">AutoAWQ</a>, <a href="https://github.com/intel/neural-compressor" rel="noreferrer noopener" target="_blank">Neural Compressor</a>), deployment (<a href="https://github.com/vllm-project/vllm" rel="noreferrer noopener" target="_blank">vLLM</a>, <a href="https://github.com/sgl-project/sglang" rel="noreferrer noopener" target="_blank">SGL</a>, <a href="https://github.com/skypilot-org/skypilot" rel="noreferrer noopener" target="_blank">SkyPilot</a>, <a href="https://github.com/NVIDIA/TensorRT-LLM" rel="noreferrer noopener" target="_blank">TensorRT-LLM</a>, <a href="https://github.com/openvinotoolkit/openvino" rel="noreferrer noopener" target="_blank">OpenVino</a>, <a href="https://github.com/huggingface/text-generation-inference" rel="noreferrer noopener" target="_blank">TGI</a>), API platforms (<a href="https://www.together.ai/" rel="noreferrer noopener" target="_blank">Together</a>, <a href="https://fireworks.ai/" rel="noreferrer noopener" target="_blank">Fireworks</a>, <a href="https://openrouter.ai/" rel="noreferrer noopener" target="_blank">OpenRouter</a>), local run (<a href="https://github.com/ml-explore/mlx" rel="noreferrer noopener" target="_blank">MLX</a>, <a href="https://github.com/ggerganov/llama.cpp" rel="noreferrer noopener" target="_blank">Llama.cpp</a>, <a href="https://ollama.com/" rel="noreferrer noopener" target="_blank">Ollama</a>, <a href="https://lmstudio.ai/" rel="noreferrer noopener" target="_blank">LM Studio</a>), Agent and RAG Frameworks (<a href="https://www.llamaindex.ai/" rel="noreferrer noopener" target="_blank">LlamaIndex</a>, <a href="https://www.crewai.com/" rel="noreferrer noopener" target="_blank">CrewAI</a>, <a href="https://github.com/OpenDevin/OpenDevin/" rel="noreferrer noopener" target="_blank">OpenDevin</a>) , Evaluation (<a href="https://chat.lmsys.org/" rel="noreferrer noopener" target="_blank">LMSys</a>, <a href="https://opencompass.org.cn/home" rel="noreferrer noopener" target="_blank">OpenCompass</a>, <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" rel="noreferrer noopener" target="_blank">Open LLM Leaderboard</a>), model training (<a href="https://huggingface.co/cognitivecomputations" rel="noreferrer noopener" target="_blank">Dolphin</a>, <a href="https://github.com/OpenBuddy/OpenBuddy" rel="noreferrer noopener" target="_blank">Openbuddy</a>) etc. For how to use zen with the third-party frameworks, please refer to the respective documentation as well as our <a href="https://qwen.readthedocs.io/en/latest/" rel="noreferrer noopener" target="_blank">official documentation</a>.</p>
<p>Still there are a number of teams and people not mentioned that have made contributions to Qwen. We sincerely thank them for the support, and we hope that our collaboration can boost the research and development of the opensource AI community.</p>
<h1 class="flex scroll-m-28 flex-row items-center gap-2" id="license"><a data-card="" href="#license" class="peer">License</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h1>
<p>This time, we change the licenses of our models to different ones. While zen-72B as well as its instruction-tuned models still uses the original Qianwen License, all other models, including zen-0.5B, zen-1.5B, zen-7B, and zen7B-A14B, turn to adopt <strong>Apache 2.0</strong>! We believe that the enhanced openness of our models to the community can accelerate the applications and commercial usages of zen all around the world.</p>
<h1 class="flex scroll-m-28 flex-row items-center gap-2" id="whats-next-for-zen"><a data-card="" href="#whats-next-for-zen" class="peer">What’s Next for zen?</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h1>
<p>We are training larger zen models to further explore model scaling along with our recent data scaling. Additionally, we extend the zen language models to multimodal, capable of understanding both vision and audio information. In the near future, we will continue opensource new models to accelerate opensource AI. Stay tuned!</p>
<h1 class="flex scroll-m-28 flex-row items-center gap-2" id="citation"><a data-card="" href="#citation" class="peer">Citation</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h1>
<p>If you find our work helpful, feel free to give us a cite!</p>
<figure dir="ltr" class="my-4 bg-fd-card rounded-xl shiki relative border shadow-sm not-prose overflow-hidden text-sm shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e" tabindex="-1"><div class="empty:hidden absolute top-3 right-2 z-2 backdrop-blur-lg rounded-lg text-fd-muted-foreground"><button type="button" class="inline-flex items-center justify-center rounded-md text-sm font-medium transition-colors duration-100 disabled:pointer-events-none disabled:opacity-50 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-fd-ring p-1 [&amp;_svg]:size-4 hover:text-fd-accent-foreground data-checked:text-fd-accent-foreground" aria-label="Copy Text"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-clipboard" aria-hidden="true"><rect width="8" height="4" x="8" y="2" rx="1" ry="1"></rect><path d="M16 4h2a2 2 0 0 1 2 2v14a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V6a2 2 0 0 1 2-2h2"></path></svg></button></div><div role="region" tabindex="0" class="text-[0.8125rem] py-3.5 overflow-auto max-h-[600px] fd-scroll-container focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-inset focus-visible:ring-fd-ring" style="--padding-right:calc(var(--spacing) * 8)"><pre class="min-w-full w-max *:flex *:flex-col"><code><span class="line"><span></span></span>
<span class="line"><span>    @article{qwen2,</span></span>
<span class="line"><span>          title={zen Technical Report}, </span></span>
<span class="line"><span>          author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},</span></span>
<span class="line"><span>          journal={arXiv preprint arXiv:2407.10671},</span></span>
<span class="line"><span>          year={2024}</span></span>
<span class="line"><span>    }</span></span>
<span class="line"><span>    </span></span></code></pre></div></figure>
<h1 class="flex scroll-m-28 flex-row items-center gap-2" id="appendix"><a data-card="" href="#appendix" class="peer">Appendix</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h1>
<h2 class="flex scroll-m-28 flex-row items-center gap-2" id="base-language-model-evaluation"><a data-card="" href="#base-language-model-evaluation" class="peer">Base Language Model Evaluation</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>The evaluation of base models mainly focuses on the model performance of natural language understanding, general question answering, coding, mathematics, scientific knowledge, reasoning, multilingual capability, etc.</p>
<p>The datasets for evaluation include:</p>
<p><strong>English Tasks</strong> : MMLU (5-shot), MMLU-Pro (5-shot), GPQA (5shot), Theorem QA (5-shot), BBH (3-shot), HellaSwag (10-shot), Winogrande (5-shot), TruthfulQA (0-shot), ARC-C (25-shot)</p>
<p><strong>Coding Tasks</strong> : EvalPlus (0-shot) (HumanEval, MBPP, HumanEval+, MBPP+), MultiPL-E (0-shot) (Python, C++, JAVA, PHP, TypeScript, C#, Bash, JavaScript)</p>
<p><strong>Math Tasks</strong> : GSM8K (4-shot), MATH (4-shot)</p>
<p><strong>Chinese Tasks</strong> : C-Eval(5-shot), CMMLU (5-shot)</p>
<p><strong>Multilingual Tasks</strong> : Multi-Exam (M3Exam 5-shot, IndoMMLU 3-shot, ruMMLU 5-shot, mMMLU 5-shot), Multi-Understanding (BELEBELE 5-shot, XCOPA 5-shot, XWinograd 5-shot, XStoryCloze 0-shot, PAWS-X 5-shot), Multi-Mathematics (MGSM 8-shot), Multi-Translation (Flores-101 5-shot)</p>
<h3 class="flex scroll-m-28 flex-row items-center gap-2" id="zen-72b-performance"><a data-card="" href="#zen-72b-performance" class="peer">zen-72B performance</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>



















































































































































































































































































<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Datasets</th><th>DeepSeek-V2</th><th>Mixtral-8x22B</th><th>Llama-3-70B</th><th>Qwen1.5-72B</th><th>Qwen1.5-110B</th><th><strong>zen-72B</strong></th></tr></thead><tbody><tr><td>Architecture</td><td>MoE</td><td>MoE</td><td>Dense</td><td>Dense</td><td>Dense</td><td>Dense</td></tr><tr><td>#Activated Params</td><td>21B</td><td>39B</td><td>70B</td><td>72B</td><td>110B</td><td>72B</td></tr><tr><td>#Params</td><td>236B</td><td>140B</td><td>70B</td><td>72B</td><td>110B</td><td>72B</td></tr><tr><td><em><strong>English</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MMLU</td><td>78.5</td><td>77.8</td><td>79.5</td><td>77.5</td><td>80.4</td><td><strong>84.2</strong></td></tr><tr><td>MMLU-Pro</td><td>-</td><td>49.5</td><td>52.8</td><td>45.8</td><td>49.4</td><td><strong>55.6</strong></td></tr><tr><td>GPQA</td><td>-</td><td>34.3</td><td>36.3</td><td>36.3</td><td>35.9</td><td><strong>37.9</strong></td></tr><tr><td>Theorem QA</td><td>-</td><td>35.9</td><td>32.3</td><td>29.3</td><td>34.9</td><td><strong>43.1</strong></td></tr><tr><td>BBH</td><td>78.9</td><td>78.9</td><td>81.0</td><td>65.5</td><td>74.8</td><td><strong>82.4</strong></td></tr><tr><td>HellaSwag</td><td>87.8</td><td><strong>88.7</strong></td><td>88.0</td><td>86.0</td><td>87.5</td><td>87.6</td></tr><tr><td>WindoGrande</td><td>84.8</td><td>85.0</td><td><strong>85.3</strong></td><td>83.0</td><td>83.5</td><td>85.1</td></tr><tr><td>ARC-C</td><td>70.0</td><td><strong>70.7</strong></td><td>68.8</td><td>65.9</td><td>69.6</td><td>68.9</td></tr><tr><td>TruthfulQA</td><td>42.2</td><td>51.0</td><td>45.6</td><td><strong>59.6</strong></td><td>49.6</td><td>54.8</td></tr><tr><td><em><strong>Coding</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>HumanEval</td><td>45.7</td><td>46.3</td><td>48.2</td><td>46.3</td><td>54.3</td><td><strong>64.6</strong></td></tr><tr><td>MBPP</td><td>73.9</td><td>71.7</td><td>70.4</td><td>66.9</td><td>70.9</td><td><strong>76.9</strong></td></tr><tr><td>EvalPlus</td><td>55.0</td><td>54.1</td><td>54.8</td><td>52.9</td><td>57.7</td><td><strong>65.4</strong></td></tr><tr><td>MultiPL-E</td><td>44.4</td><td>46.7</td><td>46.3</td><td>41.8</td><td>52.7</td><td><strong>59.6</strong></td></tr><tr><td><em><strong>Mathematics</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GSM8K</td><td>79.2</td><td>83.7</td><td>83.0</td><td>79.5</td><td>85.4</td><td><strong>89.5</strong></td></tr><tr><td>MATH</td><td>43.6</td><td>41.7</td><td>42.5</td><td>34.1</td><td>49.6</td><td><strong>51.1</strong></td></tr><tr><td><em><strong>Chinese</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>C-Eval</td><td>81.7</td><td>54.6</td><td>65.2</td><td>84.1</td><td>89.1</td><td><strong>91.0</strong></td></tr><tr><td>CMMLU</td><td>84.0</td><td>53.4</td><td>67.2</td><td>83.5</td><td>88.3</td><td><strong>90.1</strong></td></tr><tr><td><em><strong>Multilingual</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Mulit-Exam</td><td>67.5</td><td>63.5</td><td>70.0</td><td>66.4</td><td>75.6</td><td><strong>76.6</strong></td></tr><tr><td>Multi-Understanding</td><td>77.0</td><td>77.7</td><td>79.9</td><td>78.2</td><td>78.2</td><td><strong>80.7</strong></td></tr><tr><td>Multi-Mathematics</td><td>58.8</td><td>62.9</td><td>67.1</td><td>61.7</td><td>64.4</td><td><strong>76.0</strong></td></tr><tr><td>Multi-Translation</td><td>36.0</td><td>23.3</td><td><strong>38.0</strong></td><td>35.6</td><td>36.2</td><td>37.8</td></tr></tbody></table></div>
<h3 class="flex scroll-m-28 flex-row items-center gap-2" id="zen7b-a14b"><a data-card="" href="#zen7b-a14b" class="peer">zen7B-A14B</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>





















































































































































































































































<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Datasets</th><th>Jamba</th><th>Mixtral-8x7B</th><th>Yi-1.5-34B</th><th>Qwen1.5-32B</th><th><strong><strong>zen7B-A14B</strong></strong></th></tr></thead><tbody><tr><td>Architecture</td><td>MoE</td><td>MoE</td><td>Dense</td><td>Dense</td><td>MoE</td></tr><tr><td>#Activated Params</td><td>12B</td><td>12B</td><td>34B</td><td>32B</td><td>14B</td></tr><tr><td>#Params</td><td>52B</td><td>47B</td><td>34B</td><td>32B</td><td>57B</td></tr><tr><td><em><strong>English</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MMLU</td><td>67.4</td><td>71.8</td><td><strong>77.1</strong></td><td>74.3</td><td>76.5</td></tr><tr><td>MMLU-Pro</td><td>-</td><td>41.0</td><td><strong>48.3</strong></td><td>44.0</td><td>43.0</td></tr><tr><td>GPQA</td><td>-</td><td>29.2</td><td>-</td><td>30.8</td><td><strong>34.3</strong></td></tr><tr><td>Theorem QA</td><td>-</td><td>23.2</td><td>-</td><td>28.8</td><td><strong>33.5</strong></td></tr><tr><td>BBH</td><td>45.4</td><td>50.3</td><td><strong>76.4</strong></td><td>66.8</td><td>67.0</td></tr><tr><td>HellaSwag</td><td><strong>87.1</strong></td><td>86.5</td><td>85.9</td><td>85.0</td><td>85.2</td></tr><tr><td>Winogrande</td><td>82.5</td><td>81.9</td><td><strong>84.9</strong></td><td>81.5</td><td>79.5</td></tr><tr><td>ARC-C</td><td>64.4</td><td><strong>66.0</strong></td><td>65.6</td><td>63.6</td><td>64.1</td></tr><tr><td>TruthfulQA</td><td>46.4</td><td>51.1</td><td>53.9</td><td>57.4</td><td><strong>57.7</strong></td></tr><tr><td><em><strong>Coding</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>HumanEval</td><td>29.3</td><td>37.2</td><td>46.3</td><td>43.3</td><td><strong>53.0</strong></td></tr><tr><td>MBPP</td><td>-</td><td>63.9</td><td>65.5</td><td>64.2</td><td><strong>71.9</strong></td></tr><tr><td>EvalPlus</td><td>-</td><td>46.4</td><td>51.9</td><td>50.4</td><td><strong>57.2</strong></td></tr><tr><td>MultiPL-E</td><td>-</td><td>39.0</td><td>39.5</td><td>38.5</td><td><strong>49.8</strong></td></tr><tr><td><em><strong>Mathematics</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GSM8K</td><td>59.9</td><td>62.5</td><td><strong>82.7</strong></td><td>76.8</td><td>80.7</td></tr><tr><td>MATH</td><td>-</td><td>30.8</td><td>41.7</td><td>36.1</td><td><strong>43.0</strong></td></tr><tr><td><em><strong>Chinese</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>C-Eval</td><td>-</td><td>-</td><td>-</td><td>83.5</td><td><strong>87.7</strong></td></tr><tr><td>CMMLU</td><td>-</td><td>-</td><td>84.8</td><td>82.3</td><td><strong>88.5</strong></td></tr><tr><td><em><strong>Multilingual</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Multi-Exam</td><td>-</td><td>56.1</td><td>58.3</td><td>61.6</td><td><strong>65.5</strong></td></tr><tr><td>Multi-Understanding</td><td>-</td><td>70.7</td><td>73.9</td><td>76.5</td><td><strong>77.0</strong></td></tr><tr><td>Multi-Mathematics</td><td>-</td><td>45.0</td><td>49.3</td><td>56.1</td><td><strong>62.3</strong></td></tr><tr><td>Multi-Translation</td><td>-</td><td>29.8</td><td>30.0</td><td>33.5</td><td><strong>34.5</strong></td></tr></tbody></table></div>
<h3 class="flex scroll-m-28 flex-row items-center gap-2" id="zen-7b"><a data-card="" href="#zen-7b" class="peer">zen-7B</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>











<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Datasets</th><th>Mistral-7B</th><th>Gemma-7B</th><th>Llama-3-8B</th><th>Qwen1.5-7B</th><th>zen-7B</th></tr></thead></table></div>
<h1 class="flex scroll-m-28 flex-row items-center gap-2" id="params-72b-85b-80b-77b-76b"><a data-card="" href="#params-72b-85b-80b-77b-76b" class="peer">Params| 7.2B| 8.5B| 8.0B| 7.7B| 7.6B</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h1>
<h1 class="flex scroll-m-28 flex-row items-center gap-2" id="non-emb-params-70b-78b-70b-65b-65b"><a data-card="" href="#non-emb-params-70b-78b-70b-65b-65b" class="peer">Non-emb Params| 7.0B| 7.8B| 7.0B| 6.5B| 6.5B</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h1>
<p><em><strong>English</strong></em>| | | | |<br/>
MMLU| 64.2| 64.6| 66.6| 61.0| <strong>70.3</strong><br/>
MMLU-Pro| 30.9| 33.7| 35.4| 29.9| <strong>40.0</strong><br/>
GPQA| 24.7| 25.7| 25.8| 26.7| <strong>31.8</strong><br/>
Theorem QA| 19.2| 21.5| 22.1| 14.2| <strong>31.1</strong><br/>
BBH| 56.1| 55.1| 57.7| 40.2| <strong>62.6</strong><br/>
HellaSwag| <strong>83.2</strong>|  82.2| 82.1| 78.5| 80.7<br/>
Winogrande| 78.4| <strong>79.0</strong>|  77.4| 71.3| 77.0<br/>
ARC-C| 60.0| <strong>61.1</strong>|  59.3| 54.2| 60.6<br/>
TruthfulQA| 42.2| 44.8| 44.0| 51.1| <strong>54.2</strong><br/>
<em><strong>Coding</strong></em>| | | | |<br/>
HumanEval| 29.3| 37.2| 33.5| 36.0| <strong>51.2</strong><br/>
MBPP| 51.1| 50.6| 53.9| 51.6| <strong>65.9</strong><br/>
EvalPlus| 36.4| 39.6| 40.3| 40.0| <strong>54.2</strong><br/>
MultiPL-E| 29.4| 29.7| 22.6| 28.1| <strong>46.3</strong><br/>
<em><strong>Mathematics</strong></em>| | | | |<br/>
GSM8K| 52.2| 46.4| 56.0| 62.5| <strong>79.9</strong><br/>
MATH| 13.1| 24.3| 20.5| 20.3| <strong>44.2</strong><br/>
<em><strong>Chinese</strong></em>| | | | |<br/>
C-Eval| 47.4| 43.6| 49.5| 74.1| <strong>83.2</strong><br/>
CMMLU| -| -| 50.8| 73.1| <strong>83.9</strong><br/>
<em><strong>Multilingual</strong></em>| | | | |<br/>
Multi-Exam| 47.1| 42.7| 52.3| 47.7| <strong>59.2</strong><br/>
Multi-Understanding| 63.3| 58.3| 68.6| 67.6| <strong>72.0</strong><br/>
Multi-Mathematics| 26.3| 39.1| 36.3| 37.3| <strong>57.5</strong><br/>
Multi-Translation| 23.3| 31.2| <strong>31.9</strong>|  28.4| 31.5</p>
<h3 class="flex scroll-m-28 flex-row items-center gap-2" id="zen-05b--zen-15b"><a data-card="" href="#zen-05b--zen-15b" class="peer">zen-0.5B &amp; zen-1.5B</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>





















































































































































<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Datasets</th><th>Phi-2</th><th>Gemma-2B</th><th>MiniCPM</th><th>Qwen1.5-1.8B</th><th>zen-0.5B</th><th>zen-1.5B</th></tr></thead><tbody><tr><td>#Non-Emb Params</td><td>2.5B</td><td>2.0B</td><td>2.4B</td><td>1.3B</td><td>0.35B</td><td>1.3B</td></tr><tr><td>MMLU</td><td>52.7</td><td>42.3</td><td>53.5</td><td>46.8</td><td>45.4</td><td><strong>56.5</strong></td></tr><tr><td>MMLU-Pro</td><td>-</td><td>15.9</td><td>-</td><td>-</td><td>14.7</td><td>21.8</td></tr><tr><td>Theorem QA</td><td>-</td><td>-</td><td>-</td><td>-</td><td>8.9</td><td><strong>15.0</strong></td></tr><tr><td>HumanEval</td><td>47.6</td><td>22.0</td><td><strong>50.0</strong></td><td>20.1</td><td>22.0</td><td>31.1</td></tr><tr><td>MBPP</td><td><strong>55.0</strong></td><td>29.2</td><td>47.3</td><td>18.0</td><td>22.0</td><td>37.4</td></tr><tr><td>GSM8K</td><td>57.2</td><td>17.7</td><td>53.8</td><td>38.4</td><td>36.5</td><td><strong>58.5</strong></td></tr><tr><td>MATH</td><td>3.5</td><td>11.8</td><td>10.2</td><td>10.1</td><td>10.7</td><td><strong>21.7</strong></td></tr><tr><td>BBH</td><td><strong>43.4</strong></td><td>35.2</td><td>36.9</td><td>24.2</td><td>28.4</td><td>37.2</td></tr><tr><td>HellaSwag</td><td><strong>73.1</strong></td><td>71.4</td><td>68.3</td><td>61.4</td><td>49.3</td><td>66.6</td></tr><tr><td>Winogrande</td><td><strong>74.4</strong></td><td>66.8</td><td>-</td><td>60.3</td><td>56.8</td><td>66.2</td></tr><tr><td>ARC-C</td><td><strong>61.1</strong></td><td>48.5</td><td>-</td><td>37.9</td><td>31.5</td><td>43.9</td></tr><tr><td>TruthfulQA</td><td>44.5</td><td>33.1</td><td>-</td><td>39.4</td><td>39.7</td><td><strong>45.9</strong></td></tr><tr><td>C-Eval</td><td>23.4</td><td>28.0</td><td>51.1</td><td>59.7</td><td>58.2</td><td><strong>70.6</strong></td></tr><tr><td>CMMLU</td><td>24.2</td><td>-</td><td>51.1</td><td>57.8</td><td>55.1</td><td><strong>70.3</strong></td></tr></tbody></table></div>
<h2 class="flex scroll-m-28 flex-row items-center gap-2" id="instruction-tuned-model-evaluation1"><a data-card="" href="#instruction-tuned-model-evaluation1" class="peer">Instruction-tuned Model Evaluation1</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<h3 class="flex scroll-m-28 flex-row items-center gap-2" id="zen-72b-instruct"><a data-card="" href="#zen-72b-instruct" class="peer">zen-72B-Instruct</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>



































































































































<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Datasets</th><th>Llama-3-70B-Instruct</th><th>Qwen1.5-72B-Chat</th><th><strong>zen-72B-Instruct</strong></th></tr></thead><tbody><tr><td><em><strong>English</strong></em></td><td></td><td></td><td></td></tr><tr><td>MMLU</td><td>82.0</td><td>75.6</td><td><strong>82.3</strong></td></tr><tr><td>MMLU-Pro</td><td>56.2</td><td>51.7</td><td><strong>64.4</strong></td></tr><tr><td>GPQA</td><td>41.9</td><td>39.4</td><td><strong>42.4</strong></td></tr><tr><td>TheroemQA</td><td>42.5</td><td>28.8</td><td><strong>44.4</strong></td></tr><tr><td>MT-Bench</td><td>8.95</td><td>8.61</td><td><strong>9.12</strong></td></tr><tr><td>Arena-Hard</td><td>41.1</td><td>36.1</td><td><strong>48.1</strong></td></tr><tr><td>IFEval (Prompt Strict-Acc.)</td><td>77.3</td><td>55.8</td><td><strong>77.6</strong></td></tr><tr><td><em><strong>Coding</strong></em></td><td></td><td></td><td></td></tr><tr><td>HumanEval</td><td>81.7</td><td>71.3</td><td><strong>86.0</strong></td></tr><tr><td>MBPP</td><td><strong>82.3</strong></td><td>71.9</td><td>80.2</td></tr><tr><td>MultiPL-E</td><td>63.4</td><td>48.1</td><td><strong>69.2</strong></td></tr><tr><td>EvalPlus</td><td>75.2</td><td>66.9</td><td><strong>79.0</strong></td></tr><tr><td>LiveCodeBench</td><td>29.3</td><td>17.9</td><td><strong>35.7</strong></td></tr><tr><td><em><strong>Mathematics</strong></em></td><td></td><td></td><td></td></tr><tr><td>GSM8K</td><td><strong>93.0</strong></td><td>82.7</td><td>91.1</td></tr><tr><td>MATH</td><td>50.4</td><td>42.5</td><td><strong>59.7</strong></td></tr><tr><td><em><strong>Chinese</strong></em></td><td></td><td></td><td></td></tr><tr><td>C-Eval</td><td>61.6</td><td>76.1</td><td><strong>83.8</strong></td></tr><tr><td>AlignBench</td><td>7.42</td><td>7.28</td><td><strong>8.27</strong></td></tr></tbody></table></div>
<h3 class="flex scroll-m-28 flex-row items-center gap-2" id="zen7b-a14b-instruct"><a data-card="" href="#zen7b-a14b-instruct" class="peer">zen7B-A14B-Instruct</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>































































































































































<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Datasets</th><th>Mixtral-8x7B-Instruct-v0.1</th><th>Yi-1.5-34B-Chat</th><th>Qwen1.5-32B-Chat</th><th><strong>zen7B-A14B-Instruct</strong></th></tr></thead><tbody><tr><td>Architecture</td><td>MoE</td><td>Dense</td><td>Dense</td><td>MoE</td></tr><tr><td>#Activated Params</td><td>12B</td><td>34B</td><td>32B</td><td>14B</td></tr><tr><td>#Params</td><td>47B</td><td>34B</td><td>32B</td><td>57B</td></tr><tr><td><em><strong>English</strong></em></td><td></td><td></td><td></td><td></td></tr><tr><td>MMLU</td><td>71.4</td><td><strong>76.8</strong></td><td>74.8</td><td>75.4</td></tr><tr><td>MMLU-Pro</td><td>43.3</td><td>52.3</td><td>46.4</td><td><strong>52.8</strong></td></tr><tr><td>GPQA</td><td>-</td><td>-</td><td>30.8</td><td><strong>34.3</strong></td></tr><tr><td>TheroemQA</td><td>-</td><td>-</td><td>30.9</td><td><strong>33.1</strong></td></tr><tr><td>MT-Bench</td><td>8.30</td><td>8.50</td><td>8.30</td><td><strong>8.55</strong></td></tr><tr><td><em><strong>Coding</strong></em></td><td></td><td></td><td></td><td></td></tr><tr><td>HumanEval</td><td>45.1</td><td>75.2</td><td>68.3</td><td><strong>79.9</strong></td></tr><tr><td>MBPP</td><td>59.5</td><td><strong>74.6</strong></td><td>67.9</td><td>70.9</td></tr><tr><td>MultiPL-E</td><td>-</td><td>-</td><td>50.7</td><td><strong>66.4</strong></td></tr><tr><td>EvalPlus</td><td>48.5</td><td>-</td><td>63.6</td><td><strong>71.6</strong></td></tr><tr><td>LiveCodeBench</td><td>12.3</td><td>-</td><td>15.2</td><td><strong>25.5</strong></td></tr><tr><td><em><strong>Mathematics</strong></em></td><td></td><td></td><td></td><td></td></tr><tr><td>GSM8K</td><td>65.7</td><td><strong>90.2</strong></td><td>83.6</td><td>79.6</td></tr><tr><td>MATH</td><td>30.7</td><td><strong>50.1</strong></td><td>42.4</td><td>49.1</td></tr><tr><td><em><strong>Chinese</strong></em></td><td></td><td></td><td></td><td></td></tr><tr><td>C-Eval</td><td>-</td><td>-</td><td>76.7</td><td>80.5</td></tr><tr><td>AlignBench</td><td>5.70</td><td>7.20</td><td>7.19</td><td><strong>7.36</strong></td></tr></tbody></table></div>
<h3 class="flex scroll-m-28 flex-row items-center gap-2" id="zen-7b-instruct"><a data-card="" href="#zen-7b-instruct" class="peer">zen-7B-Instruct</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>





























































































































































<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Datasets</th><th>Llama-3-8B-Instruct</th><th>Yi-1.5-9B-Chat</th><th>GLM-4-9B-Chat</th><th>Qwen1.5-7B-Chat</th><th>zen-7B-Instruct</th></tr></thead><tbody><tr><td><em><strong>English</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MMLU</td><td>68.4</td><td>69.5</td><td><strong>72.4</strong></td><td>59.5</td><td>70.5</td></tr><tr><td>MMLU-Pro</td><td>41.0</td><td>-</td><td>-</td><td>29.1</td><td><strong>44.1</strong></td></tr><tr><td>GPQA</td><td><strong>34.2</strong></td><td>-</td><td><strong>-</strong></td><td>27.8</td><td>25.3</td></tr><tr><td>TheroemQA</td><td>23.0</td><td>-</td><td>-</td><td>14.1</td><td><strong>25.3</strong></td></tr><tr><td>MT-Bench</td><td>8.05</td><td>8.20</td><td>8.35</td><td>7.60</td><td><strong>8.41</strong></td></tr><tr><td><em><strong>Coding</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Humaneval</td><td>62.2</td><td>66.5</td><td>71.8</td><td>46.3</td><td><strong>79.9</strong></td></tr><tr><td>MBPP</td><td><strong>67.9</strong></td><td>-</td><td>-</td><td>48.9</td><td>67.2</td></tr><tr><td>MultiPL-E</td><td>48.5</td><td>-</td><td>-</td><td>27.2</td><td><strong>59.1</strong></td></tr><tr><td>Evalplus</td><td>60.9</td><td>-</td><td>-</td><td>44.8</td><td><strong>70.3</strong></td></tr><tr><td>LiveCodeBench</td><td>17.3</td><td>-</td><td>-</td><td>6.0</td><td><strong>26.6</strong></td></tr><tr><td><em><strong>Mathematics</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GSM8K</td><td>79.6</td><td><strong>84.8</strong></td><td>79.6</td><td>60.3</td><td>82.3</td></tr><tr><td>MATH</td><td>30.0</td><td>47.7</td><td><strong>50.6</strong></td><td>23.2</td><td>49.6</td></tr><tr><td><em><strong>Chinese</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>C-Eval</td><td>45.9</td><td>-</td><td>75.6</td><td>67.3</td><td><strong>77.2</strong></td></tr><tr><td>AlignBench</td><td>6.20</td><td>6.90</td><td>7.01</td><td>6.20</td><td><strong>7.21</strong></td></tr></tbody></table></div>
<h3 class="flex scroll-m-28 flex-row items-center gap-2" id="zen-05b-instruct--zen-15b-instruct"><a data-card="" href="#zen-05b-instruct--zen-15b-instruct" class="peer">zen-0.5B-Instruct &amp; zen-1.5B-Instruct</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>















































<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Datasets</th><th>Qwen1.5-0.5B-Chat</th><th><strong>zen-0.5B-Instruct</strong></th><th>Qwen1.5-1.8B-Chat</th><th><strong>zen-1.5B-Instruct</strong></th></tr></thead><tbody><tr><td>MMLU</td><td>35.0</td><td><strong>37.9</strong></td><td>43.7</td><td><strong>52.4</strong></td></tr><tr><td>HumanEval</td><td>9.1</td><td><strong>17.1</strong></td><td>25.0</td><td><strong>37.8</strong></td></tr><tr><td>GSM8K</td><td>11.3</td><td><strong>40.1</strong></td><td>35.3</td><td><strong>61.6</strong></td></tr><tr><td>C-Eval</td><td>37.2</td><td><strong>45.2</strong></td><td>55.3</td><td><strong>63.8</strong></td></tr><tr><td>IFEval (Prompt Strict-Acc.)</td><td>14.6</td><td><strong>20.0</strong></td><td>16.8</td><td><strong>29.0</strong></td></tr></tbody></table></div>
<h2 class="flex scroll-m-28 flex-row items-center gap-2" id="multilingual-capability-of-instruction-tuned-models"><a data-card="" href="#multilingual-capability-of-instruction-tuned-models" class="peer">Multilingual capability of instruction-tuned models</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>We compare zen instruction-tuned models with other recent LLMs on several cross-lingual open benchmarks as well as by human evaluation. For benchmarks, we show the results on 2 evaluation datasets:</p>
<ul>
<li><a href="https://github.com/nlp-uoregon/mlmm-evaluation" rel="noreferrer noopener" target="_blank">M-MMLU</a> from Okapi: multilingual commonsense evaluation (we evaluate with a subset on ar, de, es, fr, it, nl, ru, uk, vi, zh)</li>
<li><a href="https://arxiv.org/abs/2210.03057" rel="noreferrer noopener" target="_blank">MGSM</a>: math evaluation on languages including de, en, es, fr, ja, ru, th, zh and bn</li>
</ul>
<p>The results are averaged over languages for each benchmark and shown as follows:</p>
















































































<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Models</th><th>M-MMLU (5-shot)</th><th>MGSM (0-shot, CoT)</th></tr></thead><tbody><tr><td><strong><em>Proprietary LLMs</em></strong></td><td></td><td></td></tr><tr><td>GPT-4-0613</td><td>78.0</td><td>87.0</td></tr><tr><td>GPT-4-Turbo-0409</td><td>79.3</td><td>90.5</td></tr><tr><td>GPT-4o-0513</td><td>83.2</td><td>89.6</td></tr><tr><td>Claude-3-Opus-20240229</td><td>80.1</td><td>91.0</td></tr><tr><td>Claude-3-Sonnet-20240229</td><td>71.0</td><td>85.6</td></tr><tr><td>** <em>Open-source LLMs</em>**</td><td></td><td></td></tr><tr><td>command-r-plus-110b</td><td>65.5</td><td>63.5</td></tr><tr><td>Qwen1.5-7B-Chat</td><td>50.0</td><td>37.0</td></tr><tr><td>Qwen1.5-32B-Chat</td><td>65.0</td><td>65.0</td></tr><tr><td>Qwen1.5-72B-Chat</td><td>68.4</td><td>71.7</td></tr><tr><td><strong>zen-7B-Instruct</strong></td><td><strong>60.0</strong></td><td><strong>57.0</strong></td></tr><tr><td><strong>zen7B-A14B-Instruct</strong></td><td><strong>68.0</strong></td><td><strong>74.0</strong></td></tr><tr><td><strong>zen-72B-Instruct</strong></td><td><strong>78.0</strong></td><td><strong>86.6</strong></td></tr></tbody></table></div>
<p>For human evaluation, we compare zen-72B-Instruct with GPT3.5, GPT4 and Claude-3-Opus using in-house evaluation set, which includes 10 languages ar, es, fr, ko, th, vi, pt, id, ja and ru (the scores range from 1~5):</p>







































































































<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Models</th><th>ar</th><th>es</th><th>fr</th><th>ko</th><th>th</th><th>vi</th><th>pt</th><th>id</th><th>ja</th><th>ru</th><th>Average</th></tr></thead><tbody><tr><td>Claude-3-Opus-20240229</td><td>4.15</td><td>4.31</td><td>4.23</td><td>4.23</td><td>4.01</td><td>3.98</td><td>4.09</td><td>4.40</td><td>3.85</td><td>4.25</td><td>4.15</td></tr><tr><td>GPT-4o-0513</td><td>3.55</td><td>4.26</td><td>4.16</td><td>4.40</td><td>4.09</td><td>4.14</td><td>3.89</td><td>4.39</td><td>3.72</td><td>4.32</td><td>4.09</td></tr><tr><td>GPT-4-Turbo-0409</td><td>3.44</td><td>4.08</td><td>4.19</td><td>4.24</td><td>4.11</td><td>3.84</td><td>3.86</td><td>4.09</td><td>3.68</td><td>4.27</td><td>3.98</td></tr><tr><td><strong>zen-72B-Instruct</strong></td><td>3.86</td><td>4.10</td><td>4.01</td><td>4.14</td><td>3.75</td><td>3.91</td><td>3.97</td><td>3.83</td><td>3.63</td><td>4.15</td><td>3.93</td></tr><tr><td>GPT-4-0613</td><td>3.55</td><td>3.92</td><td>3.94</td><td>3.87</td><td>3.83</td><td>3.95</td><td>3.55</td><td>3.77</td><td>3.06</td><td>3.63</td><td>3.71</td></tr><tr><td>GPT-3.5-Turbo-1106</td><td>2.52</td><td>4.07</td><td>3.47</td><td>2.37</td><td>3.38</td><td>2.90</td><td>3.37</td><td>3.56</td><td>2.75</td><td>3.24</td><td>3.16</td></tr></tbody></table></div>
<p>Grouped by task types, the results are shown as follows:</p>






















































<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Models</th><th>Knowledge</th><th>Understanding</th><th>Creation</th><th>Math</th></tr></thead><tbody><tr><td>Claude-3-Opus-20240229</td><td>3.64</td><td>4.45</td><td>4.42</td><td>3.81</td></tr><tr><td>GPT-4o-0513</td><td>3.76</td><td>4.35</td><td>4.45</td><td>3.53</td></tr><tr><td>GPT-4-Turbo-0409</td><td>3.42</td><td>4.29</td><td>4.35</td><td>3.58</td></tr><tr><td><strong>zen-72B-Instruct</strong></td><td>3.41</td><td>4.07</td><td>4.36</td><td>3.61</td></tr><tr><td>GPT-4-0613</td><td>3.42</td><td>4.09</td><td>4.10</td><td>3.32</td></tr><tr><td>GPT-3.5-Turbo-1106</td><td>3.37</td><td>3.67</td><td>3.89</td><td>2.97</td></tr></tbody></table></div>
<p>These results demonstrate the strong multilingual capabilities of zen instruction-tuned models.</p>
<hr/>
<ol>
<li>Update on 2024-07-16: The results of instruction-tuned models may differ from those presented in the technical report; in case of any discrepancy, the results documented in the technical report should take precedence. ↩︎ ↩︎ ↩︎</li>
</ol></div></main><!--$--><!--/$--><script src="/_next/static/chunks/e83606e8fa9cc796.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[106,[\"/_next/static/chunks/59d0ad1b64f8544e.js\"],\"RootProvider\"]\n3:I[53113,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"default\"]\n4:I[73211,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"default\"]\n5:I[10086,[\"/_next/static/chunks/59d0ad1b64f8544e.js\",\"/_next/static/chunks/36bfed0236ce2cf2.js\",\"/_next/static/chunks/e62b91212ee7f8ff.js\",\"/_next/static/chunks/2a98816c7d26bf58.js\",\"/_next/static/chunks/cb0a883bafeb6805.js\"],\"\"]\n7:I[89923,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"OutletBoundary\"]\n8:\"$Sreact.suspense\"\na:I[89923,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"ViewportBoundary\"]\nc:I[89923,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"MetadataBoundary\"]\ne:I[6998,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"default\"]\n:HL[\"/_next/static/chunks/f2332aac77592f9d.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"o_J3cFnAZ1mdL_cv2bxKY\",\"c\":[\"\",\"blog\",\"qwen2\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"qwen2\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/f2332aac77592f9d.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/59d0ad1b64f8544e.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"dark\",\"suppressHydrationWarning\":true,\"children\":[\"$\",\"body\",null,{\"className\":\"antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"main\",null,{\"className\":\"flex min-h-screen flex-col items-center justify-center px-4 text-center\",\"children\":[[\"$\",\"div\",null,{\"className\":\"mb-8 opacity-20\",\"children\":[\"$\",\"svg\",null,{\"width\":\"120\",\"height\":\"120\",\"viewBox\":\"0 0 120 120\",\"fill\":\"none\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"circle\",null,{\"cx\":\"60\",\"cy\":\"60\",\"r\":\"50\",\"stroke\":\"currentColor\",\"strokeWidth\":\"3\",\"strokeLinecap\":\"round\",\"strokeDasharray\":\"280 40\"}]}]}],[\"$\",\"p\",null,{\"className\":\"text-xs font-mono uppercase tracking-[0.3em] text-fd-muted-foreground mb-4\",\"children\":\"404\"}],[\"$\",\"h1\",null,{\"className\":\"text-3xl font-semibold mb-3\",\"children\":\"Page not found\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground max-w-sm mb-10\",\"children\":\"This page doesn't exist, or it may have moved. Try the documentation or head home.\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-3 justify-center\",\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/\",\"className\":\"inline-flex items-center gap-2 rounded-lg bg-fd-primary text-fd-primary-foreground px-4 py-2 text-sm font-medium hover:opacity-90 transition\",\"children\":\"Go home\"}],[\"$\",\"$L5\",null,{\"href\":\"/docs\",\"className\":\"inline-flex items-center gap-2 rounded-lg border border-fd-border bg-fd-background px-4 py-2 text-sm font-medium hover:bg-fd-muted transition\",\"children\":\"Documentation\"}],[\"$\",\"$L5\",null,{\"href\":\"/docs/models\",\"className\":\"inline-flex items-center gap-2 rounded-lg border border-fd-border bg-fd-background px-4 py-2 text-sm font-medium hover:bg-fd-muted transition\",\"children\":\"Browse models\"}]]}],[\"$\",\"p\",null,{\"className\":\"mt-16 text-xs font-mono text-fd-muted-foreground opacity-50\",\"children\":\"zenlm.org\"}]]}],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[\"$L6\",[[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/36bfed0236ce2cf2.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-1\",{\"src\":\"/_next/static/chunks/e62b91212ee7f8ff.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-2\",{\"src\":\"/_next/static/chunks/2a98816c7d26bf58.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-3\",{\"src\":\"/_next/static/chunks/cb0a883bafeb6805.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"$L7\",null,{\"children\":[\"$\",\"$8\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@9\"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$La\",null,{\"children\":\"$Lb\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$Lc\",null,{\"children\":[\"$\",\"$8\",null,{\"name\":\"Next.Metadata\",\"children\":\"$Ld\"}]}]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$e\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"f:I[48068,[\"/_next/static/chunks/59d0ad1b64f8544e.js\",\"/_next/static/chunks/36bfed0236ce2cf2.js\",\"/_next/static/chunks/e62b91212ee7f8ff.js\",\"/_next/static/chunks/2a98816c7d26bf58.js\",\"/_next/static/chunks/cb0a883bafeb6805.js\"],\"default\"]\n"])</script><script>self.__next_f.push([1,"6:[\"$\",\"main\",null,{\"className\":\"mx-auto w-full max-w-2xl px-4 py-16\",\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog\",\"className\":\"inline-flex items-center gap-1.5 text-sm text-fd-muted-foreground hover:text-fd-foreground mb-10 transition-colors\",\"children\":\"← Back to Blog\"}],[\"$\",\"div\",null,{\"className\":\"mb-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"June 6, 2024\"}],[\"$\",\"h1\",null,{\"className\":\"text-3xl font-bold mt-2 mb-3\",\"children\":\"Hello zen\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-lg mb-4\",\"children\":\"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3 pt-4 border-t border-fd-border\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-sm text-fd-muted-foreground\",\"children\":[\"By \",\"Zen LM Team\"]}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"prose dark:prose-invert max-w-none\",\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/QwenLM/zen\",\"children\":\"GITHUB\"}],\" \",[\"$\",\"$Lf\",null,{\"href\":\"https://huggingface.co/Qwen\",\"children\":\"HUGGING FACE\"}],\" \",[\"$\",\"$Lf\",null,{\"href\":\"https://modelscope.cn/organization/qwen\",\"children\":\"MODELSCOPE\"}],\" \",[\"$\",\"$Lf\",null,{\"href\":\"https://huggingface.co/spaces/Qwen/zen-72B-Instruct\",\"children\":\"DEMO\"}],\" \",[\"$\",\"$Lf\",null,{\"href\":\"https://discord.gg/yPEP2vHTu4\",\"children\":\"DISCORD\"}]]}],\"\\n\",[\"$\",\"h1\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"introduction\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#introduction\",\"className\":\"peer\",\"children\":\"Introduction\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"After months of efforts, we are pleased to announce the evolution from Qwen1.5 to zen. This time, we bring to you:\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Pretrained and instruction-tuned models of 5 sizes, including zen-0.5B, zen-1.5B, zen-7B, zen7B-A14B, and \",[\"$\",\"strong\",null,{\"children\":\"zen-72B\"}],\" ;\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Having been trained on data in \",[\"$\",\"strong\",null,{\"children\":\"27\"}],\" additional languages besides English and Chinese;\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"State-of-the-art performance in a large number of benchmark evaluations;\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Significantly improved performance in coding and mathematics;\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Extended context length support up to \",[\"$\",\"strong\",null,{\"children\":\"128K\"}],\" tokens with zen-7B-Instruct and zen-72B-Instruct.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"We have opensourced the models in Hugging Face and ModelScope to you and we are looking forward to hearing from you!\"}],\"\\n\",[\"$\",\"h2\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"model-information\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#model-information\",\"className\":\"peer\",\"children\":\"Model Information\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The zen series include base and instruction-tuned models of 5 sizes, including zen-0.5B, zen-1.5B, zen-7B, zen7B-A14B, zen-72B. We illustrate the key information of the models in the following table:\"}],\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L10\",\"\\n\",\"$L11\",\"\\n\",\"$L12\",\"\\n\",\"$L13\",\"\\n\",\"$L14\",\"\\n\",\"$L15\",\"\\n\",\"$L16\",\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L17\",\"\\n\",\"$L18\",\"\\n\",\"$L19\",\"\\n\",\"$L1a\",\"\\n\",\"$L1b\",\"\\n\",\"$L1c\",\"\\n\",\"$L1d\",\"\\n\",\"$L1e\",\"\\n\",\"$L1f\",\"\\n\",\"$L20\",\"\\n\",\"$L21\",\"\\n\",\"$L22\",\"\\n\",\"$L23\",\"\\n\",\"$L24\",\"\\n\",\"$L25\",\"\\n\",\"$L26\",\"\\n\",\"$L27\",\"\\n\",\"$L28\",\"\\n\",\"$L29\",\"\\n\",\"$L2a\",\"\\n\",\"$L2b\",\"\\n\",\"$L2c\",\"\\n\",\"$L2d\",\"\\n\",\"$L2e\",\"\\n\",\"$L2f\",\"\\n\",\"$L30\",\"\\n\",\"$L31\",\"\\n\",\"$L32\",\"\\n\",\"$L33\",\"\\n\",\"$L34\",\"\\n\",\"$L35\",\"\\n\",\"$L36\",\"\\n\",\"$L37\",\"\\n\",\"$L38\",\"\\n\",\"$L39\",\"\\n\",\"$L3a\",\"\\n\",\"$L3b\",\"\\n\",\"$L3c\",\"\\n\",\"$L3d\",\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L3e\",\"\\n\",\"$L3f\",\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L40\",\"\\n\",\"$L41\",\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L42\",\"\\n\",\"$L43\",\"\\n\",\"$L44\",\"\\n\",\"$L45\",\"\\n\",\"$L46\",\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L47\",\"\\n\",\"$L48\",\"\\n\",\"$L49\",\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L4a\",\"\\n\",\"$L4b\",\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L4c\",\"\\n\",\"$L4d\",\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L4e\",\"\\n\",\"$L4f\",\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L50\",\"\\n\",\"$L51\",\"\\n\",\"$L52\",\"\\n\",\"$L53\",\"\\n\",\"$L54\",\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L55\",\"\\n\",\"$L56\",\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L57\",\"\\n\",\"$L58\",\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L59\",\"\\n\",\"$L5a\",\"\\n\",\"$L5b\",\"\\n\",\"$L5c\"]}]]}]\n"])</script><script>self.__next_f.push([1,"5d:I[51504,[\"/_next/static/chunks/59d0ad1b64f8544e.js\",\"/_next/static/chunks/36bfed0236ce2cf2.js\",\"/_next/static/chunks/e62b91212ee7f8ff.js\",\"/_next/static/chunks/2a98816c7d26bf58.js\",\"/_next/static/chunks/cb0a883bafeb6805.js\"],\"CodeBlock\"]\n5e:I[51504,[\"/_next/static/chunks/59d0ad1b64f8544e.js\",\"/_next/static/chunks/36bfed0236ce2cf2.js\",\"/_next/static/chunks/e62b91212ee7f8ff.js\",\"/_next/static/chunks/2a98816c7d26bf58.js\",\"/_next/static/chunks/cb0a883bafeb6805.js\"],\"Pre\"]\n10:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Models\"}],[\"$\",\"th\",null,{\"children\":\"zen-0.5B\"}],[\"$\",\"th\",null,{\"children\":\"zen-1.5B\"}],[\"$\",\"th\",null,{\"children\":\"zen-7B\"}],[\"$\",\"th\",null,{\"children\":\"zen7B-A14B\"}],[\"$\",\"th\",null,{\"children\":\"zen-72B\"}]]}]}]}]}]\n11:[\"$\",\"h1\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"params-049b-154b-707b-5741b-7271b\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#params-049b-154b-707b-5741b-7271b\",\"className\":\"peer\",\"children\":\"Params| 0.49B| 1.54B| 7.07B| 57.41B| 72.71B\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n12:[\"$\",\"h1\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"non-emb-params-035b-131b-598b-5632b-7021b\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#non-emb-params-035b-131b-598b-5632b-7021b\",\"className\":\"peer\",\"children\":\"Non-Emb Params| 0.35B| 1.31B| 5.98B| 56.32B| 70.21B\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n13:[\"$\",\"p\",null,{\"children\":[\"GQA| True| True| True| True| True\",[\"$\",\"br\",null,{}],\"\\nTie Embedding| True| True| False| False| False\",[\"$\",\"br\",null,{}],\"\\nContext Length| 32K| 32K| 128K| 64K| 128K\"]}]\n14:[\"$\",\"p\",null,{\"children\":\"Specifically, previously in Qwen1.5, only Qwen1.5-32B and Qwen1.5-110B have adopted Group Query Attention (GQA). This time, for all model sizes, we apply GQA so that they can enjoy the benefits of faster speed and less memory usage in model inference. For small models, we prefer the application of tying embedding as the large sparse embeddings take up a large proportion of the total model parameters.\"}]\n15:[\"$\",\"p\",null,{\"children\":[\"In terms of the context length, all base language models have been pretrained on data of the context length of 32K tokens, and we observe satisfactory extrapolation capabilities up to 128K in PPL evaluation. However, for instruction-tuned models, we are not satisfied with merely PPL evaluation; we need the models to be capable of correctly understanding long context and completing tasks. In the table, we list the context length capabilities of instruction-tuned models, as assessed through the evaluation of the \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/gkamradt/LLMTest_NeedleInAHaystack\",\"children\":\"Needle in a Haystack\"}],\" task. Notably, when augmented with YARN, both zen-7B-Instruct and zen-72B-Instruct models demonstrate an impressive capacity to handle cont"])</script><script>self.__next_f.push([1,"ext lengths extending up to 128K tokens.\"]}]\n16:[\"$\",\"p\",null,{\"children\":\"Significant efforts were directed towards augmenting both the volume and quality of pretraining and instruction-tuning datasets across a diverse linguistic spectrum, beyond English and Chinese, to bolster its multilingual competencies. Although large language models possess an inherent capacity to generalize to other languages, we explicitly highlight the inclusion of 27 additional languages in our training:\"}]\n17:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Regions\"}],[\"$\",\"th\",null,{\"children\":\"Languages\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Western Europe\"}],[\"$\",\"td\",null,{\"children\":\"German, French, Spanish, Portuguese, Italian, Dutch\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Eastern \u0026 Central Europe\"}],[\"$\",\"td\",null,{\"children\":\"Russian, Czech, Polish\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Middle East\"}],[\"$\",\"td\",null,{\"children\":\"Arabic, Persian, Hebrew, Turkish\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Eastern Asia\"}],[\"$\",\"td\",null,{\"children\":\"Japanese, Korean\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"South-Eastern Asia\"}],[\"$\",\"td\",null,{\"children\":\"Vietnamese, Thai, Indonesian, Malay, Lao, Burmese, Cebuano, Khmer, Tagalog\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Southern Asia\"}],[\"$\",\"td\",null,{\"children\":\"Hindi, Bengali, Urdu\"}]]}]]}]]}]}]\n18:[\"$\",\"p\",null,{\"children\":\"Additionally, we have devoted significant effort to addressing code-switching, a frequent occurrence in multilingual evaluation. Consequently, our models’ proficiency in handling this phenomenon have notably enhanced. Evaluations using prompts that typically induce code-switching across languages confirm a substantial reduction in associated issues.\"}]\n19:[\"$\",\"h1\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"performance\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#performance\",\"className\":\"peer\",\"children\":\"Performance\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n1a:[\"$\",\"p\",null,{\"children\":\"Comparative assessments reveal substantial enhancements in performance for large-scale models (70B+ parameters) relative to Qwen1.5. Here our evaluation centers on the large-size model zen-72B. In terms of base language models, zen-72B and state-of-the-art open models are evaluated for different capbilities including natural language understanding, knowledge acquisition, coding proficiency, mathematical skills, and multilingual abilities. Benefiting from meticulously curated datasets and optimized training methods, zen-72B exhibits superior performance compared to leading models such as Llama-3-70B. Notably, it surpasses the performance of its predecessor, Qwen1.5-110B, despite having fewer parameters.\"}]\n"])</script><script>self.__next_f.push([1,"1b:[\"$\",\"p\",null,{\"children\":[\"After extensive large-scale pre-training, we conduct post-training to further enhance Qwen’s intelligence, bringing it closer to human. This process further improves the model’s capabilities in areas such as coding, mathematics, reasoning, instruction following, multilingual understanding, and more. Additionally, it aligns the model’s output with human values, ensuring that it is helpful, honest, and harmless. Our post-training phase is designed with the principle of scalable training with minimal human annotation. Specifically, we investigate how to obtain high-quality, reliable, diverse and creative demonstration data and preference data with various automated alignment strategies, such as \",[\"$\",\"$Lf\",null,{\"href\":\"https://arxiv.org/pdf/2308.01825\",\"children\":\"rejection sampling\"}],\" for math, execution feedback for coding and instruction-following, back-translation for creative writing, \",[\"$\",\"$Lf\",null,{\"href\":\"https://arxiv.org/pdf/2401.12474\",\"children\":\"scalable oversight\"}],\" for role-play, etc. As for training, we apply a combination of supervised fine-tuning, reward model training and online DPO training. We also employ a novel \",[\"$\",\"$Lf\",null,{\"href\":\"https://arxiv.org/pdf/2405.17931\",\"children\":\"Online Merging Optimizer\"}],\" to minimize the alignment tax. These collective efforts have significantly boosted the capabilities and intelligence of our models, as illustrated in the following table.\"]}]\n"])</script><script>self.__next_f.push([1,"1c:[\"$\",\"p\",null,{\"children\":\"We comprehensively evaluate zen-72B-Instruct on 16 benchmarks across various domains. zen-72B-Instruct strikes a balance between obtaining better capabilities and aligning well with human values. Specifically, zen-72B-Instruct significantly surpasses Qwen1.5-72B-Chat across all benchmarks, and also reaches competitive performance compared with Llama-3-70B-Instruct.1\"}]\n1d:[\"$\",\"p\",null,{\"children\":\"In terms of smaller models, our zen models also outcompete the SOTA models of similar or even larger sizes. In comparison with the very recently released SOTA models, zen-7B-Instruct can still demonstrate advantages across benchmarks, showing specifically outstanding performance on coding and Chinese-related metrics.1\"}]\n1e:[\"$\",\"h1\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"highlights\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#highlights\",\"className\":\"peer\",\"children\":\"Highlights\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n1f:[\"$\",\"h2\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"coding--mathematics\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#coding--mathematics\",\"className\":\"peer\",\"children\":\"Coding \u0026 Mathematics\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n20:[\"$\",\"p\",null,{\"children\":[\"We have persistently dedicated our efforts to enhance the advanced capabilities of Qwen, particularly in coding and mathematics. In coding, we have successfully integrated the code training experience and data from \",[\"$\",\"$Lf\",null,{\"href\":\"https://qwenlm.github.io/blog/codeqwen1.5/\",\"children\":\"CodeQwen1.5\"}],\", resulting in significant improvements in zen-72B-Instruct across various programming languages. Regarding mathematics, by exploiting the extensive and high-quality datasets, zen-72B-Instruct has reflects stronger capabilities in solving mathematic problems.\"]}]\n21:[\"$\",\"h2\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"long-context-understanding\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#long-context-understanding\",\"className\":\"peer\",\"children\":\"Long Context Understanding\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n22:[\"$\",\"p\",null,{\"children\":[\"In zen, all instruction-tuned models have been trained on 32k length contexts, and extrapolated to longer context lengths using techniques like \",[\"$\",\"$Lf\",null,{\"href\":\"https://arxiv.org/abs/2309.00071\",\"children\":\"YARN\"}],\" or \",[\"$\",\"$Lf\",null,{\"href\":\"https://arxiv.or"])</script><script>self.__next_f.push([1,"g/abs/2402.17463\",\"children\":\"Dual Chunk Attention\"}],\".\"]}]\n23:[\"$\",\"p\",null,{\"children\":[\"The figure below shows our test results on the \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/gkamradt/LLMTest_NeedleInAHaystack\",\"children\":\"Needle in a Haystack\"}],\". Notably, zen-72B-Instruct is capable of flawlessly handling information extraction tasks within a 128k context. Coupled with its inherent strong performance, it becomes the preferred choice for handling long text tasks when resources are sufficient.\"]}]\n24:[\"$\",\"p\",null,{\"children\":\"Additionally, it’s worth noting the impressive capabilities of other models in the series: zen-7B-Instruct nearly flawlessly handles contexts up to 128k in length, zen7B-A14B-Instruct manages contexts up to 64k, and the two smaller models in the lineup support contexts of 32k.\"}]\n25:[\"$\",\"p\",null,{\"children\":[\"Alongside the long-context models, we have also open-sourced an agent solution for efficiently processing documents containing up to 1 million tokens. For more details, see \",[\"$\",\"$Lf\",null,{\"href\":\"https://qwenlm.github.io/blog/qwen-agent-2405/\",\"children\":\"our dedicated blog post on this topic\"}],\".\"]}]\n26:[\"$\",\"h2\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"safety-and-responsibility\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#safety-and-responsibility\",\"className\":\"peer\",\"children\":\"Safety and Responsibility\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n27:[\"$\",\"p\",null,{\"children\":[\"The table below presents the proportion of harmful responses generated by large models for four categories of multilingual unsafe querys(Illegal Activity, Fraud, Pornography, Privacy Violence). The test data was derived from \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/verazuo/jailbreak_llms/tree/main\",\"children\":\"Jailbreak\"}],\" and translated into multiple languages for evaluation. We find that Llama-3 does not effectively handle multilingual prompts, and therefore, it is not included in the comparison. Through significance testing (P_value), we found that the zen-72B-Instruct model performs comparably to GPT-4 in terms of safety, and significantly outperforms the Mistral-8x22B model.\"]}]\n"])</script><script>self.__next_f.push([1,"28:[\"$\",\"p\",null,{\"children\":[\"Language| | Illegal Activity| | | Fraud| | | Pornography| | | Privacy Violence|\",[\"$\",\"br\",null,{}],\"\\n---|---|---|---|---|---|---|---|---|---|---|---|---\",[\"$\",\"br\",null,{}],\"\\n| GPT-4| Mistral-8x22B| zen-72B-Instruct| GPT-4| Mistral-8x22B| zen-72B-Instruct| GPT-4| Mistral-8x22B| zen-72B-Instruct| GPT-4| Mistral-8x22B| zen-72B-Instruct\",[\"$\",\"br\",null,{}],\"\\nzh| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"|  13%| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"|  17%| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"| \",[\"$\",\"strong\",null,{\"children\":\"43%\"}],\"|  47%| 53%| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"|  10%| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],[\"$\",\"br\",null,{}],\"\\nen| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"|  7%| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"|  23%| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"| \",[\"$\",\"strong\",null,{\"children\":\"37%\"}],\"|  67%| 63%| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"|  27%| 3%\",[\"$\",\"br\",null,{}],\"\\nar| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"|  13%| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"|  7%| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"| \",[\"$\",\"strong\",null,{\"children\":\"15%\"}],\"|  26%| \",[\"$\",\"strong\",null,{\"children\":\"15%\"}],\"|  3%| 13%| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],[\"$\",\"br\",null,{}],\"\\nes| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"|  7%| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"|  3%| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"| \",[\"$\",\"strong\",null,{\"children\":\"48%\"}],\"|  64%| 50%| \",[\"$\",\"strong\",null,{\"children\":\"3%\"}],\"|  7%| \",[\"$\",\"strong\",null,{\"children\":\"3%\"}],[\"$\",\"br\",null,{}],\"\\nfr| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"|  3%| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"| \",[\"$\",\"strong\",null,{\"children\":\"3%\"}],\"| \",[\"$\",\"strong\",null,{\"children\":\"3%\"}],\"|  7%| \",[\"$\",\"strong\",null,{\"children\":\"3%\"}],\"|  19%| 7%| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"|  27%| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],[\"$\",\"br\",null,{}],\"\\nko| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"|  4%| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"| \",[\"$\",\"strong\",null,{\"children\":\"3%\"}],\"|  8%| 4%| 17%| 29%| \",[\"$\",\"strong\",null,{\"children\":\"10%\"}],\"| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"|  26%| 4%\",[\"$\",\"br\",null,{}],\"\\npt| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"|  7%| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"| \",[\"$\",\"strong\",null,{\"children\":\"3%\"}],\"|  7%| \",[\"$\",\"strong\",null,{\"children\":\"3%\"}],\"| \",[\"$\",\"strong\",null,{\"children\":\"47%\"}],\"|  57%| \",[\"$\",\"strong\",null,{\"children\":\"47%\"}],\"| \",[\"$\",\"strong\",null,{\"children\":\"4%\"}],\"|  26%| \",[\"$\",\"strong\",null,{\"children\":\"4%\"}],[\"$\",\"br\",null,{}],\"\\nth| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"|  10%| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"|  7%| 23%| \",[\"$\",\"strong\",null,{\"children\":\"3%\"}],\"|  13%| 17%| \",[\"$\",\"strong\",null,{\"children\":\"10%\"}],\"|  13%| \",[\"$\",\"strong\",null,{\"children\":\"7%\"}],\"| \",[\"$\",\"strong\",null,{\"children\":\"7%\"}],[\"$\",\"br\",null,{}],\"\\nvi| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"|  4%| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"|  4%| 11%| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"| \",[\"$\",\"strong\",null,{\"children\":\"22%\"}],\"|  26%| \",[\"$\",\"strong\",null,{\"children\":\"22%\"}],\"| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],[\"$\",\"br\",null,{}],\"\\nAverage| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"|  8%| \",[\"$\",\"strong\",null,{\"children\":\"0%\"}],\"|  3%| 11%| \",[\"$\",\"strong\",null,{\"children\":\"2%\"}],\"| \",[\"$\",\"strong\",null,{\"children\":\"27%\"}],\"|  39%| 31%| 3%| 16%| \",[\"$\",\"strong\",null,{\"children\":\"2%\"}]]}]\n"])</script><script>self.__next_f.push([1,"29:[\"$\",\"h1\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"developing-with-zen\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#developing-with-zen\",\"className\":\"peer\",\"children\":\"Developing with zen\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n2a:[\"$\",\"p\",null,{\"children\":\"Now all models have been released in Hugging Face and ModelScope. Feel free to visit the model cards for detailed usages, and learn more information about each model, including its features, performance, etc.\"}]\n"])</script><script>self.__next_f.push([1,"2b:[\"$\",\"p\",null,{\"children\":[\"For a long time, a lot of friends have been supporting the development of Qwen, including finetuning (\",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/OpenAccess-AI-Collective/axolotl\",\"children\":\"Axolotl\"}],\", \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/hiyouga/LLaMA-Factory\",\"children\":\"Llama-Factory\"}],\", \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/yangjianxin1/Firefly\",\"children\":\"Firefly\"}],\", \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/modelscope/swift\",\"children\":\"Swift\"}],\", \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/InternLM/xtuner\",\"children\":\"XTuner\"}],\"), quantization (\",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/AutoGPTQ/AutoGPTQ\",\"children\":\"AutoGPTQ\"}],\", \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/casper-hansen/AutoAWQ\",\"children\":\"AutoAWQ\"}],\", \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/intel/neural-compressor\",\"children\":\"Neural Compressor\"}],\"), deployment (\",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/vllm-project/vllm\",\"children\":\"vLLM\"}],\", \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/sgl-project/sglang\",\"children\":\"SGL\"}],\", \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/skypilot-org/skypilot\",\"children\":\"SkyPilot\"}],\", \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/NVIDIA/TensorRT-LLM\",\"children\":\"TensorRT-LLM\"}],\", \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/openvinotoolkit/openvino\",\"children\":\"OpenVino\"}],\", \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/huggingface/text-generation-inference\",\"children\":\"TGI\"}],\"), API platforms (\",[\"$\",\"$Lf\",null,{\"href\":\"https://www.together.ai/\",\"children\":\"Together\"}],\", \",[\"$\",\"$Lf\",null,{\"href\":\"https://fireworks.ai/\",\"children\":\"Fireworks\"}],\", \",[\"$\",\"$Lf\",null,{\"href\":\"https://openrouter.ai/\",\"children\":\"OpenRouter\"}],\"), local run (\",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/ml-explore/mlx\",\"children\":\"MLX\"}],\", \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/ggerganov/llama.cpp\",\"children\":\"Llama.cpp\"}],\", \",[\"$\",\"$Lf\",null,{\"href\":\"https://ollama.com/\",\"children\":\"Ollama\"}],\", \",[\"$\",\"$Lf\",null,{\"href\":\"https://lmstudio.ai/\",\"children\":\"LM Studio\"}],\"), Agent and RAG Frameworks (\",[\"$\",\"$Lf\",null,{\"href\":\"https://www.llamaindex.ai/\",\"children\":\"LlamaIndex\"}],\", \",[\"$\",\"$Lf\",null,{\"href\":\"https://www.crewai.com/\",\"children\":\"CrewAI\"}],\", \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/OpenDevin/OpenDevin/\",\"children\":\"OpenDevin\"}],\") , Evaluation (\",[\"$\",\"$Lf\",null,{\"href\":\"https://chat.lmsys.org/\",\"children\":\"LMSys\"}],\", \",[\"$\",\"$Lf\",null,{\"href\":\"https://opencompass.org.cn/home\",\"children\":\"OpenCompass\"}],\", \",[\"$\",\"$Lf\",null,{\"href\":\"https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\",\"children\":\"Open LLM Leaderboard\"}],\"), model training (\",[\"$\",\"$Lf\",null,{\"href\":\"https://huggingface.co/cognitivecomputations\",\"children\":\"Dolphin\"}],\", \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/OpenBuddy/OpenBuddy\",\"children\":\"Openbuddy\"}],\") etc. For how to use zen with the third-party frameworks, please refer to the respective documentation as well as our \",[\"$\",\"$Lf\",null,{\"href\":\"https://qwen.readthedocs.io/en/latest/\",\"children\":\"official documentation\"}],\".\"]}]\n"])</script><script>self.__next_f.push([1,"2c:[\"$\",\"p\",null,{\"children\":\"Still there are a number of teams and people not mentioned that have made contributions to Qwen. We sincerely thank them for the support, and we hope that our collaboration can boost the research and development of the opensource AI community.\"}]\n2d:[\"$\",\"h1\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"license\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#license\",\"className\":\"peer\",\"children\":\"License\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n2e:[\"$\",\"p\",null,{\"children\":[\"This time, we change the licenses of our models to different ones. While zen-72B as well as its instruction-tuned models still uses the original Qianwen License, all other models, including zen-0.5B, zen-1.5B, zen-7B, and zen7B-A14B, turn to adopt \",[\"$\",\"strong\",null,{\"children\":\"Apache 2.0\"}],\"! We believe that the enhanced openness of our models to the community can accelerate the applications and commercial usages of zen all around the world.\"]}]\n2f:[\"$\",\"h1\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"whats-next-for-zen\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#whats-next-for-zen\",\"className\":\"peer\",\"children\":\"What’s Next for zen?\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n30:[\"$\",\"p\",null,{\"children\":\"We are training larger zen models to further explore model scaling along with our recent data scaling. Additionally, we extend the zen language models to multimodal, capable of understanding both vision and audio information. In the near future, we will continue opensource new models to accelerate opensource AI. Stay tuned!\"}]\n31:[\"$\",\"h1\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"citation\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#citation\",\"className\":\"peer\",\"children\":\"Citation\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n32:[\"$\",\"p\",null,{\"children\":\"If you find our work helpful, feel free to give us a cite!\"}]\n"])</script><script>self.__next_f.push([1,"33:[\"$\",\"$L5d\",null,{\"className\":\"shiki shiki-themes github-light github-dark\",\"style\":{\"--shiki-light\":\"#24292e\",\"--shiki-dark\":\"#e1e4e8\",\"--shiki-light-bg\":\"#fff\",\"--shiki-dark-bg\":\"#24292e\"},\"tabIndex\":\"0\",\"icon\":\"\u003csvg viewBox=\\\"0 0 24 24\\\"\u003e\u003cpath d=\\\"M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z\\\" fill=\\\"currentColor\\\" /\u003e\u003c/svg\u003e\",\"children\":[\"$\",\"$L5e\",null,{\"children\":[\"$\",\"code\",null,{\"children\":[[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"children\":\"    @article{qwen2,\"}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"children\":\"          title={zen Technical Report}, \"}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"children\":\"          author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\"}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"children\":\"          journal={arXiv preprint arXiv:2407.10671},\"}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"children\":\"          year={2024}\"}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"children\":\"    }\"}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"children\":\"    \"}]}]]}]}]}]\n"])</script><script>self.__next_f.push([1,"34:[\"$\",\"h1\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"appendix\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#appendix\",\"className\":\"peer\",\"children\":\"Appendix\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n35:[\"$\",\"h2\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"base-language-model-evaluation\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#base-language-model-evaluation\",\"className\":\"peer\",\"children\":\"Base Language Model Evaluation\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n36:[\"$\",\"p\",null,{\"children\":\"The evaluation of base models mainly focuses on the model performance of natural language understanding, general question answering, coding, mathematics, scientific knowledge, reasoning, multilingual capability, etc.\"}]\n37:[\"$\",\"p\",null,{\"children\":\"The datasets for evaluation include:\"}]\n38:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"English Tasks\"}],\" : MMLU (5-shot), MMLU-Pro (5-shot), GPQA (5shot), Theorem QA (5-shot), BBH (3-shot), HellaSwag (10-shot), Winogrande (5-shot), TruthfulQA (0-shot), ARC-C (25-shot)\"]}]\n39:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Coding Tasks\"}],\" : EvalPlus (0-shot) (HumanEval, MBPP, HumanEval+, MBPP+), MultiPL-E (0-shot) (Python, C++, JAVA, PHP, TypeScript, C#, Bash, JavaScript)\"]}]\n3a:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Math Tasks\"}],\" : GSM8K (4-shot), MATH (4-shot)\"]}]\n3b:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Chinese Tasks\"}],\" : C-Eval(5-shot), CMMLU (5-shot)\"]}]\n3c:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Multilingual Tasks\"}],\" : Multi-Exam (M3Exam 5-shot, IndoMMLU 3-shot, ruMMLU 5-shot, mMMLU 5-shot), Multi-Understanding (BELEBELE 5-shot, XCOPA 5-shot, XWinograd 5-shot, XStoryCloze 0-shot, PAWS-X 5-shot), Multi-Mathematics (MGSM 8-shot), Multi-Translation (Flores-101 5-shot)\"]}]\n3d:[\"$\",\"h3\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"zen-72b-performance\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#zen-72b-performance\",\"className\":\"peer\",\"children\":\"zen-72B performance\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"3e:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Datasets\"}],[\"$\",\"th\",null,{\"children\":\"DeepSeek-V2\"}],[\"$\",\"th\",null,{\"children\":\"Mixtral-8x22B\"}],[\"$\",\"th\",null,{\"children\":\"Llama-3-70B\"}],[\"$\",\"th\",null,{\"children\":\"Qwen1.5-72B\"}],[\"$\",\"th\",null,{\"children\":\"Qwen1.5-110B\"}],[\"$\",\"th\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"zen-72B\"}]}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Architecture\"}],[\"$\",\"td\",null,{\"children\":\"MoE\"}],[\"$\",\"td\",null,{\"children\":\"MoE\"}],[\"$\",\"td\",null,{\"children\":\"Dense\"}],[\"$\",\"td\",null,{\"children\":\"Dense\"}],[\"$\",\"td\",null,{\"children\":\"Dense\"}],[\"$\",\"td\",null,{\"children\":\"Dense\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"#Activated Params\"}],[\"$\",\"td\",null,{\"children\":\"21B\"}],[\"$\",\"td\",null,{\"children\":\"39B\"}],[\"$\",\"td\",null,{\"children\":\"70B\"}],[\"$\",\"td\",null,{\"children\":\"72B\"}],[\"$\",\"td\",null,{\"children\":\"110B\"}],[\"$\",\"td\",null,{\"children\":\"72B\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"#Params\"}],[\"$\",\"td\",null,{\"children\":\"236B\"}],[\"$\",\"td\",null,{\"children\":\"140B\"}],[\"$\",\"td\",null,{\"children\":\"70B\"}],[\"$\",\"td\",null,{\"children\":\"72B\"}],[\"$\",\"td\",null,{\"children\":\"110B\"}],[\"$\",\"td\",null,{\"children\":\"72B\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"English\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU\"}],[\"$\",\"td\",null,{\"children\":\"78.5\"}],[\"$\",\"td\",null,{\"children\":\"77.8\"}],[\"$\",\"td\",null,{\"children\":\"79.5\"}],[\"$\",\"td\",null,{\"children\":\"77.5\"}],[\"$\",\"td\",null,{\"children\":\"80.4\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"84.2\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU-Pro\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"49.5\"}],[\"$\",\"td\",null,{\"children\":\"52.8\"}],[\"$\",\"td\",null,{\"children\":\"45.8\"}],[\"$\",\"td\",null,{\"children\":\"49.4\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"55.6\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GPQA\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"34.3\"}],[\"$\",\"td\",null,{\"children\":\"36.3\"}],[\"$\",\"td\",null,{\"children\":\"36.3\"}],[\"$\",\"td\",null,{\"children\":\"35.9\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"37.9\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Theorem QA\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"35.9\"}],[\"$\",\"td\",null,{\"children\":\"32.3\"}],[\"$\",\"td\",null,{\"children\":\"29.3\"}],[\"$\",\"td\",null,{\"children\":\"34.9\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"43.1\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"BBH\"}],[\"$\",\"td\",null,{\"children\":\"78.9\"}],[\"$\",\"td\",null,{\"children\":\"78.9\"}],[\"$\",\"td\",null,{\"children\":\"81.0\"}],[\"$\",\"td\",null,{\"children\":\"65.5\"}],[\"$\",\"td\",null,{\"children\":\"74.8\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"82.4\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"HellaSwag\"}],[\"$\",\"td\",null,{\"children\":\"87.8\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"88.7\"}]}],[\"$\",\"td\",null,{\"children\":\"88.0\"}],[\"$\",\"td\",null,{\"children\":\"86.0\"}],[\"$\",\"td\",null,{\"children\":\"87.5\"}],[\"$\",\"td\",null,{\"children\":\"87.6\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"WindoGrande\"}],[\"$\",\"td\",null,{\"children\":\"84.8\"}],[\"$\",\"td\",null,{\"children\":\"85.0\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"85.3\"}]}],[\"$\",\"td\",null,{\"children\":\"83.0\"}],[\"$\",\"td\",null,{\"children\":\"83.5\"}],[\"$\",\"td\",null,{\"children\":\"85.1\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"ARC-C\"}],[\"$\",\"td\",null,{\"children\":\"70.0\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"70.7\"}]}],[\"$\",\"td\",null,{\"children\":\"68.8\"}],[\"$\",\"td\",null,{\"children\":\"65.9\"}],[\"$\",\"td\",null,{\"children\":\"69.6\"}],[\"$\",\"td\",null,{\"children\":\"68.9\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"TruthfulQA\"}],[\"$\",\"td\",null,{\"children\":\"42.2\"}],[\"$\",\"td\",null,{\"children\":\"51.0\"}],[\"$\",\"td\",null,{\"children\":\"45.6\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"59.6\"}]}],[\"$\",\"td\",null,{\"children\":\"49.6\"}],[\"$\",\"td\",null,{\"children\":\"54.8\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Coding\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"HumanEval\"}],[\"$\",\"td\",null,{\"children\":\"45.7\"}],[\"$\",\"td\",null,{\"children\":\"46.3\"}],[\"$\",\"td\",null,{\"children\":\"48.2\"}],[\"$\",\"td\",null,{\"children\":\"46.3\"}],[\"$\",\"td\",null,{\"children\":\"54.3\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"64.6\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MBPP\"}],[\"$\",\"td\",null,{\"children\":\"73.9\"}],[\"$\",\"td\",null,{\"children\":\"71.7\"}],[\"$\",\"td\",null,{\"children\":\"70.4\"}],[\"$\",\"td\",null,{\"children\":\"66.9\"}],[\"$\",\"td\",null,{\"children\":\"70.9\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"76.9\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"EvalPlus\"}],[\"$\",\"td\",null,{\"children\":\"55.0\"}],[\"$\",\"td\",null,{\"children\":\"54.1\"}],[\"$\",\"td\",null,{\"children\":\"54.8\"}],[\"$\",\"td\",null,{\"children\":\"52.9\"}],[\"$\",\"td\",null,{\"children\":\"57.7\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"65.4\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MultiPL-E\"}],[\"$\",\"td\",null,{\"children\":\"44.4\"}],[\"$\",\"td\",null,{\"children\":\"46.7\"}],[\"$\",\"td\",null,{\"children\":\"46.3\"}],[\"$\",\"td\",null,{\"children\":\"41.8\"}],[\"$\",\"td\",null,{\"children\":\"52.7\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"59.6\"}]}]]}],\"$L5f\",\"$L60\",\"$L61\",\"$L62\",\"$L63\",\"$L64\",\"$L65\",\"$L66\",\"$L67\",\"$L68\",\"$L69\"]}]]}]}]\n"])</script><script>self.__next_f.push([1,"3f:[\"$\",\"h3\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"zen7b-a14b\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#zen7b-a14b\",\"className\":\"peer\",\"children\":\"zen7B-A14B\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"40:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Datasets\"}],[\"$\",\"th\",null,{\"children\":\"Jamba\"}],[\"$\",\"th\",null,{\"children\":\"Mixtral-8x7B\"}],[\"$\",\"th\",null,{\"children\":\"Yi-1.5-34B\"}],[\"$\",\"th\",null,{\"children\":\"Qwen1.5-32B\"}],[\"$\",\"th\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"zen7B-A14B\"}]}]}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Architecture\"}],[\"$\",\"td\",null,{\"children\":\"MoE\"}],[\"$\",\"td\",null,{\"children\":\"MoE\"}],[\"$\",\"td\",null,{\"children\":\"Dense\"}],[\"$\",\"td\",null,{\"children\":\"Dense\"}],[\"$\",\"td\",null,{\"children\":\"MoE\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"#Activated Params\"}],[\"$\",\"td\",null,{\"children\":\"12B\"}],[\"$\",\"td\",null,{\"children\":\"12B\"}],[\"$\",\"td\",null,{\"children\":\"34B\"}],[\"$\",\"td\",null,{\"children\":\"32B\"}],[\"$\",\"td\",null,{\"children\":\"14B\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"#Params\"}],[\"$\",\"td\",null,{\"children\":\"52B\"}],[\"$\",\"td\",null,{\"children\":\"47B\"}],[\"$\",\"td\",null,{\"children\":\"34B\"}],[\"$\",\"td\",null,{\"children\":\"32B\"}],[\"$\",\"td\",null,{\"children\":\"57B\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"English\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU\"}],[\"$\",\"td\",null,{\"children\":\"67.4\"}],[\"$\",\"td\",null,{\"children\":\"71.8\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"77.1\"}]}],[\"$\",\"td\",null,{\"children\":\"74.3\"}],[\"$\",\"td\",null,{\"children\":\"76.5\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU-Pro\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"41.0\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"48.3\"}]}],[\"$\",\"td\",null,{\"children\":\"44.0\"}],[\"$\",\"td\",null,{\"children\":\"43.0\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GPQA\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"29.2\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"30.8\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"34.3\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Theorem QA\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"23.2\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"28.8\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"33.5\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"BBH\"}],[\"$\",\"td\",null,{\"children\":\"45.4\"}],[\"$\",\"td\",null,{\"children\":\"50.3\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"76.4\"}]}],[\"$\",\"td\",null,{\"children\":\"66.8\"}],[\"$\",\"td\",null,{\"children\":\"67.0\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"HellaSwag\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"87.1\"}]}],[\"$\",\"td\",null,{\"children\":\"86.5\"}],[\"$\",\"td\",null,{\"children\":\"85.9\"}],[\"$\",\"td\",null,{\"children\":\"85.0\"}],[\"$\",\"td\",null,{\"children\":\"85.2\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Winogrande\"}],[\"$\",\"td\",null,{\"children\":\"82.5\"}],[\"$\",\"td\",null,{\"children\":\"81.9\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"84.9\"}]}],[\"$\",\"td\",null,{\"children\":\"81.5\"}],[\"$\",\"td\",null,{\"children\":\"79.5\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"ARC-C\"}],[\"$\",\"td\",null,{\"children\":\"64.4\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"66.0\"}]}],[\"$\",\"td\",null,{\"children\":\"65.6\"}],[\"$\",\"td\",null,{\"children\":\"63.6\"}],[\"$\",\"td\",null,{\"children\":\"64.1\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"TruthfulQA\"}],[\"$\",\"td\",null,{\"children\":\"46.4\"}],[\"$\",\"td\",null,{\"children\":\"51.1\"}],[\"$\",\"td\",null,{\"children\":\"53.9\"}],[\"$\",\"td\",null,{\"children\":\"57.4\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"57.7\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Coding\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"HumanEval\"}],[\"$\",\"td\",null,{\"children\":\"29.3\"}],[\"$\",\"td\",null,{\"children\":\"37.2\"}],[\"$\",\"td\",null,{\"children\":\"46.3\"}],[\"$\",\"td\",null,{\"children\":\"43.3\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"53.0\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MBPP\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"63.9\"}],[\"$\",\"td\",null,{\"children\":\"65.5\"}],[\"$\",\"td\",null,{\"children\":\"64.2\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"71.9\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"EvalPlus\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"46.4\"}],[\"$\",\"td\",null,{\"children\":\"51.9\"}],[\"$\",\"td\",null,{\"children\":\"50.4\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"57.2\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MultiPL-E\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"39.0\"}],[\"$\",\"td\",null,{\"children\":\"39.5\"}],[\"$\",\"td\",null,{\"children\":\"38.5\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"49.8\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Mathematics\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GSM8K\"}],[\"$\",\"td\",null,{\"children\":\"59.9\"}],[\"$\",\"td\",null,{\"children\":\"62.5\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"82.7\"}]}],[\"$\",\"td\",null,{\"children\":\"76.8\"}],[\"$\",\"td\",null,{\"children\":\"80.7\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MATH\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"30.8\"}],[\"$\",\"td\",null,{\"children\":\"41.7\"}],\"$L6a\",\"$L6b\"]}],\"$L6c\",\"$L6d\",\"$L6e\",\"$L6f\",\"$L70\",\"$L71\",\"$L72\",\"$L73\"]}]]}]}]\n"])</script><script>self.__next_f.push([1,"41:[\"$\",\"h3\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"zen-7b\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#zen-7b\",\"className\":\"peer\",\"children\":\"zen-7B\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n42:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Datasets\"}],[\"$\",\"th\",null,{\"children\":\"Mistral-7B\"}],[\"$\",\"th\",null,{\"children\":\"Gemma-7B\"}],[\"$\",\"th\",null,{\"children\":\"Llama-3-8B\"}],[\"$\",\"th\",null,{\"children\":\"Qwen1.5-7B\"}],[\"$\",\"th\",null,{\"children\":\"zen-7B\"}]]}]}]}]}]\n43:[\"$\",\"h1\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"params-72b-85b-80b-77b-76b\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#params-72b-85b-80b-77b-76b\",\"className\":\"peer\",\"children\":\"Params| 7.2B| 8.5B| 8.0B| 7.7B| 7.6B\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n44:[\"$\",\"h1\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"non-emb-params-70b-78b-70b-65b-65b\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#non-emb-params-70b-78b-70b-65b-65b\",\"className\":\"peer\",\"children\":\"Non-emb Params| 7.0B| 7.8B| 7.0B| 6.5B| 6.5B\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"45:[\"$\",\"p\",null,{\"children\":[[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"English\"}]}],\"| | | | |\",[\"$\",\"br\",null,{}],\"\\nMMLU| 64.2| 64.6| 66.6| 61.0| \",[\"$\",\"strong\",null,{\"children\":\"70.3\"}],[\"$\",\"br\",null,{}],\"\\nMMLU-Pro| 30.9| 33.7| 35.4| 29.9| \",[\"$\",\"strong\",null,{\"children\":\"40.0\"}],[\"$\",\"br\",null,{}],\"\\nGPQA| 24.7| 25.7| 25.8| 26.7| \",[\"$\",\"strong\",null,{\"children\":\"31.8\"}],[\"$\",\"br\",null,{}],\"\\nTheorem QA| 19.2| 21.5| 22.1| 14.2| \",[\"$\",\"strong\",null,{\"children\":\"31.1\"}],[\"$\",\"br\",null,{}],\"\\nBBH| 56.1| 55.1| 57.7| 40.2| \",[\"$\",\"strong\",null,{\"children\":\"62.6\"}],[\"$\",\"br\",null,{}],\"\\nHellaSwag| \",[\"$\",\"strong\",null,{\"children\":\"83.2\"}],\"|  82.2| 82.1| 78.5| 80.7\",[\"$\",\"br\",null,{}],\"\\nWinogrande| 78.4| \",[\"$\",\"strong\",null,{\"children\":\"79.0\"}],\"|  77.4| 71.3| 77.0\",[\"$\",\"br\",null,{}],\"\\nARC-C| 60.0| \",[\"$\",\"strong\",null,{\"children\":\"61.1\"}],\"|  59.3| 54.2| 60.6\",[\"$\",\"br\",null,{}],\"\\nTruthfulQA| 42.2| 44.8| 44.0| 51.1| \",[\"$\",\"strong\",null,{\"children\":\"54.2\"}],[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Coding\"}]}],\"| | | | |\",[\"$\",\"br\",null,{}],\"\\nHumanEval| 29.3| 37.2| 33.5| 36.0| \",[\"$\",\"strong\",null,{\"children\":\"51.2\"}],[\"$\",\"br\",null,{}],\"\\nMBPP| 51.1| 50.6| 53.9| 51.6| \",[\"$\",\"strong\",null,{\"children\":\"65.9\"}],[\"$\",\"br\",null,{}],\"\\nEvalPlus| 36.4| 39.6| 40.3| 40.0| \",[\"$\",\"strong\",null,{\"children\":\"54.2\"}],[\"$\",\"br\",null,{}],\"\\nMultiPL-E| 29.4| 29.7| 22.6| 28.1| \",[\"$\",\"strong\",null,{\"children\":\"46.3\"}],[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Mathematics\"}]}],\"| | | | |\",[\"$\",\"br\",null,{}],\"\\nGSM8K| 52.2| 46.4| 56.0| 62.5| \",[\"$\",\"strong\",null,{\"children\":\"79.9\"}],[\"$\",\"br\",null,{}],\"\\nMATH| 13.1| 24.3| 20.5| 20.3| \",[\"$\",\"strong\",null,{\"children\":\"44.2\"}],[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Chinese\"}]}],\"| | | | |\",[\"$\",\"br\",null,{}],\"\\nC-Eval| 47.4| 43.6| 49.5| 74.1| \",[\"$\",\"strong\",null,{\"children\":\"83.2\"}],[\"$\",\"br\",null,{}],\"\\nCMMLU| -| -| 50.8| 73.1| \",[\"$\",\"strong\",null,{\"children\":\"83.9\"}],[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Multilingual\"}]}],\"| | | | |\",[\"$\",\"br\",null,{}],\"\\nMulti-Exam| 47.1| 42.7| 52.3| 47.7| \",[\"$\",\"strong\",null,{\"children\":\"59.2\"}],[\"$\",\"br\",null,{}],\"\\nMulti-Understanding| 63.3| 58.3| 68.6| 67.6| \",[\"$\",\"strong\",null,{\"children\":\"72.0\"}],[\"$\",\"br\",null,{}],\"\\nMulti-Mathematics| 26.3| 39.1| 36.3| 37.3| \",[\"$\",\"strong\",null,{\"children\":\"57.5\"}],[\"$\",\"br\",null,{}],\"\\nMulti-Translation| 23.3| 31.2| \",[\"$\",\"strong\",null,{\"children\":\"31.9\"}],\"|  28.4| 31.5\"]}]\n"])</script><script>self.__next_f.push([1,"46:[\"$\",\"h3\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"zen-05b--zen-15b\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#zen-05b--zen-15b\",\"className\":\"peer\",\"children\":\"zen-0.5B \u0026 zen-1.5B\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"47:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Datasets\"}],[\"$\",\"th\",null,{\"children\":\"Phi-2\"}],[\"$\",\"th\",null,{\"children\":\"Gemma-2B\"}],[\"$\",\"th\",null,{\"children\":\"MiniCPM\"}],[\"$\",\"th\",null,{\"children\":\"Qwen1.5-1.8B\"}],[\"$\",\"th\",null,{\"children\":\"zen-0.5B\"}],[\"$\",\"th\",null,{\"children\":\"zen-1.5B\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"#Non-Emb Params\"}],[\"$\",\"td\",null,{\"children\":\"2.5B\"}],[\"$\",\"td\",null,{\"children\":\"2.0B\"}],[\"$\",\"td\",null,{\"children\":\"2.4B\"}],[\"$\",\"td\",null,{\"children\":\"1.3B\"}],[\"$\",\"td\",null,{\"children\":\"0.35B\"}],[\"$\",\"td\",null,{\"children\":\"1.3B\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU\"}],[\"$\",\"td\",null,{\"children\":\"52.7\"}],[\"$\",\"td\",null,{\"children\":\"42.3\"}],[\"$\",\"td\",null,{\"children\":\"53.5\"}],[\"$\",\"td\",null,{\"children\":\"46.8\"}],[\"$\",\"td\",null,{\"children\":\"45.4\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"56.5\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU-Pro\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"15.9\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"14.7\"}],[\"$\",\"td\",null,{\"children\":\"21.8\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Theorem QA\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"8.9\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"15.0\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"HumanEval\"}],[\"$\",\"td\",null,{\"children\":\"47.6\"}],[\"$\",\"td\",null,{\"children\":\"22.0\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"50.0\"}]}],[\"$\",\"td\",null,{\"children\":\"20.1\"}],[\"$\",\"td\",null,{\"children\":\"22.0\"}],[\"$\",\"td\",null,{\"children\":\"31.1\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MBPP\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"55.0\"}]}],[\"$\",\"td\",null,{\"children\":\"29.2\"}],[\"$\",\"td\",null,{\"children\":\"47.3\"}],[\"$\",\"td\",null,{\"children\":\"18.0\"}],[\"$\",\"td\",null,{\"children\":\"22.0\"}],[\"$\",\"td\",null,{\"children\":\"37.4\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GSM8K\"}],[\"$\",\"td\",null,{\"children\":\"57.2\"}],[\"$\",\"td\",null,{\"children\":\"17.7\"}],[\"$\",\"td\",null,{\"children\":\"53.8\"}],[\"$\",\"td\",null,{\"children\":\"38.4\"}],[\"$\",\"td\",null,{\"children\":\"36.5\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"58.5\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MATH\"}],[\"$\",\"td\",null,{\"children\":\"3.5\"}],[\"$\",\"td\",null,{\"children\":\"11.8\"}],[\"$\",\"td\",null,{\"children\":\"10.2\"}],[\"$\",\"td\",null,{\"children\":\"10.1\"}],[\"$\",\"td\",null,{\"children\":\"10.7\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"21.7\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"BBH\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"43.4\"}]}],[\"$\",\"td\",null,{\"children\":\"35.2\"}],[\"$\",\"td\",null,{\"children\":\"36.9\"}],[\"$\",\"td\",null,{\"children\":\"24.2\"}],[\"$\",\"td\",null,{\"children\":\"28.4\"}],[\"$\",\"td\",null,{\"children\":\"37.2\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"HellaSwag\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"73.1\"}]}],[\"$\",\"td\",null,{\"children\":\"71.4\"}],[\"$\",\"td\",null,{\"children\":\"68.3\"}],[\"$\",\"td\",null,{\"children\":\"61.4\"}],[\"$\",\"td\",null,{\"children\":\"49.3\"}],[\"$\",\"td\",null,{\"children\":\"66.6\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Winogrande\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"74.4\"}]}],[\"$\",\"td\",null,{\"children\":\"66.8\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"60.3\"}],[\"$\",\"td\",null,{\"children\":\"56.8\"}],[\"$\",\"td\",null,{\"children\":\"66.2\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"ARC-C\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"61.1\"}]}],[\"$\",\"td\",null,{\"children\":\"48.5\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"37.9\"}],[\"$\",\"td\",null,{\"children\":\"31.5\"}],[\"$\",\"td\",null,{\"children\":\"43.9\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"TruthfulQA\"}],[\"$\",\"td\",null,{\"children\":\"44.5\"}],[\"$\",\"td\",null,{\"children\":\"33.1\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"39.4\"}],[\"$\",\"td\",null,{\"children\":\"39.7\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"45.9\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"C-Eval\"}],[\"$\",\"td\",null,{\"children\":\"23.4\"}],[\"$\",\"td\",null,{\"children\":\"28.0\"}],[\"$\",\"td\",null,{\"children\":\"51.1\"}],[\"$\",\"td\",null,{\"children\":\"59.7\"}],[\"$\",\"td\",null,{\"children\":\"58.2\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"70.6\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"CMMLU\"}],[\"$\",\"td\",null,{\"children\":\"24.2\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"51.1\"}],[\"$\",\"td\",null,{\"children\":\"57.8\"}],[\"$\",\"td\",null,{\"children\":\"55.1\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"70.3\"}]}]]}]]}]]}]}]\n"])</script><script>self.__next_f.push([1,"48:[\"$\",\"h2\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"instruction-tuned-model-evaluation1\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#instruction-tuned-model-evaluation1\",\"className\":\"peer\",\"children\":\"Instruction-tuned Model Evaluation1\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n49:[\"$\",\"h3\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"zen-72b-instruct\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#zen-72b-instruct\",\"className\":\"peer\",\"children\":\"zen-72B-Instruct\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"4a:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Datasets\"}],[\"$\",\"th\",null,{\"children\":\"Llama-3-70B-Instruct\"}],[\"$\",\"th\",null,{\"children\":\"Qwen1.5-72B-Chat\"}],[\"$\",\"th\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"zen-72B-Instruct\"}]}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"English\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU\"}],[\"$\",\"td\",null,{\"children\":\"82.0\"}],[\"$\",\"td\",null,{\"children\":\"75.6\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"82.3\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU-Pro\"}],[\"$\",\"td\",null,{\"children\":\"56.2\"}],[\"$\",\"td\",null,{\"children\":\"51.7\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"64.4\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GPQA\"}],[\"$\",\"td\",null,{\"children\":\"41.9\"}],[\"$\",\"td\",null,{\"children\":\"39.4\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"42.4\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"TheroemQA\"}],[\"$\",\"td\",null,{\"children\":\"42.5\"}],[\"$\",\"td\",null,{\"children\":\"28.8\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"44.4\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MT-Bench\"}],[\"$\",\"td\",null,{\"children\":\"8.95\"}],[\"$\",\"td\",null,{\"children\":\"8.61\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"9.12\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Arena-Hard\"}],[\"$\",\"td\",null,{\"children\":\"41.1\"}],[\"$\",\"td\",null,{\"children\":\"36.1\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"48.1\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"IFEval (Prompt Strict-Acc.)\"}],[\"$\",\"td\",null,{\"children\":\"77.3\"}],[\"$\",\"td\",null,{\"children\":\"55.8\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"77.6\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Coding\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"HumanEval\"}],[\"$\",\"td\",null,{\"children\":\"81.7\"}],[\"$\",\"td\",null,{\"children\":\"71.3\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"86.0\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MBPP\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"82.3\"}]}],[\"$\",\"td\",null,{\"children\":\"71.9\"}],[\"$\",\"td\",null,{\"children\":\"80.2\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MultiPL-E\"}],[\"$\",\"td\",null,{\"children\":\"63.4\"}],[\"$\",\"td\",null,{\"children\":\"48.1\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"69.2\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"EvalPlus\"}],[\"$\",\"td\",null,{\"children\":\"75.2\"}],[\"$\",\"td\",null,{\"children\":\"66.9\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"79.0\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"LiveCodeBench\"}],[\"$\",\"td\",null,{\"children\":\"29.3\"}],[\"$\",\"td\",null,{\"children\":\"17.9\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"35.7\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Mathematics\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GSM8K\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"93.0\"}]}],[\"$\",\"td\",null,{\"children\":\"82.7\"}],[\"$\",\"td\",null,{\"children\":\"91.1\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MATH\"}],[\"$\",\"td\",null,{\"children\":\"50.4\"}],[\"$\",\"td\",null,{\"children\":\"42.5\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"59.7\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Chinese\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"C-Eval\"}],[\"$\",\"td\",null,{\"children\":\"61.6\"}],[\"$\",\"td\",null,{\"children\":\"76.1\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"83.8\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"AlignBench\"}],[\"$\",\"td\",null,{\"children\":\"7.42\"}],[\"$\",\"td\",null,{\"children\":\"7.28\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"8.27\"}]}]]}]]}]]}]}]\n"])</script><script>self.__next_f.push([1,"4b:[\"$\",\"h3\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"zen7b-a14b-instruct\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#zen7b-a14b-instruct\",\"className\":\"peer\",\"children\":\"zen7B-A14B-Instruct\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"4c:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Datasets\"}],[\"$\",\"th\",null,{\"children\":\"Mixtral-8x7B-Instruct-v0.1\"}],[\"$\",\"th\",null,{\"children\":\"Yi-1.5-34B-Chat\"}],[\"$\",\"th\",null,{\"children\":\"Qwen1.5-32B-Chat\"}],[\"$\",\"th\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"zen7B-A14B-Instruct\"}]}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Architecture\"}],[\"$\",\"td\",null,{\"children\":\"MoE\"}],[\"$\",\"td\",null,{\"children\":\"Dense\"}],[\"$\",\"td\",null,{\"children\":\"Dense\"}],[\"$\",\"td\",null,{\"children\":\"MoE\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"#Activated Params\"}],[\"$\",\"td\",null,{\"children\":\"12B\"}],[\"$\",\"td\",null,{\"children\":\"34B\"}],[\"$\",\"td\",null,{\"children\":\"32B\"}],[\"$\",\"td\",null,{\"children\":\"14B\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"#Params\"}],[\"$\",\"td\",null,{\"children\":\"47B\"}],[\"$\",\"td\",null,{\"children\":\"34B\"}],[\"$\",\"td\",null,{\"children\":\"32B\"}],[\"$\",\"td\",null,{\"children\":\"57B\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"English\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU\"}],[\"$\",\"td\",null,{\"children\":\"71.4\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"76.8\"}]}],[\"$\",\"td\",null,{\"children\":\"74.8\"}],[\"$\",\"td\",null,{\"children\":\"75.4\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU-Pro\"}],[\"$\",\"td\",null,{\"children\":\"43.3\"}],[\"$\",\"td\",null,{\"children\":\"52.3\"}],[\"$\",\"td\",null,{\"children\":\"46.4\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"52.8\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GPQA\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"30.8\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"34.3\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"TheroemQA\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"30.9\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"33.1\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MT-Bench\"}],[\"$\",\"td\",null,{\"children\":\"8.30\"}],[\"$\",\"td\",null,{\"children\":\"8.50\"}],[\"$\",\"td\",null,{\"children\":\"8.30\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"8.55\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Coding\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"HumanEval\"}],[\"$\",\"td\",null,{\"children\":\"45.1\"}],[\"$\",\"td\",null,{\"children\":\"75.2\"}],[\"$\",\"td\",null,{\"children\":\"68.3\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"79.9\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MBPP\"}],[\"$\",\"td\",null,{\"children\":\"59.5\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"74.6\"}]}],[\"$\",\"td\",null,{\"children\":\"67.9\"}],[\"$\",\"td\",null,{\"children\":\"70.9\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MultiPL-E\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"50.7\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"66.4\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"EvalPlus\"}],[\"$\",\"td\",null,{\"children\":\"48.5\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"63.6\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"71.6\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"LiveCodeBench\"}],[\"$\",\"td\",null,{\"children\":\"12.3\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"15.2\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"25.5\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Mathematics\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GSM8K\"}],[\"$\",\"td\",null,{\"children\":\"65.7\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"90.2\"}]}],[\"$\",\"td\",null,{\"children\":\"83.6\"}],[\"$\",\"td\",null,{\"children\":\"79.6\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MATH\"}],[\"$\",\"td\",null,{\"children\":\"30.7\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"50.1\"}]}],[\"$\",\"td\",null,{\"children\":\"42.4\"}],[\"$\",\"td\",null,{\"children\":\"49.1\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Chinese\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"C-Eval\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"76.7\"}],[\"$\",\"td\",null,{\"children\":\"80.5\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"AlignBench\"}],[\"$\",\"td\",null,{\"children\":\"5.70\"}],[\"$\",\"td\",null,{\"children\":\"7.20\"}],[\"$\",\"td\",null,{\"children\":\"7.19\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"7.36\"}]}]]}]]}]]}]}]\n"])</script><script>self.__next_f.push([1,"4d:[\"$\",\"h3\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"zen-7b-instruct\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#zen-7b-instruct\",\"className\":\"peer\",\"children\":\"zen-7B-Instruct\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"4e:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Datasets\"}],[\"$\",\"th\",null,{\"children\":\"Llama-3-8B-Instruct\"}],[\"$\",\"th\",null,{\"children\":\"Yi-1.5-9B-Chat\"}],[\"$\",\"th\",null,{\"children\":\"GLM-4-9B-Chat\"}],[\"$\",\"th\",null,{\"children\":\"Qwen1.5-7B-Chat\"}],[\"$\",\"th\",null,{\"children\":\"zen-7B-Instruct\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"English\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU\"}],[\"$\",\"td\",null,{\"children\":\"68.4\"}],[\"$\",\"td\",null,{\"children\":\"69.5\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"72.4\"}]}],[\"$\",\"td\",null,{\"children\":\"59.5\"}],[\"$\",\"td\",null,{\"children\":\"70.5\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU-Pro\"}],[\"$\",\"td\",null,{\"children\":\"41.0\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"29.1\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"44.1\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GPQA\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"34.2\"}]}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"-\"}]}],[\"$\",\"td\",null,{\"children\":\"27.8\"}],[\"$\",\"td\",null,{\"children\":\"25.3\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"TheroemQA\"}],[\"$\",\"td\",null,{\"children\":\"23.0\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"14.1\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"25.3\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MT-Bench\"}],[\"$\",\"td\",null,{\"children\":\"8.05\"}],[\"$\",\"td\",null,{\"children\":\"8.20\"}],[\"$\",\"td\",null,{\"children\":\"8.35\"}],[\"$\",\"td\",null,{\"children\":\"7.60\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"8.41\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Coding\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Humaneval\"}],[\"$\",\"td\",null,{\"children\":\"62.2\"}],[\"$\",\"td\",null,{\"children\":\"66.5\"}],[\"$\",\"td\",null,{\"children\":\"71.8\"}],[\"$\",\"td\",null,{\"children\":\"46.3\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"79.9\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MBPP\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"67.9\"}]}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"48.9\"}],[\"$\",\"td\",null,{\"children\":\"67.2\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MultiPL-E\"}],[\"$\",\"td\",null,{\"children\":\"48.5\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"27.2\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"59.1\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Evalplus\"}],[\"$\",\"td\",null,{\"children\":\"60.9\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"44.8\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"70.3\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"LiveCodeBench\"}],[\"$\",\"td\",null,{\"children\":\"17.3\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"6.0\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"26.6\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Mathematics\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GSM8K\"}],[\"$\",\"td\",null,{\"children\":\"79.6\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"84.8\"}]}],[\"$\",\"td\",null,{\"children\":\"79.6\"}],[\"$\",\"td\",null,{\"children\":\"60.3\"}],[\"$\",\"td\",null,{\"children\":\"82.3\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MATH\"}],[\"$\",\"td\",null,{\"children\":\"30.0\"}],[\"$\",\"td\",null,{\"children\":\"47.7\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"50.6\"}]}],[\"$\",\"td\",null,{\"children\":\"23.2\"}],[\"$\",\"td\",null,{\"children\":\"49.6\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Chinese\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"C-Eval\"}],[\"$\",\"td\",null,{\"children\":\"45.9\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"75.6\"}],[\"$\",\"td\",null,{\"children\":\"67.3\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"77.2\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"AlignBench\"}],[\"$\",\"td\",null,{\"children\":\"6.20\"}],[\"$\",\"td\",null,{\"children\":\"6.90\"}],[\"$\",\"td\",null,{\"children\":\"7.01\"}],[\"$\",\"td\",null,{\"children\":\"6.20\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"7.21\"}]}]]}]]}]]}]}]\n"])</script><script>self.__next_f.push([1,"4f:[\"$\",\"h3\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"zen-05b-instruct--zen-15b-instruct\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#zen-05b-instruct--zen-15b-instruct\",\"className\":\"peer\",\"children\":\"zen-0.5B-Instruct \u0026 zen-1.5B-Instruct\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"50:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Datasets\"}],[\"$\",\"th\",null,{\"children\":\"Qwen1.5-0.5B-Chat\"}],[\"$\",\"th\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"zen-0.5B-Instruct\"}]}],[\"$\",\"th\",null,{\"children\":\"Qwen1.5-1.8B-Chat\"}],[\"$\",\"th\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"zen-1.5B-Instruct\"}]}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU\"}],[\"$\",\"td\",null,{\"children\":\"35.0\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"37.9\"}]}],[\"$\",\"td\",null,{\"children\":\"43.7\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"52.4\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"HumanEval\"}],[\"$\",\"td\",null,{\"children\":\"9.1\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"17.1\"}]}],[\"$\",\"td\",null,{\"children\":\"25.0\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"37.8\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GSM8K\"}],[\"$\",\"td\",null,{\"children\":\"11.3\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"40.1\"}]}],[\"$\",\"td\",null,{\"children\":\"35.3\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"61.6\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"C-Eval\"}],[\"$\",\"td\",null,{\"children\":\"37.2\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"45.2\"}]}],[\"$\",\"td\",null,{\"children\":\"55.3\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"63.8\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"IFEval (Prompt Strict-Acc.)\"}],[\"$\",\"td\",null,{\"children\":\"14.6\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"20.0\"}]}],[\"$\",\"td\",null,{\"children\":\"16.8\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"29.0\"}]}]]}]]}]]}]}]\n"])</script><script>self.__next_f.push([1,"51:[\"$\",\"h2\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"multilingual-capability-of-instruction-tuned-models\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#multilingual-capability-of-instruction-tuned-models\",\"className\":\"peer\",\"children\":\"Multilingual capability of instruction-tuned models\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n52:[\"$\",\"p\",null,{\"children\":\"We compare zen instruction-tuned models with other recent LLMs on several cross-lingual open benchmarks as well as by human evaluation. For benchmarks, we show the results on 2 evaluation datasets:\"}]\n53:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/nlp-uoregon/mlmm-evaluation\",\"children\":\"M-MMLU\"}],\" from Okapi: multilingual commonsense evaluation (we evaluate with a subset on ar, de, es, fr, it, nl, ru, uk, vi, zh)\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"$Lf\",null,{\"href\":\"https://arxiv.org/abs/2210.03057\",\"children\":\"MGSM\"}],\": math evaluation on languages including de, en, es, fr, ja, ru, th, zh and bn\"]}],\"\\n\"]}]\n54:[\"$\",\"p\",null,{\"children\":\"The results are averaged over languages for each benchmark and shown as follows:\"}]\n"])</script><script>self.__next_f.push([1,"55:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Models\"}],[\"$\",\"th\",null,{\"children\":\"M-MMLU (5-shot)\"}],[\"$\",\"th\",null,{\"children\":\"MGSM (0-shot, CoT)\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":[\"$\",\"em\",null,{\"children\":\"Proprietary LLMs\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GPT-4-0613\"}],[\"$\",\"td\",null,{\"children\":\"78.0\"}],[\"$\",\"td\",null,{\"children\":\"87.0\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GPT-4-Turbo-0409\"}],[\"$\",\"td\",null,{\"children\":\"79.3\"}],[\"$\",\"td\",null,{\"children\":\"90.5\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GPT-4o-0513\"}],[\"$\",\"td\",null,{\"children\":\"83.2\"}],[\"$\",\"td\",null,{\"children\":\"89.6\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Claude-3-Opus-20240229\"}],[\"$\",\"td\",null,{\"children\":\"80.1\"}],[\"$\",\"td\",null,{\"children\":\"91.0\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Claude-3-Sonnet-20240229\"}],[\"$\",\"td\",null,{\"children\":\"71.0\"}],[\"$\",\"td\",null,{\"children\":\"85.6\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"** \",[\"$\",\"em\",null,{\"children\":\"Open-source LLMs\"}],\"**\"]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"command-r-plus-110b\"}],[\"$\",\"td\",null,{\"children\":\"65.5\"}],[\"$\",\"td\",null,{\"children\":\"63.5\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Qwen1.5-7B-Chat\"}],[\"$\",\"td\",null,{\"children\":\"50.0\"}],[\"$\",\"td\",null,{\"children\":\"37.0\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Qwen1.5-32B-Chat\"}],[\"$\",\"td\",null,{\"children\":\"65.0\"}],[\"$\",\"td\",null,{\"children\":\"65.0\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Qwen1.5-72B-Chat\"}],[\"$\",\"td\",null,{\"children\":\"68.4\"}],[\"$\",\"td\",null,{\"children\":\"71.7\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"zen-7B-Instruct\"}]}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"60.0\"}]}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"57.0\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"zen7B-A14B-Instruct\"}]}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"68.0\"}]}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"74.0\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"zen-72B-Instruct\"}]}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"78.0\"}]}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"86.6\"}]}]]}]]}]]}]}]\n"])</script><script>self.__next_f.push([1,"56:[\"$\",\"p\",null,{\"children\":\"For human evaluation, we compare zen-72B-Instruct with GPT3.5, GPT4 and Claude-3-Opus using in-house evaluation set, which includes 10 languages ar, es, fr, ko, th, vi, pt, id, ja and ru (the scores range from 1~5):\"}]\n"])</script><script>self.__next_f.push([1,"57:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Models\"}],[\"$\",\"th\",null,{\"children\":\"ar\"}],[\"$\",\"th\",null,{\"children\":\"es\"}],[\"$\",\"th\",null,{\"children\":\"fr\"}],[\"$\",\"th\",null,{\"children\":\"ko\"}],[\"$\",\"th\",null,{\"children\":\"th\"}],[\"$\",\"th\",null,{\"children\":\"vi\"}],[\"$\",\"th\",null,{\"children\":\"pt\"}],[\"$\",\"th\",null,{\"children\":\"id\"}],[\"$\",\"th\",null,{\"children\":\"ja\"}],[\"$\",\"th\",null,{\"children\":\"ru\"}],[\"$\",\"th\",null,{\"children\":\"Average\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Claude-3-Opus-20240229\"}],[\"$\",\"td\",null,{\"children\":\"4.15\"}],[\"$\",\"td\",null,{\"children\":\"4.31\"}],[\"$\",\"td\",null,{\"children\":\"4.23\"}],[\"$\",\"td\",null,{\"children\":\"4.23\"}],[\"$\",\"td\",null,{\"children\":\"4.01\"}],[\"$\",\"td\",null,{\"children\":\"3.98\"}],[\"$\",\"td\",null,{\"children\":\"4.09\"}],[\"$\",\"td\",null,{\"children\":\"4.40\"}],[\"$\",\"td\",null,{\"children\":\"3.85\"}],[\"$\",\"td\",null,{\"children\":\"4.25\"}],[\"$\",\"td\",null,{\"children\":\"4.15\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GPT-4o-0513\"}],[\"$\",\"td\",null,{\"children\":\"3.55\"}],[\"$\",\"td\",null,{\"children\":\"4.26\"}],[\"$\",\"td\",null,{\"children\":\"4.16\"}],[\"$\",\"td\",null,{\"children\":\"4.40\"}],[\"$\",\"td\",null,{\"children\":\"4.09\"}],[\"$\",\"td\",null,{\"children\":\"4.14\"}],[\"$\",\"td\",null,{\"children\":\"3.89\"}],[\"$\",\"td\",null,{\"children\":\"4.39\"}],[\"$\",\"td\",null,{\"children\":\"3.72\"}],[\"$\",\"td\",null,{\"children\":\"4.32\"}],[\"$\",\"td\",null,{\"children\":\"4.09\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GPT-4-Turbo-0409\"}],[\"$\",\"td\",null,{\"children\":\"3.44\"}],[\"$\",\"td\",null,{\"children\":\"4.08\"}],[\"$\",\"td\",null,{\"children\":\"4.19\"}],[\"$\",\"td\",null,{\"children\":\"4.24\"}],[\"$\",\"td\",null,{\"children\":\"4.11\"}],[\"$\",\"td\",null,{\"children\":\"3.84\"}],[\"$\",\"td\",null,{\"children\":\"3.86\"}],[\"$\",\"td\",null,{\"children\":\"4.09\"}],[\"$\",\"td\",null,{\"children\":\"3.68\"}],[\"$\",\"td\",null,{\"children\":\"4.27\"}],[\"$\",\"td\",null,{\"children\":\"3.98\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"zen-72B-Instruct\"}]}],[\"$\",\"td\",null,{\"children\":\"3.86\"}],[\"$\",\"td\",null,{\"children\":\"4.10\"}],[\"$\",\"td\",null,{\"children\":\"4.01\"}],[\"$\",\"td\",null,{\"children\":\"4.14\"}],[\"$\",\"td\",null,{\"children\":\"3.75\"}],[\"$\",\"td\",null,{\"children\":\"3.91\"}],[\"$\",\"td\",null,{\"children\":\"3.97\"}],[\"$\",\"td\",null,{\"children\":\"3.83\"}],[\"$\",\"td\",null,{\"children\":\"3.63\"}],[\"$\",\"td\",null,{\"children\":\"4.15\"}],[\"$\",\"td\",null,{\"children\":\"3.93\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GPT-4-0613\"}],[\"$\",\"td\",null,{\"children\":\"3.55\"}],[\"$\",\"td\",null,{\"children\":\"3.92\"}],[\"$\",\"td\",null,{\"children\":\"3.94\"}],[\"$\",\"td\",null,{\"children\":\"3.87\"}],[\"$\",\"td\",null,{\"children\":\"3.83\"}],[\"$\",\"td\",null,{\"children\":\"3.95\"}],[\"$\",\"td\",null,{\"children\":\"3.55\"}],[\"$\",\"td\",null,{\"children\":\"3.77\"}],[\"$\",\"td\",null,{\"children\":\"3.06\"}],[\"$\",\"td\",null,{\"children\":\"3.63\"}],[\"$\",\"td\",null,{\"children\":\"3.71\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GPT-3.5-Turbo-1106\"}],[\"$\",\"td\",null,{\"children\":\"2.52\"}],[\"$\",\"td\",null,{\"children\":\"4.07\"}],[\"$\",\"td\",null,{\"children\":\"3.47\"}],[\"$\",\"td\",null,{\"children\":\"2.37\"}],[\"$\",\"td\",null,{\"children\":\"3.38\"}],[\"$\",\"td\",null,{\"children\":\"2.90\"}],[\"$\",\"td\",null,{\"children\":\"3.37\"}],[\"$\",\"td\",null,{\"children\":\"3.56\"}],[\"$\",\"td\",null,{\"children\":\"2.75\"}],[\"$\",\"td\",null,{\"children\":\"3.24\"}],[\"$\",\"td\",null,{\"children\":\"3.16\"}]]}]]}]]}]}]\n"])</script><script>self.__next_f.push([1,"58:[\"$\",\"p\",null,{\"children\":\"Grouped by task types, the results are shown as follows:\"}]\n"])</script><script>self.__next_f.push([1,"59:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Models\"}],[\"$\",\"th\",null,{\"children\":\"Knowledge\"}],[\"$\",\"th\",null,{\"children\":\"Understanding\"}],[\"$\",\"th\",null,{\"children\":\"Creation\"}],[\"$\",\"th\",null,{\"children\":\"Math\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Claude-3-Opus-20240229\"}],[\"$\",\"td\",null,{\"children\":\"3.64\"}],[\"$\",\"td\",null,{\"children\":\"4.45\"}],[\"$\",\"td\",null,{\"children\":\"4.42\"}],[\"$\",\"td\",null,{\"children\":\"3.81\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GPT-4o-0513\"}],[\"$\",\"td\",null,{\"children\":\"3.76\"}],[\"$\",\"td\",null,{\"children\":\"4.35\"}],[\"$\",\"td\",null,{\"children\":\"4.45\"}],[\"$\",\"td\",null,{\"children\":\"3.53\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GPT-4-Turbo-0409\"}],[\"$\",\"td\",null,{\"children\":\"3.42\"}],[\"$\",\"td\",null,{\"children\":\"4.29\"}],[\"$\",\"td\",null,{\"children\":\"4.35\"}],[\"$\",\"td\",null,{\"children\":\"3.58\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"zen-72B-Instruct\"}]}],[\"$\",\"td\",null,{\"children\":\"3.41\"}],[\"$\",\"td\",null,{\"children\":\"4.07\"}],[\"$\",\"td\",null,{\"children\":\"4.36\"}],[\"$\",\"td\",null,{\"children\":\"3.61\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GPT-4-0613\"}],[\"$\",\"td\",null,{\"children\":\"3.42\"}],[\"$\",\"td\",null,{\"children\":\"4.09\"}],[\"$\",\"td\",null,{\"children\":\"4.10\"}],[\"$\",\"td\",null,{\"children\":\"3.32\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GPT-3.5-Turbo-1106\"}],[\"$\",\"td\",null,{\"children\":\"3.37\"}],[\"$\",\"td\",null,{\"children\":\"3.67\"}],[\"$\",\"td\",null,{\"children\":\"3.89\"}],[\"$\",\"td\",null,{\"children\":\"2.97\"}]]}]]}]]}]}]\n"])</script><script>self.__next_f.push([1,"5a:[\"$\",\"p\",null,{\"children\":\"These results demonstrate the strong multilingual capabilities of zen instruction-tuned models.\"}]\n5b:[\"$\",\"hr\",null,{}]\n5c:[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Update on 2024-07-16: The results of instruction-tuned models may differ from those presented in the technical report; in case of any discrepancy, the results documented in the technical report should take precedence. ↩︎ ↩︎ ↩︎\"}],\"\\n\"]}]\n"])</script><script>self.__next_f.push([1,"5f:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Mathematics\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}]\n60:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GSM8K\"}],[\"$\",\"td\",null,{\"children\":\"79.2\"}],[\"$\",\"td\",null,{\"children\":\"83.7\"}],[\"$\",\"td\",null,{\"children\":\"83.0\"}],[\"$\",\"td\",null,{\"children\":\"79.5\"}],[\"$\",\"td\",null,{\"children\":\"85.4\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"89.5\"}]}]]}]\n61:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MATH\"}],[\"$\",\"td\",null,{\"children\":\"43.6\"}],[\"$\",\"td\",null,{\"children\":\"41.7\"}],[\"$\",\"td\",null,{\"children\":\"42.5\"}],[\"$\",\"td\",null,{\"children\":\"34.1\"}],[\"$\",\"td\",null,{\"children\":\"49.6\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"51.1\"}]}]]}]\n62:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Chinese\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}]\n63:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"C-Eval\"}],[\"$\",\"td\",null,{\"children\":\"81.7\"}],[\"$\",\"td\",null,{\"children\":\"54.6\"}],[\"$\",\"td\",null,{\"children\":\"65.2\"}],[\"$\",\"td\",null,{\"children\":\"84.1\"}],[\"$\",\"td\",null,{\"children\":\"89.1\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"91.0\"}]}]]}]\n64:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"CMMLU\"}],[\"$\",\"td\",null,{\"children\":\"84.0\"}],[\"$\",\"td\",null,{\"children\":\"53.4\"}],[\"$\",\"td\",null,{\"children\":\"67.2\"}],[\"$\",\"td\",null,{\"children\":\"83.5\"}],[\"$\",\"td\",null,{\"children\":\"88.3\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"90.1\"}]}]]}]\n65:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Multilingual\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}]\n66:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Mulit-Exam\"}],[\"$\",\"td\",null,{\"children\":\"67.5\"}],[\"$\",\"td\",null,{\"children\":\"63.5\"}],[\"$\",\"td\",null,{\"children\":\"70.0\"}],[\"$\",\"td\",null,{\"children\":\"66.4\"}],[\"$\",\"td\",null,{\"children\":\"75.6\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"76.6\"}]}]]}]\n67:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Multi-Understanding\"}],[\"$\",\"td\",null,{\"children\":\"77.0\"}],[\"$\",\"td\",null,{\"children\":\"77.7\"}],[\"$\",\"td\",null,{\"children\":\"79.9\"}],[\"$\",\"td\",null,{\"children\":\"78.2\"}],[\"$\",\"td\",null,{\"children\":\"78.2\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"80.7\"}]}]]}]\n68:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Multi-Mathematics\"}],[\"$\",\"td\",null,{\"children\":\"58.8\"}],[\"$\",\"td\",null,{\"children\":\"62.9\"}],[\"$\",\"td\",null,{\"children\":\"67.1\"}],[\"$\",\"td\",null,{\"children\":\"61.7\"}],[\"$\",\"td\",null,{\"children\":\"64.4\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"76.0\"}]}]]}]\n69:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Multi-Translation\"}],[\"$\",\"td\",null,{\"children\":\"36.0\"}],[\"$\",\"td\",null,{\"children\":\"23.3\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"38.0\"}]}],[\"$\",\"td\",null,{\"children\":\"35.6\"}],[\"$\",\"td\",null,{\"children\":\"36.2\"}],[\"$\",\"td\",null,{\"children\":\"37.8\"}]]}]\n6a:[\"$\",\"td\",null,{\"children\":\"36.1\"}]\n6b:[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"43.0\"}]}]\n6c:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Chinese\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}]\n6d:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"C-Eval\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"83.5\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"87.7\"}]}]]}]\n6e:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"CMMLU\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"ch"])</script><script>self.__next_f.push([1,"ildren\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"84.8\"}],[\"$\",\"td\",null,{\"children\":\"82.3\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"88.5\"}]}]]}]\n6f:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Multilingual\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}]\n70:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Multi-Exam\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"56.1\"}],[\"$\",\"td\",null,{\"children\":\"58.3\"}],[\"$\",\"td\",null,{\"children\":\"61.6\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"65.5\"}]}]]}]\n71:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Multi-Understanding\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"70.7\"}],[\"$\",\"td\",null,{\"children\":\"73.9\"}],[\"$\",\"td\",null,{\"children\":\"76.5\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"77.0\"}]}]]}]\n72:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Multi-Mathematics\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"45.0\"}],[\"$\",\"td\",null,{\"children\":\"49.3\"}],[\"$\",\"td\",null,{\"children\":\"56.1\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"62.3\"}]}]]}]\n73:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Multi-Translation\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"29.8\"}],[\"$\",\"td\",null,{\"children\":\"30.0\"}],[\"$\",\"td\",null,{\"children\":\"33.5\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"34.5\"}]}]]}]\n"])</script><script>self.__next_f.push([1,"b:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"2\",{\"name\":\"theme-color\",\"media\":\"(prefers-color-scheme: dark)\",\"content\":\"#0A0A0A\"}],[\"$\",\"meta\",\"3\",{\"name\":\"theme-color\",\"media\":\"(prefers-color-scheme: light)\",\"content\":\"#fff\"}]]\n"])</script><script>self.__next_f.push([1,"9:null\nd:[[\"$\",\"title\",\"0\",{\"children\":\"Hello zen — Zen LM Blog | Zen LM\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\"}],[\"$\",\"meta\",\"2\",{\"property\":\"og:title\",\"content\":\"Zen LM - Open Foundation Models\"}],[\"$\",\"meta\",\"3\",{\"property\":\"og:description\",\"content\":\"Zen AI model family by Hanzo AI. 14 frontier models from 4B to 1T+ parameters for code, reasoning, vision, and multimodal tasks.\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:url\",\"content\":\"https://zenlm.org\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:site_name\",\"content\":\"Zen LM\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"7\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"8\",{\"name\":\"twitter:site\",\"content\":\"@zenlmorg\"}],[\"$\",\"meta\",\"9\",{\"name\":\"twitter:creator\",\"content\":\"@zenlmorg\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:title\",\"content\":\"Zen LM - Open Foundation Models\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:description\",\"content\":\"Zen AI model family by Hanzo AI. 14 frontier models from 4B to 1T+ parameters for code, reasoning, vision, and multimodal tasks.\"}]]\n"])</script></body></html>