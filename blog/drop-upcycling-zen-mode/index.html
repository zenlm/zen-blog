<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta http-equiv=refresh content="5; url=https://qwen.ai/blog?id=drop-upcycling-zen-mode"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Drop-Upcycling and the Birth of Zen MoDE Architecture | Qwen</title><meta name=keywords content="Research,MoE,Architecture,Drop-Upcycling,Zen MoDE"><meta name=description content="How Drop-Upcycling (arXiv:2502.19261) transforms dense checkpoints into MoE models at 1/4 training cost, and how it shapes Zen MoDE — our Mixture of Distilled Experts architecture."><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/blog/drop-upcycling-zen-mode/><link crossorigin=anonymous href=/assets/css/stylesheet.6c0865a4bc8dbcbd27d78777eb33b404568f7fbf3185cca7b978e5275a4dd049.css integrity="sha256-bAhlpLyNvL0n14d36zO0BFaPf78xhcynuXjlJ1pN0Ek=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/drop-upcycling-zen-mode/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:title" content="Drop-Upcycling and the Birth of Zen MoDE Architecture"><meta property="og:description" content="How Drop-Upcycling (arXiv:2502.19261) transforms dense checkpoints into MoE models at 1/4 training cost, and how it shapes Zen MoDE — our Mixture of Distilled Experts architecture."><meta property="og:type" content="article"><meta property="og:url" content="https://qwenlm.github.io/blog/drop-upcycling-zen-mode/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2026-02-28T12:00:00-08:00"><meta property="article:modified_time" content="2026-02-28T12:00:00-08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Drop-Upcycling and the Birth of Zen MoDE Architecture"><meta name=twitter:description content="How Drop-Upcycling (arXiv:2502.19261) transforms dense checkpoints into MoE models at 1/4 training cost, and how it shapes Zen MoDE — our Mixture of Distilled Experts architecture."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Drop-Upcycling and the Birth of Zen MoDE Architecture","item":"https://qwenlm.github.io/blog/drop-upcycling-zen-mode/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Drop-Upcycling and the Birth of Zen MoDE Architecture","name":"Drop-Upcycling and the Birth of Zen MoDE Architecture","description":"How Drop-Upcycling (arXiv:2502.19261) transforms dense checkpoints into MoE models at 1/4 training cost, and how it shapes Zen MoDE — our Mixture of Distilled Experts architecture.","keywords":["Research","MoE","Architecture","Drop-Upcycling","Zen MoDE"],"articleBody":"DROP-UPCYCLING PAPER ZEN MODELS ZEN CODE\nMixture of Experts (MoE) is the architecture that makes trillion-parameter models economically viable. By routing each token through a small subset of expert networks rather than the full parameter set, MoE achieves large-model quality at dense-model inference cost. The problem: training an MoE from scratch is expensive. You are paying for both the scale and the specialization overhead.\nDrop-Upcycling is a technique that converts a trained dense checkpoint into an MoE at roughly 1/4 the training cost of building the MoE from scratch. It is one of the foundational techniques behind Zen MoDE — our Mixture of Distilled Experts architecture. This post explains how it works, why it works, and how we apply it at three scales.\nWhy Naive Expert Cloning Fails The obvious approach to dense-to-MoE conversion: clone the dense FFN block N times to create N experts, initialize a router, and continue training. This costs almost nothing upfront. The problem reveals itself within a few thousand steps: weight correlation collapse.\nWhen all experts start from identical weights, they receive identical gradients on every token that routes to multiple of them simultaneously. The router has no signal to differentiate them. Gradient updates push all experts in the same direction. Within tens of thousands of steps, the experts have converged to nearly identical weights despite being nominally separate. The MoE behaves like a dense model with routing overhead and no specialization benefit.\nThis is not a training instability — it is a symmetry problem. Identical initialization creates a saddle point in the loss landscape where all expert-breaking perturbations are equally likely but none are preferred. The model sits at the saddle indefinitely.\nDrop-Upcycling: Breaking Symmetry With Structured Noise Paper: arXiv:2502.19261\nDrop-Upcycling solves the symmetry problem by deliberately damaging each expert’s initialization in a structured way. For each expert i, randomly select p% of the FFN rows and reinitialize them from a normal distribution:\nw_j^(i) = N(0, σ²) if j ∈ dropped_rows(i) w_j^(i) = w_j^dense otherwise The dropped set is different for each expert (sampled independently). This breaks the symmetry: experts start from the same functional foundation but with different “holes” in their weight matrices.\nimport torch import torch.nn as nn from typing import List def drop_upcycle( dense_ffn: nn.Linear, n_experts: int, drop_rate: float = 0.1, init_std: float = 0.02, ) -\u003e List[nn.Linear]: \"\"\" Convert a single dense FFN layer into n_experts expert layers using Drop-Upcycling initialization. Args: dense_ffn: Pretrained dense FFN layer n_experts: Number of MoE experts to create drop_rate: Fraction of rows to reinitialize per expert init_std: Standard deviation for reinitialized rows Returns: List of n_experts initialized expert layers \"\"\" d_out, d_in = dense_ffn.weight.shape n_drop = max(1, int(d_out * drop_rate)) experts = [] for i in range(n_experts): expert = nn.Linear(d_in, d_out, bias=dense_ffn.bias is not None) with torch.no_grad(): # Start from dense weights expert.weight.copy_(dense_ffn.weight) if dense_ffn.bias is not None: expert.bias.copy_(dense_ffn.bias) # Drop a unique random subset of rows drop_indices = torch.randperm(d_out)[:n_drop] expert.weight[drop_indices] = torch.randn(n_drop, d_in) * init_std if dense_ffn.bias is not None: expert.bias[drop_indices] = 0.0 experts.append(expert) return experts def upcycle_transformer_block( dense_block, n_experts: int, drop_rate: float = 0.1, ) -\u003e dict: \"\"\"Upcycle a full transformer FFN block into MoE experts.\"\"\" return { 'gate_proj': drop_upcycle(dense_block.gate_proj, n_experts, drop_rate), 'up_proj': drop_upcycle(dense_block.up_proj, n_experts, drop_rate), 'down_proj': drop_upcycle(dense_block.down_proj, n_experts, drop_rate), } The drop_rate hyperparameter is the key dial. Too low (\u003c 5%) and experts remain too correlated. Too high (\u003e 30%) and you lose the functional initialization benefit — the expert essentially starts from random weights. The sweet spot we found empirically: 8-12% for general language models, 15% for code-specialized models (where more aggressive diversity is needed to separate syntax vs. semantics experts).\nTraining Dynamics: Implicit Specialization Signal Why do dropped experts specialize rather than just learn to patch their own holes? The mechanism is elegant.\nAfter the first few training steps, the “intact” experts (those with more of the original dense weights) perform better on common tokens — they have a head start. The router, which is optimizing for overall performance, learns to send common tokens to the better-performing intact experts. The dropped experts receive a different token distribution: harder tokens, rarer constructs, edge cases that the intact experts handle poorly.\nThis is the implicit specialization signal: experts do not specialize by design, they specialize by default. Each expert optimizes for the token distribution it actually receives, and that distribution is different for each expert because their relative competencies differ. By 50K training steps, the expert specialization is measurable:\nIntact experts (low drop rate) converge toward high-frequency, syntactic functions Heavily-dropped experts develop novel representations for rare or complex tokens The dense-to-MoE transition effectively turns a capability gap (some experts start worse) into a specialization signal (those experts become domain-specific).\nResults at Scale On the primary benchmark suite (comparing Drop-Upcycled MoE vs. MoE trained from scratch):\n5.9B active parameters: Drop-Upcycled MoE achieves 13B-equivalent quality at 1/4 the training FLOPs MMLU: Drop-Upcycling reaches 75.4 vs. from-scratch MoE at 74.8 (at same FLOP budget) HumanEval: 68.2 vs. 65.1 — Drop-Upcycling is better here because code has cleaner specialization axes Training efficiency: 4x speedup to target quality vs. from-scratch MoE The 1/4 FLOP claim requires context: the dense checkpoint training cost is amortized. If you already have the dense model (which you do, because you trained it first), the incremental cost to get an MoE is roughly 1/4 of a from-scratch MoE run. The total cost (dense + MoE) is higher than from-scratch, but for organizations that already have dense checkpoints — which is everyone — the marginal cost argument is what matters.\nZen MoDE: Three Scales of Application Zen MoDE (Mixture of Distilled Experts) applies Drop-Upcycling at three scales:\nzen4-mini (4B total, 4B active) — Dense. No upcycling needed at this scale; the routing overhead would dominate the compute savings. Zen4-mini uses a dense Qwen3 base.\nzen4-max (30B total, 3B active) — 16 experts, 2 active per token. Drop-Upcycled from an 8B dense checkpoint. Drop rate: 8%. Router: learned top-2 routing with load balancing. The transition from 8B dense to 30B MoE takes 200M training tokens, roughly 3 days on 32×H100.\nzen4-ultra (1T total, 32B active) — 384 experts, 8 active per token. This is our frontier model based on the Kimi K2.5 architecture. The upcycling here was done by the upstream team; we train behavioral adapters on top using GT-QLoRA (see the companion post on that technique).\nWhat Do the Experts Actually Learn? We ran expert attribution analysis on zen4-max after 500M post-upcycling training tokens. The methodology: for each expert, collect the 10K tokens that activate it most strongly and analyze the distribution.\nThe results cluster into recognizable domains:\nExpert Group Token Characteristics Experts 0-3 High-frequency English function words, punctuation Experts 4-6 Code tokens: brackets, operators, keywords Experts 7-9 Mathematical notation, numerals, equations Experts 10-12 Multilingual tokens (Chinese, Arabic, Cyrillic) Experts 13-15 Rare English words, technical terminology This is not designed specialization — it emerged from the implicit signal described above. The router discovered that routing code tokens to experts 4-6 produces better outputs than routing them to experts 0-3. No explicit supervision was provided.\nQ-GaLore for MoE Training Efficiency Training a 30B MoE requires careful memory management. We use Q-GaLore (Quantized Gradient Low-Rank Projection) for the upcycling phase:\nGradient projection: instead of storing full gradients for all parameters, project them into a low-rank subspace (rank 128 for most layers) Quantize the projected gradients to INT8 before accumulation Result: 50% memory reduction vs. standard LoRA, with +5.19 MMLU points vs. QLoRA on equivalent compute The memory savings matter because upcycling requires loading both the dense checkpoint (for initialization) and the growing MoE checkpoint (for training) simultaneously. Q-GaLore makes this tractable on 8×A100 80GB configurations where it would otherwise OOM.\nResearch Frontier: Progressive Router Pruning The current Drop-Upcycling approach creates experts of fixed capacity. An open question we are actively investigating: can you progressively prune the router to identify which experts are actually being used, then collapse unused capacity back into the shared parameters?\nEarly results suggest that after 500M training tokens, roughly 20-30% of experts receive less than 2% of routing probability across the evaluation corpus. These “dormant” experts can be pruned and their parameters absorbed back into the shared FFN without measurable quality degradation. This gives a dynamic MoE that starts dense, develops expert structure, and then self-compresses to its natural capacity.\nThe mechanism matters for continual learning: as new domains are added, dormant experts can be “awakened” and repurposed for the new domain rather than creating new experts from scratch. This connects Drop-Upcycling directly to the SuRe + OPCM continual learning stack described in our companion post.\nZen LM is a joint initiative of Hanzo AI Inc. (Techstars ‘17) and Zoo Labs Foundation (501c3).\n","wordCount":"1443","inLanguage":"en","datePublished":"2026-02-28T12:00:00-08:00","dateModified":"2026-02-28T12:00:00-08:00","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qwenlm.github.io/blog/drop-upcycling-zen-mode/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://qwenlm.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><style>.modal-overlay{position:fixed;top:0;left:0;width:100%;height:100%;background-color:rgba(0,0,0,.5);display:flex;align-items:center;z-index:1000;animation:fadeIn .3s ease-in-out}.modal-container{margin-left:auto;margin-right:auto;background-color:var(--theme);border-radius:8px;box-shadow:0 4px 20px rgba(0,0,0,.15);width:90%;max-width:420px;height:fit-content;padding:30px;text-align:center;position:relative;animation:slideIn .4s ease-out}.modal-container a{color:var(--hero2)}.modal-icon{width:70px;height:70px;background-color:#f0f7ff;border-radius:50%;display:flex;align-items:center;justify-content:center;margin:0 auto 20px;color:#1a73e8;font-size:30px}.modal-title{font-size:1.5rem;font-weight:600;color:var(--primary);margin:0 0 15px}.modal-message{font-size:1rem;color:var(--secondary);line-height:1.5;margin:0 0 25px}.countdown{font-size:1.2rem;color:#666;margin:20px 0;font-weight:500}.modal-buttons{display:flex;justify-content:center;gap:15px;margin-top:25px}.modal-buttons .btn{padding:6px 16px;border-radius:8px;font-size:1.2rem;font-weight:500;cursor:pointer;transition:all .3s ease;border:none}.btn-primary{background-color:#1a73e8;color:#fff}.btn-primary:hover{background-color:#1557b0}.btn-secondary{background-color:#f1f3f4;color:#333}.btn-secondary:hover{background-color:#e0e0e0}@keyframes fadeIn{from{opacity:0}to{opacity:1}}@keyframes slideIn{from{opacity:0;transform:translateY(-50px)}to{opacity:1;transform:translateY(0)}}@media(max-width:480px){.modal-container{max-width:95%;width:calc(95vw - 40px);padding:20px}}</style><div class=modal-overlay><div class=modal-container><div class=modal-icon><svg xmlns="http://www.w3.org/2000/svg" width="36" height="36" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71"/></svg></div><h2 class=modal-title>We have a new blog!<br>View this page at <a href="https://qwen.ai/blog?id=drop-upcycling-zen-mode">qwen.ai</a>.</h2><p class=modal-message>This page will automatically redirect in <span class=countdown id=countdown>5</span> seconds.</p><p class=modal-message>If you are not redirected automatically, please click the button below.</p><div class=modal-buttons><button class="btn btn-primary" onclick=redirectToPage()>Go Now</button></div></div></div><script>let countdown=5;const countdownElement=document.getElementById("countdown"),timer=setInterval(()=>{countdown--,countdownElement.textContent=countdown,countdown<=0&&clearInterval(timer)},1e3);function stayHere(){document.querySelector(".modal-overlay").style.display="none"}function redirectToPage(){window.location.href="https://qwen.ai/blog?id=drop-upcycling-zen-mode"}</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://chat.qwen.ai title="Try Qwen Chat"><span>Try Qwen Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Drop-Upcycling and the Birth of Zen MoDE Architecture</h1><div class=post-description>How Drop-Upcycling (arXiv:2502.19261) transforms dense checkpoints into MoE models at 1/4 training cost, and how it shapes Zen MoDE — our Mixture of Distilled Experts architecture.</div><div class=post-meta><span title='2026-02-28 12:00:00 -0800 -0800'>February 28, 2026</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;1443 words&nbsp;·&nbsp;Qwen Team</div></div></div><main class=main><article class=post-single><div class=post-content><p><a href=https://arxiv.org/abs/2502.19261 class="btn external" target=_blank>DROP-UPCYCLING PAPER</a>
<a href=https://huggingface.co/zenlm class="btn external" target=_blank>ZEN MODELS</a>
<a href=https://github.com/zenlm class="btn external" target=_blank>ZEN CODE</a></p><p>Mixture of Experts (MoE) is the architecture that makes trillion-parameter models economically viable. By routing each token through a small subset of expert networks rather than the full parameter set, MoE achieves large-model quality at dense-model inference cost. The problem: training an MoE from scratch is expensive. You are paying for both the scale and the specialization overhead.</p><p>Drop-Upcycling is a technique that converts a trained dense checkpoint into an MoE at roughly 1/4 the training cost of building the MoE from scratch. It is one of the foundational techniques behind <strong>Zen MoDE</strong> — our Mixture of Distilled Experts architecture. This post explains how it works, why it works, and how we apply it at three scales.</p><h2 id=why-naive-expert-cloning-fails>Why Naive Expert Cloning Fails<a hidden class=anchor aria-hidden=true href=#why-naive-expert-cloning-fails>#</a></h2><p>The obvious approach to dense-to-MoE conversion: clone the dense FFN block N times to create N experts, initialize a router, and continue training. This costs almost nothing upfront. The problem reveals itself within a few thousand steps: <strong>weight correlation collapse</strong>.</p><p>When all experts start from identical weights, they receive identical gradients on every token that routes to multiple of them simultaneously. The router has no signal to differentiate them. Gradient updates push all experts in the same direction. Within tens of thousands of steps, the experts have converged to nearly identical weights despite being nominally separate. The MoE behaves like a dense model with routing overhead and no specialization benefit.</p><p>This is not a training instability — it is a symmetry problem. Identical initialization creates a saddle point in the loss landscape where all expert-breaking perturbations are equally likely but none are preferred. The model sits at the saddle indefinitely.</p><h2 id=drop-upcycling-breaking-symmetry-with-structured-noise>Drop-Upcycling: Breaking Symmetry With Structured Noise<a hidden class=anchor aria-hidden=true href=#drop-upcycling-breaking-symmetry-with-structured-noise>#</a></h2><p><strong>Paper</strong>: arXiv:2502.19261</p><p>Drop-Upcycling solves the symmetry problem by deliberately damaging each expert&rsquo;s initialization in a structured way. For each expert i, randomly select p% of the FFN rows and reinitialize them from a normal distribution:</p><pre tabindex=0><code>w_j^(i) = N(0, σ²)   if j ∈ dropped_rows(i)
w_j^(i) = w_j^dense  otherwise
</code></pre><p>The dropped set is different for each expert (sampled independently). This breaks the symmetry: experts start from the same functional foundation but with different &ldquo;holes&rdquo; in their weight matrices.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>List</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>drop_upcycle</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>dense_ffn</span><span class=p>:</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>n_experts</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>drop_rate</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>init_std</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.02</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>List</span><span class=p>[</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Convert a single dense FFN layer into n_experts expert layers
</span></span></span><span class=line><span class=cl><span class=s2>    using Drop-Upcycling initialization.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Args:
</span></span></span><span class=line><span class=cl><span class=s2>        dense_ffn: Pretrained dense FFN layer
</span></span></span><span class=line><span class=cl><span class=s2>        n_experts: Number of MoE experts to create
</span></span></span><span class=line><span class=cl><span class=s2>        drop_rate: Fraction of rows to reinitialize per expert
</span></span></span><span class=line><span class=cl><span class=s2>        init_std: Standard deviation for reinitialized rows
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Returns:
</span></span></span><span class=line><span class=cl><span class=s2>        List of n_experts initialized expert layers
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>d_out</span><span class=p>,</span> <span class=n>d_in</span> <span class=o>=</span> <span class=n>dense_ffn</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>    <span class=n>n_drop</span> <span class=o>=</span> <span class=nb>max</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=nb>int</span><span class=p>(</span><span class=n>d_out</span> <span class=o>*</span> <span class=n>drop_rate</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>experts</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_experts</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>expert</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>dense_ffn</span><span class=o>.</span><span class=n>bias</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=c1># Start from dense weights</span>
</span></span><span class=line><span class=cl>            <span class=n>expert</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>copy_</span><span class=p>(</span><span class=n>dense_ffn</span><span class=o>.</span><span class=n>weight</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>dense_ffn</span><span class=o>.</span><span class=n>bias</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>expert</span><span class=o>.</span><span class=n>bias</span><span class=o>.</span><span class=n>copy_</span><span class=p>(</span><span class=n>dense_ffn</span><span class=o>.</span><span class=n>bias</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># Drop a unique random subset of rows</span>
</span></span><span class=line><span class=cl>            <span class=n>drop_indices</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randperm</span><span class=p>(</span><span class=n>d_out</span><span class=p>)[:</span><span class=n>n_drop</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>expert</span><span class=o>.</span><span class=n>weight</span><span class=p>[</span><span class=n>drop_indices</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>n_drop</span><span class=p>,</span> <span class=n>d_in</span><span class=p>)</span> <span class=o>*</span> <span class=n>init_std</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>dense_ffn</span><span class=o>.</span><span class=n>bias</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>expert</span><span class=o>.</span><span class=n>bias</span><span class=p>[</span><span class=n>drop_indices</span><span class=p>]</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>experts</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>expert</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>experts</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>upcycle_transformer_block</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>dense_block</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>n_experts</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>drop_rate</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.1</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Upcycle a full transformer FFN block into MoE experts.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;gate_proj&#39;</span><span class=p>:</span> <span class=n>drop_upcycle</span><span class=p>(</span><span class=n>dense_block</span><span class=o>.</span><span class=n>gate_proj</span><span class=p>,</span> <span class=n>n_experts</span><span class=p>,</span> <span class=n>drop_rate</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;up_proj&#39;</span><span class=p>:</span>   <span class=n>drop_upcycle</span><span class=p>(</span><span class=n>dense_block</span><span class=o>.</span><span class=n>up_proj</span><span class=p>,</span>   <span class=n>n_experts</span><span class=p>,</span> <span class=n>drop_rate</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;down_proj&#39;</span><span class=p>:</span> <span class=n>drop_upcycle</span><span class=p>(</span><span class=n>dense_block</span><span class=o>.</span><span class=n>down_proj</span><span class=p>,</span> <span class=n>n_experts</span><span class=p>,</span> <span class=n>drop_rate</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span></code></pre></div><p>The <code>drop_rate</code> hyperparameter is the key dial. Too low (&lt; 5%) and experts remain too correlated. Too high (> 30%) and you lose the functional initialization benefit — the expert essentially starts from random weights. The sweet spot we found empirically: <strong>8-12% for general language models, 15% for code-specialized models</strong> (where more aggressive diversity is needed to separate syntax vs. semantics experts).</p><h2 id=training-dynamics-implicit-specialization-signal>Training Dynamics: Implicit Specialization Signal<a hidden class=anchor aria-hidden=true href=#training-dynamics-implicit-specialization-signal>#</a></h2><p>Why do dropped experts specialize rather than just learn to patch their own holes? The mechanism is elegant.</p><p>After the first few training steps, the &ldquo;intact&rdquo; experts (those with more of the original dense weights) perform better on common tokens — they have a head start. The router, which is optimizing for overall performance, learns to send common tokens to the better-performing intact experts. The dropped experts receive a different token distribution: harder tokens, rarer constructs, edge cases that the intact experts handle poorly.</p><p>This is the <strong>implicit specialization signal</strong>: experts do not specialize by design, they specialize by default. Each expert optimizes for the token distribution it actually receives, and that distribution is different for each expert because their relative competencies differ. By 50K training steps, the expert specialization is measurable:</p><ul><li>Intact experts (low drop rate) converge toward high-frequency, syntactic functions</li><li>Heavily-dropped experts develop novel representations for rare or complex tokens</li></ul><p>The dense-to-MoE transition effectively turns a capability gap (some experts start worse) into a specialization signal (those experts become domain-specific).</p><h2 id=results-at-scale>Results at Scale<a hidden class=anchor aria-hidden=true href=#results-at-scale>#</a></h2><p>On the primary benchmark suite (comparing Drop-Upcycled MoE vs. MoE trained from scratch):</p><ul><li><strong>5.9B active parameters</strong>: Drop-Upcycled MoE achieves 13B-equivalent quality at 1/4 the training FLOPs</li><li><strong>MMLU</strong>: Drop-Upcycling reaches 75.4 vs. from-scratch MoE at 74.8 (at same FLOP budget)</li><li><strong>HumanEval</strong>: 68.2 vs. 65.1 — Drop-Upcycling is better here because code has cleaner specialization axes</li><li><strong>Training efficiency</strong>: 4x speedup to target quality vs. from-scratch MoE</li></ul><p>The 1/4 FLOP claim requires context: the dense checkpoint training cost is amortized. If you already have the dense model (which you do, because you trained it first), the incremental cost to get an MoE is roughly 1/4 of a from-scratch MoE run. The total cost (dense + MoE) is higher than from-scratch, but for organizations that already have dense checkpoints — which is everyone — the marginal cost argument is what matters.</p><h2 id=zen-mode-three-scales-of-application>Zen MoDE: Three Scales of Application<a hidden class=anchor aria-hidden=true href=#zen-mode-three-scales-of-application>#</a></h2><p>Zen MoDE (Mixture of Distilled Experts) applies Drop-Upcycling at three scales:</p><p><strong>zen4-mini (4B total, 4B active)</strong> — Dense. No upcycling needed at this scale; the routing overhead would dominate the compute savings. Zen4-mini uses a dense Qwen3 base.</p><p><strong>zen4-max (30B total, 3B active)</strong> — 16 experts, 2 active per token. Drop-Upcycled from an 8B dense checkpoint. Drop rate: 8%. Router: learned top-2 routing with load balancing. The transition from 8B dense to 30B MoE takes 200M training tokens, roughly 3 days on 32×H100.</p><p><strong>zen4-ultra (1T total, 32B active)</strong> — 384 experts, 8 active per token. This is our frontier model based on the Kimi K2.5 architecture. The upcycling here was done by the upstream team; we train behavioral adapters on top using GT-QLoRA (see the companion post on that technique).</p><h2 id=what-do-the-experts-actually-learn>What Do the Experts Actually Learn?<a hidden class=anchor aria-hidden=true href=#what-do-the-experts-actually-learn>#</a></h2><p>We ran expert attribution analysis on zen4-max after 500M post-upcycling training tokens. The methodology: for each expert, collect the 10K tokens that activate it most strongly and analyze the distribution.</p><p>The results cluster into recognizable domains:</p><table><thead><tr><th>Expert Group</th><th>Token Characteristics</th></tr></thead><tbody><tr><td>Experts 0-3</td><td>High-frequency English function words, punctuation</td></tr><tr><td>Experts 4-6</td><td>Code tokens: brackets, operators, keywords</td></tr><tr><td>Experts 7-9</td><td>Mathematical notation, numerals, equations</td></tr><tr><td>Experts 10-12</td><td>Multilingual tokens (Chinese, Arabic, Cyrillic)</td></tr><tr><td>Experts 13-15</td><td>Rare English words, technical terminology</td></tr></tbody></table><p>This is not designed specialization — it emerged from the implicit signal described above. The router discovered that routing code tokens to experts 4-6 produces better outputs than routing them to experts 0-3. No explicit supervision was provided.</p><h2 id=q-galore-for-moe-training-efficiency>Q-GaLore for MoE Training Efficiency<a hidden class=anchor aria-hidden=true href=#q-galore-for-moe-training-efficiency>#</a></h2><p>Training a 30B MoE requires careful memory management. We use Q-GaLore (Quantized Gradient Low-Rank Projection) for the upcycling phase:</p><ul><li>Gradient projection: instead of storing full gradients for all parameters, project them into a low-rank subspace (rank 128 for most layers)</li><li>Quantize the projected gradients to INT8 before accumulation</li><li>Result: 50% memory reduction vs. standard LoRA, with +5.19 MMLU points vs. QLoRA on equivalent compute</li></ul><p>The memory savings matter because upcycling requires loading both the dense checkpoint (for initialization) and the growing MoE checkpoint (for training) simultaneously. Q-GaLore makes this tractable on 8×A100 80GB configurations where it would otherwise OOM.</p><h2 id=research-frontier-progressive-router-pruning>Research Frontier: Progressive Router Pruning<a hidden class=anchor aria-hidden=true href=#research-frontier-progressive-router-pruning>#</a></h2><p>The current Drop-Upcycling approach creates experts of fixed capacity. An open question we are actively investigating: can you progressively prune the router to identify which experts are actually being used, then collapse unused capacity back into the shared parameters?</p><p>Early results suggest that after 500M training tokens, roughly 20-30% of experts receive less than 2% of routing probability across the evaluation corpus. These &ldquo;dormant&rdquo; experts can be pruned and their parameters absorbed back into the shared FFN without measurable quality degradation. This gives a dynamic MoE that starts dense, develops expert structure, and then self-compresses to its natural capacity.</p><p>The mechanism matters for continual learning: as new domains are added, dormant experts can be &ldquo;awakened&rdquo; and repurposed for the new domain rather than creating new experts from scratch. This connects Drop-Upcycling directly to the SuRe + OPCM continual learning stack described in our companion post.</p><hr><p><em>Zen LM is a joint initiative of Hanzo AI Inc. (Techstars &lsquo;17) and Zoo Labs Foundation (501c3).</em></p></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://qwenlm.github.io/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>