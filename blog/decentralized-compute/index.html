<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Decentralized Compute for AI Training | Zen LM</title><meta name=keywords content="Infrastructure,Training,Decentralization"><meta name=description content="How we're building a decentralized compute network for training large AI models."><meta name=author content="Zach Kelling"><link rel=canonical href=https://zenlm.org/blog/decentralized-compute/><link crossorigin=anonymous href=/assets/css/stylesheet.6c0865a4bc8dbcbd27d78777eb33b404568f7fbf3185cca7b978e5275a4dd049.css integrity="sha256-bAhlpLyNvL0n14d36zO0BFaPf78xhcynuXjlJ1pN0Ek=" rel="preload stylesheet" as=style><link rel=icon href=https://zenlm.org/favicon.png><link rel=apple-touch-icon href=https://zenlm.org/favicon.png><link rel=manifest href=https://zenlm.org/site.webmanifest><meta name=theme-color content="#615CED"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script><meta property="og:title" content="Decentralized Compute for AI Training"><meta property="og:description" content="How we're building a decentralized compute network for training large AI models."><meta property="og:type" content="article"><meta property="og:url" content="https://zenlm.org/blog/decentralized-compute/"><meta property="og:image" content="https://zenlm.org/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-08-05T10:00:00-08:00"><meta property="article:modified_time" content="2024-08-05T10:00:00-08:00"><meta property="og:site_name" content="Zen LM"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://zenlm.org/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Decentralized Compute for AI Training"><meta name=twitter:description content="How we're building a decentralized compute network for training large AI models."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://zenlm.org/blog/"},{"@type":"ListItem","position":2,"name":"Decentralized Compute for AI Training","item":"https://zenlm.org/blog/decentralized-compute/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Decentralized Compute for AI Training","name":"Decentralized Compute for AI Training","description":"How we're building a decentralized compute network for training large AI models.","keywords":["Infrastructure","Training","Decentralization"],"articleBody":"Training large AI models requires significant compute resources. These resources are concentrated in a few hyperscalers, creating bottlenecks and single points of control. Today we announce the Zoo Compute Network, a decentralized alternative.\nThe Compute Concentration Problem Current AI training is dominated by:\nCloud providers: AWS, GCP, Azure control most AI-grade compute Hardware scarcity: H100s have year-long waitlists High costs: Training GPT-4 class models costs $100M+ Geographic concentration: Most clusters are in a few regions This concentration creates risks:\nAccess barriers: Only well-funded organizations can train frontier models Single points of failure: Outages affect entire research programs Regulatory exposure: One jurisdiction can impact global AI development Vendor lock-in: Switching costs are enormous The Zoo Compute Network The Zoo Compute Network aggregates distributed GPU resources into a unified training substrate. Anyone with suitable hardware can contribute. Anyone can access the aggregated compute.\nArchitecture +------------------+ +------------------+ +------------------+ | Compute Node 1 | | Compute Node 2 | | Compute Node N | | (8x H100) | | (4x A100) | | (16x H100) | +--------+---------+ +--------+---------+ +--------+---------+ | | | v v v +-----------------------------------------------------------------------+ | Coordination Layer | | - Task scheduling | | - Gradient aggregation | | - Checkpoint management | | - Payment settlement | +-----------------------------------------------------------------------+ | v +-----------------------------------------------------------------------+ | Client Interface | | - Job submission | | - Progress monitoring | | - Result retrieval | +-----------------------------------------------------------------------+ Node Requirements Compute nodes must meet minimum specifications:\nTier GPU Memory Network Uptime SLA Bronze 4x A100 40GB 256GB 100 Gbps 95% Silver 8x A100 80GB 512GB 200 Gbps 99% Gold 8x H100 80GB 1TB 400 Gbps 99.5% Nodes are verified through proof-of-work benchmarks before joining the network.\nCoordination Protocol The coordination layer handles distributed training logistics:\nTask Scheduling\nJobs are divided into tasks and assigned to available nodes:\n# Job submission job = ComputeJob( model_config=model_config, data_config=data_config, training_config=training_config, budget_max=10000, # ZEN tokens ) job_id = await network.submit(job) The scheduler optimizes for:\nData locality (minimize transfers) Network topology (co-locate communicating nodes) Cost efficiency (use cheapest suitable nodes) Reliability (distribute across failure domains) Gradient Aggregation\nDistributed training requires gradient synchronization. The network supports:\nAll-reduce for data-parallel training Point-to-point for pipeline/tensor parallelism Asynchronous updates for fault tolerance Aggregation happens through a tree topology that minimizes bandwidth usage.\nCheckpoint Management\nTraining state is continuously checkpointed:\n# Automatic checkpointing checkpoint_config = CheckpointConfig( interval=1000, # steps storage=\"ipfs\", redundancy=3, ) Checkpoints are content-addressed and distributed. Training can resume from any node.\nEconomics For Compute Providers\nProviders stake ZEN tokens as collateral and earn rewards for:\nUptime (base reward) Computation completed (work reward) Network contribution (bandwidth bonus) Slashing occurs for:\nDowntime during committed periods Incorrect computation (detected via verification) Bandwidth violations Expected returns: 15-25% APY on staked tokens plus hardware depreciation coverage.\nFor Users\nUsers pay per compute-hour in ZEN tokens:\nTier Price (ZEN/GPU-hour) Approx USD Bronze 2.5 $5 Silver 4.0 $8 Gold 8.0 $16 Prices are market-determined through ongoing auctions. Users can specify maximum price and wait for availability.\nNetwork Fee\n5% of payments go to the network treasury, funding:\nProtocol development Security audits Community grants Verification How do we know compute was done correctly?\nSampling-Based Verification\nRandom subsets of computation are re-run by verifiers. Discrepancies trigger investigation:\nP(detection) = 1 - (1 - sample_rate)^n With 1% sampling and 100 steps, detection probability is 63%. With 5% sampling, it’s 99.4%.\nGradient Consistency\nAggregated gradients are checked for statistical anomalies. Fabricated gradients have detectable patterns.\nTrusted Execution (Optional)\nFor high-value jobs, nodes can run in TEE enclaves (SGX, TDX). Provides cryptographic attestation of correct execution.\nReal-World Performance We’ve run several training jobs on the network:\nZen-2-7B Training Duration: 3 weeks Nodes used: 24 (rotating pool of 40) Total compute: 8,400 GPU-hours Cost: 21,000 ZEN (~$42,000) Efficiency: 89% of centralized equivalent Embedding Model Training Duration: 5 days Nodes used: 8 Total compute: 960 GPU-hours Cost: 2,400 ZEN (~$4,800) Efficiency: 94% of centralized equivalent Efficiency losses come from coordination overhead and network heterogeneity. Ongoing optimizations are closing the gap.\nJoining the Network As a Compute Provider Hardware check: Verify your setup meets tier requirements Software install: Run the Zoo Compute daemon Stake: Lock ZEN tokens as collateral Benchmark: Complete verification benchmarks Operate: Maintain uptime and connectivity Documentation: docs.zoo.ngo/compute/providers\nAs a User Install client: pip install zoo-compute Fund account: Acquire ZEN tokens Submit jobs: Use API or CLI from zoo_compute import Client client = Client(api_key=\"...\") job = client.train( config=\"./training_config.yaml\", max_budget=5000, ) await job.wait() Documentation: docs.zoo.ngo/compute/users\nRoadmap Q3 2024: Public beta with 100+ nodes Q4 2024: Production launch, verification improvements Q1 2025: Cross-chain bridging for payments Q2 2025: TEE support for all tiers\nConclusion Decentralized compute is essential for decentralized AI. The Zoo Compute Network provides a permissionless, efficient, and verifiable substrate for training large models. As the network grows, it becomes more resilient and more accessible.\nCompute should be a utility, not a moat.\nZach Kelling is a co-founder of Zoo Labs Foundation.\n","wordCount":"812","inLanguage":"en","datePublished":"2024-08-05T10:00:00-08:00","dateModified":"2024-08-05T10:00:00-08:00","author":{"@type":"Person","name":"Zach Kelling"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://zenlm.org/blog/decentralized-compute/"},"publisher":{"@type":"Organization","name":"Zen LM","logo":{"@type":"ImageObject","url":"https://zenlm.org/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Zen LM (Alt + H)"><img src=https://zenlm.org/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://zeekay.blog title=zeekay><span>zeekay</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://hanzo.blog title=hanzo.blog><span>hanzo.blog</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://hanzo.ai/chat title="Try Zen Chat"><span>Try Zen Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://blog.zoo.ngo title="zoo blog"><span>zoo blog</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Decentralized Compute for AI Training</h1><div class=post-description>How we're building a decentralized compute network for training large AI models.</div><div class=post-meta><span title='2024-08-05 10:00:00 -0800 -0800'>August 5, 2024</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;812 words&nbsp;·&nbsp;Zach Kelling</div></div></div><main class=main><article class=post-single><div class=post-content><p>Training large AI models requires significant compute resources. These resources are concentrated in a few hyperscalers, creating bottlenecks and single points of control. Today we announce the Zoo Compute Network, a decentralized alternative.</p><h2 id=the-compute-concentration-problem>The Compute Concentration Problem<a hidden class=anchor aria-hidden=true href=#the-compute-concentration-problem>#</a></h2><p>Current AI training is dominated by:</p><ul><li><strong>Cloud providers</strong>: AWS, GCP, Azure control most AI-grade compute</li><li><strong>Hardware scarcity</strong>: H100s have year-long waitlists</li><li><strong>High costs</strong>: Training GPT-4 class models costs $100M+</li><li><strong>Geographic concentration</strong>: Most clusters are in a few regions</li></ul><p>This concentration creates risks:</p><ol><li><strong>Access barriers</strong>: Only well-funded organizations can train frontier models</li><li><strong>Single points of failure</strong>: Outages affect entire research programs</li><li><strong>Regulatory exposure</strong>: One jurisdiction can impact global AI development</li><li><strong>Vendor lock-in</strong>: Switching costs are enormous</li></ol><h2 id=the-zoo-compute-network>The Zoo Compute Network<a hidden class=anchor aria-hidden=true href=#the-zoo-compute-network>#</a></h2><p>The Zoo Compute Network aggregates distributed GPU resources into a unified training substrate. Anyone with suitable hardware can contribute. Anyone can access the aggregated compute.</p><h3 id=architecture>Architecture<a hidden class=anchor aria-hidden=true href=#architecture>#</a></h3><pre tabindex=0><code>+------------------+     +------------------+     +------------------+
|  Compute Node 1  |     |  Compute Node 2  |     |  Compute Node N  |
|  (8x H100)       |     |  (4x A100)       |     |  (16x H100)      |
+--------+---------+     +--------+---------+     +--------+---------+
         |                        |                        |
         v                        v                        v
+-----------------------------------------------------------------------+
|                         Coordination Layer                             |
|  - Task scheduling                                                     |
|  - Gradient aggregation                                                |
|  - Checkpoint management                                               |
|  - Payment settlement                                                  |
+-----------------------------------------------------------------------+
                                  |
                                  v
+-----------------------------------------------------------------------+
|                           Client Interface                             |
|  - Job submission                                                      |
|  - Progress monitoring                                                 |
|  - Result retrieval                                                    |
+-----------------------------------------------------------------------+
</code></pre><h3 id=node-requirements>Node Requirements<a hidden class=anchor aria-hidden=true href=#node-requirements>#</a></h3><p>Compute nodes must meet minimum specifications:</p><table><thead><tr><th>Tier</th><th>GPU</th><th>Memory</th><th>Network</th><th>Uptime SLA</th></tr></thead><tbody><tr><td>Bronze</td><td>4x A100 40GB</td><td>256GB</td><td>100 Gbps</td><td>95%</td></tr><tr><td>Silver</td><td>8x A100 80GB</td><td>512GB</td><td>200 Gbps</td><td>99%</td></tr><tr><td>Gold</td><td>8x H100 80GB</td><td>1TB</td><td>400 Gbps</td><td>99.5%</td></tr></tbody></table><p>Nodes are verified through proof-of-work benchmarks before joining the network.</p><h3 id=coordination-protocol>Coordination Protocol<a hidden class=anchor aria-hidden=true href=#coordination-protocol>#</a></h3><p>The coordination layer handles distributed training logistics:</p><p><strong>Task Scheduling</strong></p><p>Jobs are divided into tasks and assigned to available nodes:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Job submission</span>
</span></span><span class=line><span class=cl><span class=n>job</span> <span class=o>=</span> <span class=n>ComputeJob</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model_config</span><span class=o>=</span><span class=n>model_config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>data_config</span><span class=o>=</span><span class=n>data_config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>training_config</span><span class=o>=</span><span class=n>training_config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>budget_max</span><span class=o>=</span><span class=mi>10000</span><span class=p>,</span>  <span class=c1># ZEN tokens</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>job_id</span> <span class=o>=</span> <span class=k>await</span> <span class=n>network</span><span class=o>.</span><span class=n>submit</span><span class=p>(</span><span class=n>job</span><span class=p>)</span>
</span></span></code></pre></div><p>The scheduler optimizes for:</p><ul><li>Data locality (minimize transfers)</li><li>Network topology (co-locate communicating nodes)</li><li>Cost efficiency (use cheapest suitable nodes)</li><li>Reliability (distribute across failure domains)</li></ul><p><strong>Gradient Aggregation</strong></p><p>Distributed training requires gradient synchronization. The network supports:</p><ul><li>All-reduce for data-parallel training</li><li>Point-to-point for pipeline/tensor parallelism</li><li>Asynchronous updates for fault tolerance</li></ul><p>Aggregation happens through a tree topology that minimizes bandwidth usage.</p><p><strong>Checkpoint Management</strong></p><p>Training state is continuously checkpointed:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Automatic checkpointing</span>
</span></span><span class=line><span class=cl><span class=n>checkpoint_config</span> <span class=o>=</span> <span class=n>CheckpointConfig</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>interval</span><span class=o>=</span><span class=mi>1000</span><span class=p>,</span>  <span class=c1># steps</span>
</span></span><span class=line><span class=cl>    <span class=n>storage</span><span class=o>=</span><span class=s2>&#34;ipfs&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>redundancy</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>Checkpoints are content-addressed and distributed. Training can resume from any node.</p><h3 id=economics>Economics<a hidden class=anchor aria-hidden=true href=#economics>#</a></h3><p><strong>For Compute Providers</strong></p><p>Providers stake ZEN tokens as collateral and earn rewards for:</p><ul><li>Uptime (base reward)</li><li>Computation completed (work reward)</li><li>Network contribution (bandwidth bonus)</li></ul><p>Slashing occurs for:</p><ul><li>Downtime during committed periods</li><li>Incorrect computation (detected via verification)</li><li>Bandwidth violations</li></ul><p>Expected returns: 15-25% APY on staked tokens plus hardware depreciation coverage.</p><p><strong>For Users</strong></p><p>Users pay per compute-hour in ZEN tokens:</p><table><thead><tr><th>Tier</th><th>Price (ZEN/GPU-hour)</th><th>Approx USD</th></tr></thead><tbody><tr><td>Bronze</td><td>2.5</td><td>$5</td></tr><tr><td>Silver</td><td>4.0</td><td>$8</td></tr><tr><td>Gold</td><td>8.0</td><td>$16</td></tr></tbody></table><p>Prices are market-determined through ongoing auctions. Users can specify maximum price and wait for availability.</p><p><strong>Network Fee</strong></p><p>5% of payments go to the network treasury, funding:</p><ul><li>Protocol development</li><li>Security audits</li><li>Community grants</li></ul><h3 id=verification>Verification<a hidden class=anchor aria-hidden=true href=#verification>#</a></h3><p>How do we know compute was done correctly?</p><p><strong>Sampling-Based Verification</strong></p><p>Random subsets of computation are re-run by verifiers. Discrepancies trigger investigation:</p><pre tabindex=0><code>P(detection) = 1 - (1 - sample_rate)^n
</code></pre><p>With 1% sampling and 100 steps, detection probability is 63%. With 5% sampling, it&rsquo;s 99.4%.</p><p><strong>Gradient Consistency</strong></p><p>Aggregated gradients are checked for statistical anomalies. Fabricated gradients have detectable patterns.</p><p><strong>Trusted Execution (Optional)</strong></p><p>For high-value jobs, nodes can run in TEE enclaves (SGX, TDX). Provides cryptographic attestation of correct execution.</p><h2 id=real-world-performance>Real-World Performance<a hidden class=anchor aria-hidden=true href=#real-world-performance>#</a></h2><p>We&rsquo;ve run several training jobs on the network:</p><h3 id=zen-2-7b-training>Zen-2-7B Training<a hidden class=anchor aria-hidden=true href=#zen-2-7b-training>#</a></h3><ul><li><strong>Duration</strong>: 3 weeks</li><li><strong>Nodes used</strong>: 24 (rotating pool of 40)</li><li><strong>Total compute</strong>: 8,400 GPU-hours</li><li><strong>Cost</strong>: 21,000 ZEN (~$42,000)</li><li><strong>Efficiency</strong>: 89% of centralized equivalent</li></ul><h3 id=embedding-model-training>Embedding Model Training<a hidden class=anchor aria-hidden=true href=#embedding-model-training>#</a></h3><ul><li><strong>Duration</strong>: 5 days</li><li><strong>Nodes used</strong>: 8</li><li><strong>Total compute</strong>: 960 GPU-hours</li><li><strong>Cost</strong>: 2,400 ZEN (~$4,800)</li><li><strong>Efficiency</strong>: 94% of centralized equivalent</li></ul><p>Efficiency losses come from coordination overhead and network heterogeneity. Ongoing optimizations are closing the gap.</p><h2 id=joining-the-network>Joining the Network<a hidden class=anchor aria-hidden=true href=#joining-the-network>#</a></h2><h3 id=as-a-compute-provider>As a Compute Provider<a hidden class=anchor aria-hidden=true href=#as-a-compute-provider>#</a></h3><ol><li><strong>Hardware check</strong>: Verify your setup meets tier requirements</li><li><strong>Software install</strong>: Run the Zoo Compute daemon</li><li><strong>Stake</strong>: Lock ZEN tokens as collateral</li><li><strong>Benchmark</strong>: Complete verification benchmarks</li><li><strong>Operate</strong>: Maintain uptime and connectivity</li></ol><p>Documentation: docs.zoo.ngo/compute/providers</p><h3 id=as-a-user>As a User<a hidden class=anchor aria-hidden=true href=#as-a-user>#</a></h3><ol><li><strong>Install client</strong>: <code>pip install zoo-compute</code></li><li><strong>Fund account</strong>: Acquire ZEN tokens</li><li><strong>Submit jobs</strong>: Use API or CLI</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>zoo_compute</span> <span class=kn>import</span> <span class=n>Client</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>Client</span><span class=p>(</span><span class=n>api_key</span><span class=o>=</span><span class=s2>&#34;...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>job</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>train</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>config</span><span class=o>=</span><span class=s2>&#34;./training_config.yaml&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_budget</span><span class=o>=</span><span class=mi>5000</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>await</span> <span class=n>job</span><span class=o>.</span><span class=n>wait</span><span class=p>()</span>
</span></span></code></pre></div><p>Documentation: docs.zoo.ngo/compute/users</p><h2 id=roadmap>Roadmap<a hidden class=anchor aria-hidden=true href=#roadmap>#</a></h2><p><strong>Q3 2024</strong>: Public beta with 100+ nodes
<strong>Q4 2024</strong>: Production launch, verification improvements
<strong>Q1 2025</strong>: Cross-chain bridging for payments
<strong>Q2 2025</strong>: TEE support for all tiers</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Decentralized compute is essential for decentralized AI. The Zoo Compute Network provides a permissionless, efficient, and verifiable substrate for training large models. As the network grows, it becomes more resilient and more accessible.</p><p>Compute should be a utility, not a moat.</p><hr><p><em>Zach Kelling is a co-founder of Zoo Labs Foundation.</em></p></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://zenlm.org/>Zen LM</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>