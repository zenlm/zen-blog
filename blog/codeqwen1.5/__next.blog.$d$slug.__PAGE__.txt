1:"$Sreact.fragment"
2:I[10086,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/36bfed0236ce2cf2.js","/_next/static/chunks/e62b91212ee7f8ff.js","/_next/static/chunks/2a98816c7d26bf58.js","/_next/static/chunks/cb0a883bafeb6805.js"],""]
3:I[48068,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/36bfed0236ce2cf2.js","/_next/static/chunks/e62b91212ee7f8ff.js","/_next/static/chunks/2a98816c7d26bf58.js","/_next/static/chunks/cb0a883bafeb6805.js"],"default"]
1f:I[51504,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/36bfed0236ce2cf2.js","/_next/static/chunks/e62b91212ee7f8ff.js","/_next/static/chunks/2a98816c7d26bf58.js","/_next/static/chunks/cb0a883bafeb6805.js"],"CodeBlock"]
20:I[51504,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/36bfed0236ce2cf2.js","/_next/static/chunks/e62b91212ee7f8ff.js","/_next/static/chunks/2a98816c7d26bf58.js","/_next/static/chunks/cb0a883bafeb6805.js"],"Pre"]
21:I[89923,["/_next/static/chunks/4d80e004cf4896dd.js","/_next/static/chunks/350ee4303b732916.js"],"OutletBoundary"]
22:"$Sreact.suspense"
0:{"buildId":"o_J3cFnAZ1mdL_cv2bxKY","rsc":["$","$1","c",{"children":[["$","main",null,{"className":"mx-auto w-full max-w-2xl px-4 py-16","children":[["$","$L2",null,{"href":"/blog","className":"inline-flex items-center gap-1.5 text-sm text-fd-muted-foreground hover:text-fd-foreground mb-10 transition-colors","children":"← Back to Blog"}],["$","div",null,{"className":"mb-8","children":[["$","time",null,{"className":"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider","children":"April 15, 2024"}],["$","h1",null,{"className":"text-3xl font-bold mt-2 mb-3","children":"Code with CodeQwen1.5"}],["$","p",null,{"className":"text-fd-muted-foreground text-lg mb-4","children":"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD"}],["$","div",null,{"className":"flex items-center gap-3 pt-4 border-t border-fd-border","children":[["$","span",null,{"className":"text-sm text-fd-muted-foreground","children":["By ","Zen LM Team"]}],["$","div",null,{"className":"flex gap-1.5 ml-auto","children":[]}]]}]]}],["$","div",null,{"className":"prose dark:prose-invert max-w-none","children":[["$","p",null,{"children":[["$","$L3",null,{"href":"https://github.com/QwenLM/CodeQwen1.5","children":"GITHUB"}]," ",["$","$L3",null,{"href":"https://huggingface.co/Qwen","children":"HUGGING FACE"}]," ",["$","$L3",null,{"href":"https://modelscope.cn/organization/qwen","children":"MODELSCOPE"}]," ",["$","$L3",null,{"href":"https://huggingface.co/spaces/Qwen/CodeQwen1.5-7b-Chat-demo","children":"DEMO"}]," ",["$","$L3",null,{"href":"https://discord.gg/yPEP2vHTu4","children":"DISCORD"}]]}],"\n",["$","h1",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"introduction","children":[["$","a",null,{"data-card":"","href":"#introduction","className":"peer","children":"Introduction"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}],"\n",["$","p",null,{"children":"The advent of advanced programming tools, which harnesses the power of large language models (LLMs), has significantly enhanced programmer productivity and accuracy. Notwithstanding these advancements, dominant coding assistants like Github Copilot, built upon proprietary LLMs, pose notable challenges in terms of cost, privacy, security, and potential copyright infringement. Recognizing the imperative for a more transparent and accessible alternative, the open-source community has embarked on a concerted endeavor to develop open codeLLMs. This initiative has already given rise to several promising open-source models, including StarCoder2, CodeLlama, and DeepSeek-Coder, offering a path forward, albeit one that necessitates continued refinement."}],"\n",["$","p",null,{"children":"Today, we are delighted to introduce a new member of the Qwen1.5 open-source family, the CodeQwen1.5-7B, a specialized codeLLM built upon the Qwen1.5 language model. CodeQwen1.5-7B has been pretrained with around 3 trillion tokens of code-related data. It supports an extensive repertoire of 92 programming languages, and it exhibits exceptional capacity in long-context understanding and generation with the ability to process information of 64K tokens. In terms of performance, CodeQwen1.5 demonstrates impressive capabilities in basic code generation, long-context modeliing, code editation and SQL. We believe this model can significantly enhance developer productivity and streamline software development workflows within diverse technological environments."}],"\n","$L4","\n","$L5","\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","$L6","\n","$L7","\n","$L8","\n","$L9","\n","$La","\n","$Lb","\n","$Lc","\n","$Ld","\n","$Le","\n","$Lf","\n","$L10","\n","$L11","\n","$L12","\n","$L13","\n","$L14","\n","$L15","\n","$L16","\n","$L17","\n","$L18","\n","$L19"]}]]}],["$L1a","$L1b","$L1c","$L1d"],"$L1e"]}],"loading":null,"isPartial":false}
4:["$","h1",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"codeqwen-is-a-basic-coder","children":[["$","a",null,{"data-card":"","href":"#codeqwen-is-a-basic-coder","className":"peer","children":"CodeQwen is a Basic Coder"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
5:["$","p",null,{"children":"Code generation is a key competence for large language models, as they are tasked with translating natural language instructions into executable code with unwavering precision. CodeQwen1.5, with only 7 billion parameters, has surpassed larger models in basic code generation capabilities, further narrowing the gap in coding proficiency between GPT-4 and opensource code LLMs. We conducted a thorough evaluation on HumanEval and MBPP to provide a clear and fair comparison as follows."}]
6:["$","div",null,{"className":"relative overflow-auto prose-no-margin my-6","children":["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"Model"}],["$","th",null,{"children":"Size"}],["$","th",null,{"children":"HumanEval0-shot"}],["$","th",null,{"children":"HumanEval+0-shot"}],["$","th",null,{"children":"MBPP0-shot"}],["$","th",null,{"children":"MBPP+0-shot"}],["$","th",null,{"children":"MBPP3-shot"}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"Base Model"}]}],["$","td",null,{}],["$","td",null,{}],["$","td",null,{}],["$","td",null,{}],["$","td",null,{}],["$","td",null,{}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"CodeLlama-Base"}],["$","td",null,{"children":"7B"}],["$","td",null,{"children":"33.5"}],["$","td",null,{"children":"25.6"}],["$","td",null,{"children":"52.1"}],["$","td",null,{"children":"41.6"}],["$","td",null,{"children":"38.6"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"StarCoder2"}],["$","td",null,{"children":"7B"}],["$","td",null,{"children":"35.4"}],["$","td",null,{"children":"29.9"}],["$","td",null,{"children":"54.4"}],["$","td",null,{"children":"45.6"}],["$","td",null,{"children":"51.0"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"DeepSeek-Coder-Base"}],["$","td",null,{"children":"6.7B"}],["$","td",null,{"children":"47.6"}],["$","td",null,{"children":"39.6"}],["$","td",null,{"children":"70.2"}],["$","td",null,{"children":"56.6"}],["$","td",null,{"children":"60.6"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"CodeQwen1.5"}]}],["$","td",null,{"children":"7B"}],["$","td",null,{"children":"51.8"}],["$","td",null,{"children":"45.7"}],["$","td",null,{"children":"72.2"}],["$","td",null,{"children":"60.2"}],["$","td",null,{"children":"61.8"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"Chat Model"}]}],["$","td",null,{}],["$","td",null,{}],["$","td",null,{}],["$","td",null,{}],["$","td",null,{}],["$","td",null,{}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"GPT-3.5-Turbo"}],["$","td",null,{"children":"-"}],["$","td",null,{"children":"76.8"}],["$","td",null,{"children":"70.7"}],["$","td",null,{"children":"82.5"}],["$","td",null,{"children":"69.7"}],["$","td",null,{"children":"70.8"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"GPT-4-Turbo (Nov 2023)"}],["$","td",null,{"children":"-"}],["$","td",null,{"children":"85.4"}],["$","td",null,{"children":"81.7"}],["$","td",null,{"children":"83.5"}],["$","td",null,{"children":"70.7"}],["$","td",null,{"children":"80.0"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"DeepSeek-Coder-Instruct"}],["$","td",null,{"children":"6.7B"}],["$","td",null,{"children":"78.6"}],["$","td",null,{"children":"70.1"}],["$","td",null,{"children":"73.2"}],["$","td",null,{"children":"63.4"}],["$","td",null,{"children":"65.4"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"CodeQwen1.5-Chat"}]}],["$","td",null,{"children":"7B"}],["$","td",null,{"children":"83.5"}],["$","td",null,{"children":"78.7"}],["$","td",null,{"children":"77.7"}],["$","td",null,{"children":"67.2"}],["$","td",null,{"children":"70.6"}]]}]]}]]}]}]
7:["$","p",null,{"children":"In addition to the widely recognized HumanEval and MBPP benchmarks, we explored LiveCodeBench. This benchmark assesses code performance by introducing fresh challenges sourced from coding competitions such as LeetCode, AtCoder, and CodeForces over time. Our evaluation of CodeQwen1.5 on LiveCodeBench spanned from September 1, 2023, to April 1, 2024. The findings indicate that CodeQwen1.5 ranks among the top open-access models currently available. Note: it is possible that the inclusion of LeetCode data in our pretraining corpus may contribute to the performance in LiveCodeBench."}]
8:["$","p",null,{"children":"The evaluations mentioned primarily revolve around Python capabilities; however, CodeQwen1.5 is not merely a Python specialist but also an expert across multiple programming languages. We conducted a comprehensive evaluation of CodeQwen1.5 in the eight mainstream languages featured in MultiPL-E, including Python, C++, Java, PHP, TypeScript, C#, Bash, and JavaScript. The results highlight the exceptional programming capabilities of CodeQwen1.5."}]
9:["$","h1",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"codeqwen-is-a-long-context-coder","children":[["$","a",null,{"data-card":"","href":"#codeqwen-is-a-long-context-coder","className":"peer","children":"CodeQwen is a Long Context Coder"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
a:["$","p",null,{"children":"Long context capability is crucial for code LLMs, serving as the core skill for understanding repository-level code and becoming a code agent. However, current code models still have very limited support for length, which hinders their potential for practical application. CodeQwen1.5 aims to further advance the progress of open-source code models in long context modeling. To achieve this, we have collected and constructed long sequence code data at the repository level for pre-training. Through careful data proportioning and organization, we have enabled it to support input lengths of up to 64K tokens."}]
b:["$","p",null,{"children":[["$","strong",null,{"children":"Evaluation 1"}]," : We collected high-quality repo from GitHub Trending repositories on 2024-3-28 that were not included in CodeQwen1.5’s training data to observe the effectiveness of long context modeling. The following figure demonstrates that as the sequence length increases, CodeQwen1.5’s Perplexity (PPL) still manages to maintain a downward trend."]}]
c:["$","p",null,{"children":[["$","strong",null,{"children":"Evaluation 2"}]," : We created a synthetic task called ",["$","code",null,{"children":"Needle in the Code"}],", inspired by popular long-context evaluations in the text domain. In this task, we inserted a very simple custom function at various positions within a longer codebase (we chose Megatron to honor its contributions to open-source LLMs!) and tested whether the model could replicate this function at the end of the codebase. The figure below shows that CodeQwen is capable of successfully completing this task within a 64k length range."]}]
d:["$","p",null,{"children":"Both Evaluation 1 and Evaluation 2 serve as initial and foundational assessments. For the Chat model, we aim to evaluate its long context capabilities with more practical tasks. However, our objective is to examine the Chat model’s capability to handle long contexts through more pragmatic, real-world evaluation tasks."}]
e:["$","p",null,{"children":[["$","strong",null,{"children":"Evaluation 3"}]," : SWE Bench is a benchmark designed to assess the ability of Large Language Models (LLMs) or agents to tackle practical software development challenges. It presents contestants with a code repository and an associated issue, tasking them with generating a commit patch that resolves the issue effectively. The benchmark uniquely emphasizes the long-context processing capabilities of code LLMs, necessitating both deep comprehension of the given codebase and the generation of extensive, unit-test-passing code."]}]
f:["$","p",null,{"children":"Currently, participants in the SWE Bench competition predominantly are proprietary models. We introduce CodeQwen1.5 as an open-source model entry. Despite achieving a score of 0.89, CodeQwen1.5 surpasses ChatGPT-3.5, demonstrating the nascent yet promising competitiveness of open-source code models against their proprietary counterparts."}]
10:["$","h1",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"codeqwen-is-a-debugger","children":[["$","a",null,{"data-card":"","href":"#codeqwen-is-a-debugger","className":"peer","children":"CodeQwen is a Debugger"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
11:["$","p",null,{"children":"An effective code assistant must demonstrate proficiency in both generating code in response to given specifications and adeptly modifying or debugging existing code to accommodate evolving requirements or rectify errors. In assessing CodeQwen1.5’s proficiency in code modification tasks, we concentrated our evaluation on the CodeEditorBench suite, encompassing four distinct dimensions: Debugging, Translation, Language Switching, and Code Polishing. The results indicate that CodeQwen1.5 achieves the SOTA performance at the 7 billion parameter scale."}]
12:["$","h1",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"codeqwen-is-a-sqler","children":[["$","a",null,{"data-card":"","href":"#codeqwen-is-a-sqler","className":"peer","children":"CodeQwen is a SQLer"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
13:["$","p",null,{"children":"CodeQwen1.5 serves as a solution to bridge the gap between non-programming professionals and efficient data interaction. It alleviates the steep learning curve associated with SQL by enabling users without coding expertise to query databases through natural language. We evaluated CodeQwen1.5-Chat’s performance on two popular Text-to-SQL benchmarks, Spider and Bird. Experimental results pose CodeQwen1.5 a second position close to GPT-4 (results come from DIN-SQL, a SOTA prompting method). This outstanding performance is attributed to the utilization of synthetic data throughout both pre-training and fine-tuning stages. Synthetic data, characterized by its scalability, verifiability, and variety, emerges as a compelling area for future research due to its proven effectiveness in enhancing CodeQwen1.5’s SQL capabilities."}]
14:["$","h1",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"develop-with-codeqwen15","children":[["$","a",null,{"data-card":"","href":"#develop-with-codeqwen15","className":"peer","children":"Develop with CodeQwen1.5"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
15:["$","p",null,{"children":["CodeQwen1.5 is part of the Qwen1.5 open-source family. We advise you to read our blog for ",["$","$L3",null,{"href":"https://qwenlm.github.io/blog/qwen1.5/","children":"Qwen1.5"}]," to figure out the usages with Transformers, vLLM, llama.cpp, Ollama, etc."]}]
16:["$","h1",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"conclusion","children":[["$","a",null,{"data-card":"","href":"#conclusion","className":"peer","children":"Conclusion"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
17:["$","p",null,{"children":"We have released CodeQwen1.5-7B and CodeQwen1.5-7B-Chat, an open and versatile code LLM. The models are intended to aid progress in code assistance and code agents, benefiting the research community. We’ll keep investing heavily in smart code development, with the ultimate goal of creating AI programmers."}]
18:["$","h1",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"citation","children":[["$","a",null,{"data-card":"","href":"#citation","className":"peer","children":"Citation"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
19:["$","$L1f",null,{"className":"shiki shiki-themes github-light github-dark","style":{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},"tabIndex":"0","icon":"<svg viewBox=\"0 0 24 24\"><path d=\"M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z\" fill=\"currentColor\" /></svg>","children":["$","$L20",null,{"children":["$","code",null,{"children":[["$","span",null,{"className":"line","children":["$","span",null,{}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"    @misc{codeqwen1.5,"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"        title = {Code with CodeQwen1.5},"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"        url = {https://qwenlm.github.io/blog/codeqwen1.5/},"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"        author = {Qwen Team},"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"        month = {April},"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"        year = {2024}"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"    }"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"    "}]}]]}]}]}]
1a:["$","script","script-0",{"src":"/_next/static/chunks/36bfed0236ce2cf2.js","async":true}]
1b:["$","script","script-1",{"src":"/_next/static/chunks/e62b91212ee7f8ff.js","async":true}]
1c:["$","script","script-2",{"src":"/_next/static/chunks/2a98816c7d26bf58.js","async":true}]
1d:["$","script","script-3",{"src":"/_next/static/chunks/cb0a883bafeb6805.js","async":true}]
1e:["$","$L21",null,{"children":["$","$22",null,{"name":"Next.MetadataOutlet","children":"$@23"}]}]
23:null
