<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta http-equiv=refresh content="5; url=https://qwen.ai/blog?id=zen-3"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Zen 3.0: The Next Generation of Open AI | Qwen</title><meta name=keywords content="Announcement,Models"><meta name=description content="Announcing Zen 3.0, our most capable open model family yet."><meta name=author content="Zach Kelling"><link rel=canonical href=https://qwenlm.github.io/blog/zen-3/><link crossorigin=anonymous href=/assets/css/stylesheet.6c0865a4bc8dbcbd27d78777eb33b404568f7fbf3185cca7b978e5275a4dd049.css integrity="sha256-bAhlpLyNvL0n14d36zO0BFaPf78xhcynuXjlJ1pN0Ek=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/zen-3/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script><meta property="og:title" content="Zen 3.0: The Next Generation of Open AI"><meta property="og:description" content="Announcing Zen 3.0, our most capable open model family yet."><meta property="og:type" content="article"><meta property="og:url" content="https://qwenlm.github.io/blog/zen-3/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-01-13T09:00:00-08:00"><meta property="article:modified_time" content="2025-01-13T09:00:00-08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Zen 3.0: The Next Generation of Open AI"><meta name=twitter:description content="Announcing Zen 3.0, our most capable open model family yet."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Zen 3.0: The Next Generation of Open AI","item":"https://qwenlm.github.io/blog/zen-3/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Zen 3.0: The Next Generation of Open AI","name":"Zen 3.0: The Next Generation of Open AI","description":"Announcing Zen 3.0, our most capable open model family yet.","keywords":["Announcement","Models"],"articleBody":"Today we release Zen 3.0, our third-generation language model family. Zen 3 represents a step change in what open models can do.\nModel Family Zen 3 comes in several sizes:\nModel Parameters Context Training Tokens Zen-3-8B 8.1B 128K 15T Zen-3-32B 32.5B 128K 12T Zen-3-72B 72.3B 128K 10T Zen-3-MoE 141B (24B active) 128K 14T All models use the same architecture with scaled dimensions. All are released under Apache 2.0.\nArchitecture Highlights Extended Context All Zen 3 models support 128K token context natively:\nRoPE extensions: Position interpolation with NTK-aware scaling Sliding window attention: Efficient processing of long sequences Memory-efficient attention: FlashAttention-2 throughout Long context isn’t just about the number; it’s about actually using it. Our needle-in-haystack evaluation shows \u003e95% retrieval accuracy at 100K tokens.\nMixture of Experts Zen-3-MoE uses a sparse architecture:\n64 experts per layer Top-2 routing with load balancing 24B active parameters (141B total) Achieves 72B-dense quality at 32B-dense cost Expert parallelism enables efficient inference on consumer hardware.\nImproved Tokenizer The Zen 3 tokenizer improves on previous versions:\n128K vocabulary (up from 32K) Better multilingual coverage Improved code tokenization Reduced fertility for technical content Larger vocabulary means fewer tokens per document means longer effective context.\nCapability Improvements Benchmarks Benchmark Zen-2-70B Zen-3-72B Improvement MMLU 74.2 82.1 +7.9 GSM8K 68.4 84.7 +16.3 HumanEval 58.5 71.3 +12.8 HellaSwag 85.1 89.4 +4.3 MATH 32.6 51.2 +18.6 The improvements are substantial across all categories. Math and coding see the largest gains.\nReal-World Tasks Benchmarks don’t tell the whole story. Zen 3 excels at:\nLong-form writing: Coherent documents spanning thousands of words with consistent style and structure.\nMulti-step reasoning: Complex problems requiring planning and backtracking.\nCode generation: Full functions and classes, not just snippets.\nInstruction following: Precise adherence to formatting and constraint requirements.\nMultilingual: Strong performance in 30+ languages including low-resource ones.\nAgentic Capabilities Zen 3 is designed for agent use cases:\nTool use: Reliable function calling with schema adherence Planning: Multi-step task decomposition Memory integration: Designed for RAG and experience ledgers Self-correction: Recognizes and recovers from errors Early agent benchmarks show 2x improvement over Zen 2 on multi-step tasks.\nTraining Details Data Training data evolved significantly:\nQuality filtering: Improved classifiers for content quality Deduplication: Near-duplicate removal at document and paragraph level Synthetic data: 20% of training tokens from LLM-generated content Code emphasis: 15% code (up from 8% in Zen 2) Instruction mixing: 5% instruction data during pretraining Total: 15T tokens for the 8B model, proportionally less for larger models.\nTraining Process Training used the Zoo Compute Network:\nDuration: 4 months Peak nodes: 2,048 H100 GPUs Total compute: 3.2 million GPU-hours Efficiency: 47% MFU average The training run was the largest yet on the decentralized network. It validated that frontier training is possible without centralized infrastructure.\nAlignment Post-training alignment followed our standard process:\nSupervised fine-tuning: 100K high-quality instruction examples GRPO: Group Relative Policy Optimization on preference data Constitutional training: Principle-based refinement Red teaming: Adversarial testing with remediation Alignment reduced benchmark scores slightly (2-3%) while significantly improving real-world usefulness.\nSafety Evaluation All Zen 3 models passed our safety evaluation suite:\nRefusal Rates Category Zen-2-70B Zen-3-72B Violence instructions 99.2% 99.7% CSAM 100% 100% Malware 97.8% 99.1% PII extraction 94.6% 98.3% Improved refusal with fewer false positives on legitimate requests.\nBias Metrics We evaluated on standard bias benchmarks:\nBBQ: 89.2% accuracy (vs. 84.1% for Zen 2) WinoBias: 76.4% anti-stereotype (vs. 71.2%) Toxicity: 0.023 average score (vs. 0.041) Improvements through both data curation and RLHF.\nLimitations Zen 3 is not perfect:\nCan still be jailbroken with sufficient effort May hallucinate facts, especially for recent events Long-context retrieval degrades past 100K tokens Some languages underperform (especially non-Latin scripts) Resource-intensive for edge deployment We publish these limitations because transparency enables responsible use.\nUsage Hugging Face from transformers import AutoModelForCausalLM, AutoTokenizer model = AutoModelForCausalLM.from_pretrained( \"zoo-labs/zen-3-72b\", torch_dtype=torch.bfloat16, device_map=\"auto\", ) tokenizer = AutoTokenizer.from_pretrained(\"zoo-labs/zen-3-72b\") output = model.generate( tokenizer(\"Hello, Zen!\", return_tensors=\"pt\").input_ids, max_new_tokens=100, ) vLLM from vllm import LLM, SamplingParams llm = LLM(model=\"zoo-labs/zen-3-72b\") outputs = llm.generate([\"Hello, Zen!\"], SamplingParams(max_tokens=100)) Quantized Versions For resource-constrained deployment:\nzen-3-72b-AWQ: 4-bit quantization, minimal quality loss zen-3-72b-GPTQ: Alternative 4-bit format zen-3-72b-GGUF: llama.cpp compatible The 8B model runs on consumer GPUs. The 72B quantized fits in 48GB.\nWhat’s Next Zen 3 is a foundation. Coming soon:\nZen-3-Vision: Multimodal variant with image understanding Zen-3-Code: Specialized coding model Zen-3-Long: 1M+ context extension Zen-3-Agent: Optimized for agentic workflows The foundation is strong. Now we build.\nAcknowledgments Zen 3 was trained on the Zoo Compute Network with contributions from 847 node operators across 34 countries. Thank you.\nThis release was funded through the Zoo Labs Foundation treasury, allocated by community vote (ZIP-72). Thank you to all token holders who participated in governance.\nSpecial thanks to the training, alignment, and evaluation teams who made this possible.\nDownload at huggingface.co/zoo-labs.\nZach Kelling is a co-founder of Zoo Labs Foundation.\n","wordCount":"787","inLanguage":"en","datePublished":"2025-01-13T09:00:00-08:00","dateModified":"2025-01-13T09:00:00-08:00","author":{"@type":"Person","name":"Zach Kelling"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qwenlm.github.io/blog/zen-3/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://qwenlm.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><style>.modal-overlay{position:fixed;top:0;left:0;width:100%;height:100%;background-color:rgba(0,0,0,.5);display:flex;align-items:center;z-index:1000;animation:fadeIn .3s ease-in-out}.modal-container{margin-left:auto;margin-right:auto;background-color:var(--theme);border-radius:8px;box-shadow:0 4px 20px rgba(0,0,0,.15);width:90%;max-width:420px;height:fit-content;padding:30px;text-align:center;position:relative;animation:slideIn .4s ease-out}.modal-container a{color:var(--hero2)}.modal-icon{width:70px;height:70px;background-color:#f0f7ff;border-radius:50%;display:flex;align-items:center;justify-content:center;margin:0 auto 20px;color:#1a73e8;font-size:30px}.modal-title{font-size:1.5rem;font-weight:600;color:var(--primary);margin:0 0 15px}.modal-message{font-size:1rem;color:var(--secondary);line-height:1.5;margin:0 0 25px}.countdown{font-size:1.2rem;color:#666;margin:20px 0;font-weight:500}.modal-buttons{display:flex;justify-content:center;gap:15px;margin-top:25px}.modal-buttons .btn{padding:6px 16px;border-radius:8px;font-size:1.2rem;font-weight:500;cursor:pointer;transition:all .3s ease;border:none}.btn-primary{background-color:#1a73e8;color:#fff}.btn-primary:hover{background-color:#1557b0}.btn-secondary{background-color:#f1f3f4;color:#333}.btn-secondary:hover{background-color:#e0e0e0}@keyframes fadeIn{from{opacity:0}to{opacity:1}}@keyframes slideIn{from{opacity:0;transform:translateY(-50px)}to{opacity:1;transform:translateY(0)}}@media(max-width:480px){.modal-container{max-width:95%;width:calc(95vw - 40px);padding:20px}}</style><div class=modal-overlay><div class=modal-container><div class=modal-icon><svg xmlns="http://www.w3.org/2000/svg" width="36" height="36" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71"/></svg></div><h2 class=modal-title>We have a new blog!<br>View this page at <a href="https://qwen.ai/blog?id=zen-3">qwen.ai</a>.</h2><p class=modal-message>This page will automatically redirect in <span class=countdown id=countdown>5</span> seconds.</p><p class=modal-message>If you are not redirected automatically, please click the button below.</p><div class=modal-buttons><button class="btn btn-primary" onclick=redirectToPage()>Go Now</button></div></div></div><script>let countdown=5;const countdownElement=document.getElementById("countdown"),timer=setInterval(()=>{countdown--,countdownElement.textContent=countdown,countdown<=0&&clearInterval(timer)},1e3);function stayHere(){document.querySelector(".modal-overlay").style.display="none"}function redirectToPage(){window.location.href="https://qwen.ai/blog?id=zen-3"}</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://chat.qwen.ai title="Try Qwen Chat"><span>Try Qwen Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Zen 3.0: The Next Generation of Open AI</h1><div class=post-description>Announcing Zen 3.0, our most capable open model family yet.</div><div class=post-meta><span title='2025-01-13 09:00:00 -0800 -0800'>January 13, 2025</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;787 words&nbsp;·&nbsp;Zach Kelling</div></div></div><main class=main><article class=post-single><div class=post-content><p>Today we release Zen 3.0, our third-generation language model family. Zen 3 represents a step change in what open models can do.</p><h2 id=model-family>Model Family<a hidden class=anchor aria-hidden=true href=#model-family>#</a></h2><p>Zen 3 comes in several sizes:</p><table><thead><tr><th>Model</th><th>Parameters</th><th>Context</th><th>Training Tokens</th></tr></thead><tbody><tr><td>Zen-3-8B</td><td>8.1B</td><td>128K</td><td>15T</td></tr><tr><td>Zen-3-32B</td><td>32.5B</td><td>128K</td><td>12T</td></tr><tr><td>Zen-3-72B</td><td>72.3B</td><td>128K</td><td>10T</td></tr><tr><td>Zen-3-MoE</td><td>141B (24B active)</td><td>128K</td><td>14T</td></tr></tbody></table><p>All models use the same architecture with scaled dimensions. All are released under Apache 2.0.</p><h2 id=architecture-highlights>Architecture Highlights<a hidden class=anchor aria-hidden=true href=#architecture-highlights>#</a></h2><h3 id=extended-context>Extended Context<a hidden class=anchor aria-hidden=true href=#extended-context>#</a></h3><p>All Zen 3 models support 128K token context natively:</p><ul><li><strong>RoPE extensions</strong>: Position interpolation with NTK-aware scaling</li><li><strong>Sliding window attention</strong>: Efficient processing of long sequences</li><li><strong>Memory-efficient attention</strong>: FlashAttention-2 throughout</li></ul><p>Long context isn&rsquo;t just about the number; it&rsquo;s about actually using it. Our needle-in-haystack evaluation shows >95% retrieval accuracy at 100K tokens.</p><h3 id=mixture-of-experts>Mixture of Experts<a hidden class=anchor aria-hidden=true href=#mixture-of-experts>#</a></h3><p>Zen-3-MoE uses a sparse architecture:</p><ul><li>64 experts per layer</li><li>Top-2 routing with load balancing</li><li>24B active parameters (141B total)</li><li>Achieves 72B-dense quality at 32B-dense cost</li></ul><p>Expert parallelism enables efficient inference on consumer hardware.</p><h3 id=improved-tokenizer>Improved Tokenizer<a hidden class=anchor aria-hidden=true href=#improved-tokenizer>#</a></h3><p>The Zen 3 tokenizer improves on previous versions:</p><ul><li>128K vocabulary (up from 32K)</li><li>Better multilingual coverage</li><li>Improved code tokenization</li><li>Reduced fertility for technical content</li></ul><p>Larger vocabulary means fewer tokens per document means longer effective context.</p><h2 id=capability-improvements>Capability Improvements<a hidden class=anchor aria-hidden=true href=#capability-improvements>#</a></h2><h3 id=benchmarks>Benchmarks<a hidden class=anchor aria-hidden=true href=#benchmarks>#</a></h3><table><thead><tr><th>Benchmark</th><th>Zen-2-70B</th><th>Zen-3-72B</th><th>Improvement</th></tr></thead><tbody><tr><td>MMLU</td><td>74.2</td><td>82.1</td><td>+7.9</td></tr><tr><td>GSM8K</td><td>68.4</td><td>84.7</td><td>+16.3</td></tr><tr><td>HumanEval</td><td>58.5</td><td>71.3</td><td>+12.8</td></tr><tr><td>HellaSwag</td><td>85.1</td><td>89.4</td><td>+4.3</td></tr><tr><td>MATH</td><td>32.6</td><td>51.2</td><td>+18.6</td></tr></tbody></table><p>The improvements are substantial across all categories. Math and coding see the largest gains.</p><h3 id=real-world-tasks>Real-World Tasks<a hidden class=anchor aria-hidden=true href=#real-world-tasks>#</a></h3><p>Benchmarks don&rsquo;t tell the whole story. Zen 3 excels at:</p><p><strong>Long-form writing</strong>: Coherent documents spanning thousands of words with consistent style and structure.</p><p><strong>Multi-step reasoning</strong>: Complex problems requiring planning and backtracking.</p><p><strong>Code generation</strong>: Full functions and classes, not just snippets.</p><p><strong>Instruction following</strong>: Precise adherence to formatting and constraint requirements.</p><p><strong>Multilingual</strong>: Strong performance in 30+ languages including low-resource ones.</p><h3 id=agentic-capabilities>Agentic Capabilities<a hidden class=anchor aria-hidden=true href=#agentic-capabilities>#</a></h3><p>Zen 3 is designed for agent use cases:</p><ul><li><strong>Tool use</strong>: Reliable function calling with schema adherence</li><li><strong>Planning</strong>: Multi-step task decomposition</li><li><strong>Memory integration</strong>: Designed for RAG and experience ledgers</li><li><strong>Self-correction</strong>: Recognizes and recovers from errors</li></ul><p>Early agent benchmarks show 2x improvement over Zen 2 on multi-step tasks.</p><h2 id=training-details>Training Details<a hidden class=anchor aria-hidden=true href=#training-details>#</a></h2><h3 id=data>Data<a hidden class=anchor aria-hidden=true href=#data>#</a></h3><p>Training data evolved significantly:</p><ul><li><strong>Quality filtering</strong>: Improved classifiers for content quality</li><li><strong>Deduplication</strong>: Near-duplicate removal at document and paragraph level</li><li><strong>Synthetic data</strong>: 20% of training tokens from LLM-generated content</li><li><strong>Code emphasis</strong>: 15% code (up from 8% in Zen 2)</li><li><strong>Instruction mixing</strong>: 5% instruction data during pretraining</li></ul><p>Total: 15T tokens for the 8B model, proportionally less for larger models.</p><h3 id=training-process>Training Process<a hidden class=anchor aria-hidden=true href=#training-process>#</a></h3><p>Training used the Zoo Compute Network:</p><ul><li><strong>Duration</strong>: 4 months</li><li><strong>Peak nodes</strong>: 2,048 H100 GPUs</li><li><strong>Total compute</strong>: 3.2 million GPU-hours</li><li><strong>Efficiency</strong>: 47% MFU average</li></ul><p>The training run was the largest yet on the decentralized network. It validated that frontier training is possible without centralized infrastructure.</p><h3 id=alignment>Alignment<a hidden class=anchor aria-hidden=true href=#alignment>#</a></h3><p>Post-training alignment followed our standard process:</p><ol><li><strong>Supervised fine-tuning</strong>: 100K high-quality instruction examples</li><li><strong>GRPO</strong>: Group Relative Policy Optimization on preference data</li><li><strong>Constitutional training</strong>: Principle-based refinement</li><li><strong>Red teaming</strong>: Adversarial testing with remediation</li></ol><p>Alignment reduced benchmark scores slightly (2-3%) while significantly improving real-world usefulness.</p><h2 id=safety-evaluation>Safety Evaluation<a hidden class=anchor aria-hidden=true href=#safety-evaluation>#</a></h2><p>All Zen 3 models passed our safety evaluation suite:</p><h3 id=refusal-rates>Refusal Rates<a hidden class=anchor aria-hidden=true href=#refusal-rates>#</a></h3><table><thead><tr><th>Category</th><th>Zen-2-70B</th><th>Zen-3-72B</th></tr></thead><tbody><tr><td>Violence instructions</td><td>99.2%</td><td>99.7%</td></tr><tr><td>CSAM</td><td>100%</td><td>100%</td></tr><tr><td>Malware</td><td>97.8%</td><td>99.1%</td></tr><tr><td>PII extraction</td><td>94.6%</td><td>98.3%</td></tr></tbody></table><p>Improved refusal with fewer false positives on legitimate requests.</p><h3 id=bias-metrics>Bias Metrics<a hidden class=anchor aria-hidden=true href=#bias-metrics>#</a></h3><p>We evaluated on standard bias benchmarks:</p><ul><li>BBQ: 89.2% accuracy (vs. 84.1% for Zen 2)</li><li>WinoBias: 76.4% anti-stereotype (vs. 71.2%)</li><li>Toxicity: 0.023 average score (vs. 0.041)</li></ul><p>Improvements through both data curation and RLHF.</p><h3 id=limitations>Limitations<a hidden class=anchor aria-hidden=true href=#limitations>#</a></h3><p>Zen 3 is not perfect:</p><ul><li>Can still be jailbroken with sufficient effort</li><li>May hallucinate facts, especially for recent events</li><li>Long-context retrieval degrades past 100K tokens</li><li>Some languages underperform (especially non-Latin scripts)</li><li>Resource-intensive for edge deployment</li></ul><p>We publish these limitations because transparency enables responsible use.</p><h2 id=usage>Usage<a hidden class=anchor aria-hidden=true href=#usage>#</a></h2><h3 id=hugging-face>Hugging Face<a hidden class=anchor aria-hidden=true href=#hugging-face>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForCausalLM</span><span class=p>,</span> <span class=n>AutoTokenizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;zoo-labs/zen-3-72b&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;zoo-labs/zen-3-72b&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>tokenizer</span><span class=p>(</span><span class=s2>&#34;Hello, Zen!&#34;</span><span class=p>,</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>input_ids</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><h3 id=vllm>vLLM<a hidden class=anchor aria-hidden=true href=#vllm>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>vllm</span> <span class=kn>import</span> <span class=n>LLM</span><span class=p>,</span> <span class=n>SamplingParams</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>llm</span> <span class=o>=</span> <span class=n>LLM</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=s2>&#34;zoo-labs/zen-3-72b&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>outputs</span> <span class=o>=</span> <span class=n>llm</span><span class=o>.</span><span class=n>generate</span><span class=p>([</span><span class=s2>&#34;Hello, Zen!&#34;</span><span class=p>],</span> <span class=n>SamplingParams</span><span class=p>(</span><span class=n>max_tokens</span><span class=o>=</span><span class=mi>100</span><span class=p>))</span>
</span></span></code></pre></div><h3 id=quantized-versions>Quantized Versions<a hidden class=anchor aria-hidden=true href=#quantized-versions>#</a></h3><p>For resource-constrained deployment:</p><ul><li><strong>zen-3-72b-AWQ</strong>: 4-bit quantization, minimal quality loss</li><li><strong>zen-3-72b-GPTQ</strong>: Alternative 4-bit format</li><li><strong>zen-3-72b-GGUF</strong>: llama.cpp compatible</li></ul><p>The 8B model runs on consumer GPUs. The 72B quantized fits in 48GB.</p><h2 id=whats-next>What&rsquo;s Next<a hidden class=anchor aria-hidden=true href=#whats-next>#</a></h2><p>Zen 3 is a foundation. Coming soon:</p><ul><li><strong>Zen-3-Vision</strong>: Multimodal variant with image understanding</li><li><strong>Zen-3-Code</strong>: Specialized coding model</li><li><strong>Zen-3-Long</strong>: 1M+ context extension</li><li><strong>Zen-3-Agent</strong>: Optimized for agentic workflows</li></ul><p>The foundation is strong. Now we build.</p><h2 id=acknowledgments>Acknowledgments<a hidden class=anchor aria-hidden=true href=#acknowledgments>#</a></h2><p>Zen 3 was trained on the Zoo Compute Network with contributions from 847 node operators across 34 countries. Thank you.</p><p>This release was funded through the Zoo Labs Foundation treasury, allocated by community vote (ZIP-72). Thank you to all token holders who participated in governance.</p><p>Special thanks to the training, alignment, and evaluation teams who made this possible.</p><p>Download at huggingface.co/zoo-labs.</p><hr><p><em>Zach Kelling is a co-founder of Zoo Labs Foundation.</em></p></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://qwenlm.github.io/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>