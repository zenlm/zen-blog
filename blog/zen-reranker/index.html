<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Zen Reranker: Two-Stage Retrieval Done Right | Zen LM</title><meta name=keywords content="Research,Retrieval,Reranking"><meta name=description content="Introducing the Zen Reranker, a cross-encoder model that dramatically improves retrieval quality in two-stage pipelines."><meta name=author content="Zach Kelling"><link rel=canonical href=https://zenlm.org/blog/zen-reranker/><link crossorigin=anonymous href=/assets/css/stylesheet.6c0865a4bc8dbcbd27d78777eb33b404568f7fbf3185cca7b978e5275a4dd049.css integrity="sha256-bAhlpLyNvL0n14d36zO0BFaPf78xhcynuXjlJ1pN0Ek=" rel="preload stylesheet" as=style><link rel=icon href=https://zenlm.org/favicon.png><link rel=apple-touch-icon href=https://zenlm.org/favicon.png><link rel=manifest href=https://zenlm.org/site.webmanifest><meta name=theme-color content="#615CED"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script><meta property="og:title" content="Zen Reranker: Two-Stage Retrieval Done Right"><meta property="og:description" content="Introducing the Zen Reranker, a cross-encoder model that dramatically improves retrieval quality in two-stage pipelines."><meta property="og:type" content="article"><meta property="og:url" content="https://zenlm.org/blog/zen-reranker/"><meta property="og:image" content="https://zenlm.org/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2023-03-13T09:00:00-08:00"><meta property="article:modified_time" content="2023-03-13T09:00:00-08:00"><meta property="og:site_name" content="Zen LM"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://zenlm.org/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Zen Reranker: Two-Stage Retrieval Done Right"><meta name=twitter:description content="Introducing the Zen Reranker, a cross-encoder model that dramatically improves retrieval quality in two-stage pipelines."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://zenlm.org/blog/"},{"@type":"ListItem","position":2,"name":"Zen Reranker: Two-Stage Retrieval Done Right","item":"https://zenlm.org/blog/zen-reranker/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Zen Reranker: Two-Stage Retrieval Done Right","name":"Zen Reranker: Two-Stage Retrieval Done Right","description":"Introducing the Zen Reranker, a cross-encoder model that dramatically improves retrieval quality in two-stage pipelines.","keywords":["Research","Retrieval","Reranking"],"articleBody":"Embedding-based retrieval is fast but imprecise. Cross-encoder reranking is precise but slow. The combination unlocks the best of both. Today we release the Zen Reranker, purpose-built for two-stage retrieval.\nTwo-Stage Retrieval Modern retrieval pipelines typically operate in two stages:\nQuery -\u003e [Embedding Retrieval] -\u003e Top-K Candidates -\u003e [Reranker] -\u003e Final Results (fast, approximate) (slow, precise) Stage 1: Bi-encoder embeddings enable fast approximate search over millions of documents. Retrieve top-100 to top-1000 candidates.\nStage 2: Cross-encoder reranker scores each candidate against the query with full attention. Reorder to get final top-10 or top-20.\nThe reranker sees query-document pairs together, enabling much finer relevance distinctions than independent embeddings.\nThe Zen Reranker Our reranker builds on several key design decisions:\nArchitecture Base: 330M parameter encoder-only transformer Input: Concatenated query and document with separator tokens Output: Single relevance score (0-1) Context: 512 tokens (query + document combined) Training Data We curated training data from multiple sources:\nMS MARCO: Search query-passage pairs (positive and hard negatives) Natural Questions: Question-answer pairs from Wikipedia Synthetic pairs: LLM-generated query-document pairs with labels Human judgments: 50K expert-annotated pairs across domains Total: 12M training pairs with 4-way relevance labels.\nTraining Objective We use a listwise loss that considers the full ranking:\ndef listwise_loss(scores, labels): # Softmax over candidate scores probs = softmax(scores) # Cross-entropy with label distribution return -sum(labels * log(probs)) This outperforms pointwise binary classification by teaching the model to rank, not just classify.\nBenchmark Results BEIR Reranking Reranking BM25 top-100 results:\nReranker NDCG@10 Time (ms/query) No reranking 42.1 - monoT5-base 49.3 180 MiniLM-reranker 47.8 45 Zen Reranker 52.7 62 Reranking Zen Embeddings Combined with our 7680d embeddings:\nPipeline NDCG@10 Latency Zen-Embed only 57.3 8ms Zen-Embed + Reranker 64.1 70ms The two-stage pipeline achieves +12% improvement with acceptable latency overhead.\nDomain-Specific Performance Domain BM25 +Zen Reranker Improvement Scientific (SCIDOCS) 15.8 21.4 +35% Finance (FiQA) 29.6 38.2 +29% Covid (TREC-COVID) 65.5 78.3 +20% Quora duplicate 78.9 84.6 +7% Specialized domains with complex language benefit most.\nUsage Basic Usage from zen.reranker import ZenReranker reranker = ZenReranker.from_pretrained(\"zoo-labs/zen-reranker\") query = \"What causes climate change?\" documents = [ \"Greenhouse gases trap heat in the atmosphere...\", \"The weather today is sunny and warm...\", \"CO2 emissions from burning fossil fuels...\" ] scores = reranker.score(query, documents) # [0.92, 0.12, 0.87] Integration with Retrievers from zen.retrieval import ZenRetriever from zen.reranker import ZenReranker retriever = ZenRetriever(\"zoo-labs/zen-embed-xl\") reranker = ZenReranker.from_pretrained(\"zoo-labs/zen-reranker\") # Stage 1: Fast retrieval candidates = retriever.retrieve(query, k=100) # Stage 2: Precise reranking reranked = reranker.rerank(query, candidates, k=10) Batched Inference For production workloads:\n# Batch queries for efficiency queries = [\"query 1\", \"query 2\", ...] candidate_lists = [[docs...], [docs...], ...] results = reranker.batch_rerank( queries, candidate_lists, batch_size=32, k=10 ) Deployment Considerations Hardware Requirements Minimum: 4GB GPU memory Recommended: 8GB+ for batched inference CPU: Viable for low-throughput (\u003c10 QPS) Latency Optimization Batching: Process multiple query-doc pairs together Quantization: INT8 reduces latency 40% with \u003c1% quality loss Early termination: Stop scoring when top-k is confident Caching: Cache scores for repeated query-document pairs Scaling For high-throughput applications:\nDeploy multiple replicas behind load balancer Use async inference with request queuing Consider distilled smaller models for extreme latency requirements Model Release The Zen Reranker is available under Apache 2.0:\nHugging Face: huggingface.co/zoo-labs/zen-reranker ONNX: Optimized for deployment TensorRT: NVIDIA-optimized variant Conclusion Two-stage retrieval with a quality reranker is the pragmatic choice for production search systems. The Zen Reranker provides state-of-the-art reranking in an efficient, easy-to-deploy package.\nFast first, then precise.\nZach Kelling is a co-founder of Zoo Labs Foundation.\n","wordCount":"573","inLanguage":"en","datePublished":"2023-03-13T09:00:00-08:00","dateModified":"2023-03-13T09:00:00-08:00","author":{"@type":"Person","name":"Zach Kelling"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://zenlm.org/blog/zen-reranker/"},"publisher":{"@type":"Organization","name":"Zen LM","logo":{"@type":"ImageObject","url":"https://zenlm.org/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Zen LM (Alt + H)"><img src=https://zenlm.org/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://zeekay.blog title=zeekay><span>zeekay</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://hanzo.blog title=hanzo.blog><span>hanzo.blog</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://hanzo.ai/chat title="Try Zen Chat"><span>Try Zen Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://blog.zoo.ngo title="zoo blog"><span>zoo blog</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Zen Reranker: Two-Stage Retrieval Done Right</h1><div class=post-description>Introducing the Zen Reranker, a cross-encoder model that dramatically improves retrieval quality in two-stage pipelines.</div><div class=post-meta><span title='2023-03-13 09:00:00 -0800 -0800'>March 13, 2023</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;573 words&nbsp;·&nbsp;Zach Kelling</div></div></div><main class=main><article class=post-single><div class=post-content><p>Embedding-based retrieval is fast but imprecise. Cross-encoder reranking is precise but slow. The combination unlocks the best of both. Today we release the Zen Reranker, purpose-built for two-stage retrieval.</p><h2 id=two-stage-retrieval>Two-Stage Retrieval<a hidden class=anchor aria-hidden=true href=#two-stage-retrieval>#</a></h2><p>Modern retrieval pipelines typically operate in two stages:</p><pre tabindex=0><code>Query -&gt; [Embedding Retrieval] -&gt; Top-K Candidates -&gt; [Reranker] -&gt; Final Results
         (fast, approximate)                          (slow, precise)
</code></pre><p><strong>Stage 1</strong>: Bi-encoder embeddings enable fast approximate search over millions of documents. Retrieve top-100 to top-1000 candidates.</p><p><strong>Stage 2</strong>: Cross-encoder reranker scores each candidate against the query with full attention. Reorder to get final top-10 or top-20.</p><p>The reranker sees query-document pairs together, enabling much finer relevance distinctions than independent embeddings.</p><h2 id=the-zen-reranker>The Zen Reranker<a hidden class=anchor aria-hidden=true href=#the-zen-reranker>#</a></h2><p>Our reranker builds on several key design decisions:</p><h3 id=architecture>Architecture<a hidden class=anchor aria-hidden=true href=#architecture>#</a></h3><ul><li><strong>Base</strong>: 330M parameter encoder-only transformer</li><li><strong>Input</strong>: Concatenated query and document with separator tokens</li><li><strong>Output</strong>: Single relevance score (0-1)</li><li><strong>Context</strong>: 512 tokens (query + document combined)</li></ul><h3 id=training-data>Training Data<a hidden class=anchor aria-hidden=true href=#training-data>#</a></h3><p>We curated training data from multiple sources:</p><ol><li><strong>MS MARCO</strong>: Search query-passage pairs (positive and hard negatives)</li><li><strong>Natural Questions</strong>: Question-answer pairs from Wikipedia</li><li><strong>Synthetic pairs</strong>: LLM-generated query-document pairs with labels</li><li><strong>Human judgments</strong>: 50K expert-annotated pairs across domains</li></ol><p>Total: 12M training pairs with 4-way relevance labels.</p><h3 id=training-objective>Training Objective<a hidden class=anchor aria-hidden=true href=#training-objective>#</a></h3><p>We use a listwise loss that considers the full ranking:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>listwise_loss</span><span class=p>(</span><span class=n>scores</span><span class=p>,</span> <span class=n>labels</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># Softmax over candidate scores</span>
</span></span><span class=line><span class=cl>    <span class=n>probs</span> <span class=o>=</span> <span class=n>softmax</span><span class=p>(</span><span class=n>scores</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># Cross-entropy with label distribution</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=o>-</span><span class=nb>sum</span><span class=p>(</span><span class=n>labels</span> <span class=o>*</span> <span class=n>log</span><span class=p>(</span><span class=n>probs</span><span class=p>))</span>
</span></span></code></pre></div><p>This outperforms pointwise binary classification by teaching the model to rank, not just classify.</p><h2 id=benchmark-results>Benchmark Results<a hidden class=anchor aria-hidden=true href=#benchmark-results>#</a></h2><h3 id=beir-reranking>BEIR Reranking<a hidden class=anchor aria-hidden=true href=#beir-reranking>#</a></h3><p>Reranking BM25 top-100 results:</p><table><thead><tr><th>Reranker</th><th>NDCG@10</th><th>Time (ms/query)</th></tr></thead><tbody><tr><td>No reranking</td><td>42.1</td><td>-</td></tr><tr><td>monoT5-base</td><td>49.3</td><td>180</td></tr><tr><td>MiniLM-reranker</td><td>47.8</td><td>45</td></tr><tr><td><strong>Zen Reranker</strong></td><td>52.7</td><td>62</td></tr></tbody></table><h3 id=reranking-zen-embeddings>Reranking Zen Embeddings<a hidden class=anchor aria-hidden=true href=#reranking-zen-embeddings>#</a></h3><p>Combined with our 7680d embeddings:</p><table><thead><tr><th>Pipeline</th><th>NDCG@10</th><th>Latency</th></tr></thead><tbody><tr><td>Zen-Embed only</td><td>57.3</td><td>8ms</td></tr><tr><td>Zen-Embed + Reranker</td><td>64.1</td><td>70ms</td></tr></tbody></table><p>The two-stage pipeline achieves +12% improvement with acceptable latency overhead.</p><h3 id=domain-specific-performance>Domain-Specific Performance<a hidden class=anchor aria-hidden=true href=#domain-specific-performance>#</a></h3><table><thead><tr><th>Domain</th><th>BM25</th><th>+Zen Reranker</th><th>Improvement</th></tr></thead><tbody><tr><td>Scientific (SCIDOCS)</td><td>15.8</td><td>21.4</td><td>+35%</td></tr><tr><td>Finance (FiQA)</td><td>29.6</td><td>38.2</td><td>+29%</td></tr><tr><td>Covid (TREC-COVID)</td><td>65.5</td><td>78.3</td><td>+20%</td></tr><tr><td>Quora duplicate</td><td>78.9</td><td>84.6</td><td>+7%</td></tr></tbody></table><p>Specialized domains with complex language benefit most.</p><h2 id=usage>Usage<a hidden class=anchor aria-hidden=true href=#usage>#</a></h2><h3 id=basic-usage>Basic Usage<a hidden class=anchor aria-hidden=true href=#basic-usage>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>zen.reranker</span> <span class=kn>import</span> <span class=n>ZenReranker</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>reranker</span> <span class=o>=</span> <span class=n>ZenReranker</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;zoo-labs/zen-reranker&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>query</span> <span class=o>=</span> <span class=s2>&#34;What causes climate change?&#34;</span>
</span></span><span class=line><span class=cl><span class=n>documents</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Greenhouse gases trap heat in the atmosphere...&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;The weather today is sunny and warm...&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;CO2 emissions from burning fossil fuels...&#34;</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>scores</span> <span class=o>=</span> <span class=n>reranker</span><span class=o>.</span><span class=n>score</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>documents</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># [0.92, 0.12, 0.87]</span>
</span></span></code></pre></div><h3 id=integration-with-retrievers>Integration with Retrievers<a hidden class=anchor aria-hidden=true href=#integration-with-retrievers>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>zen.retrieval</span> <span class=kn>import</span> <span class=n>ZenRetriever</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>zen.reranker</span> <span class=kn>import</span> <span class=n>ZenReranker</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>retriever</span> <span class=o>=</span> <span class=n>ZenRetriever</span><span class=p>(</span><span class=s2>&#34;zoo-labs/zen-embed-xl&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>reranker</span> <span class=o>=</span> <span class=n>ZenReranker</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;zoo-labs/zen-reranker&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Stage 1: Fast retrieval</span>
</span></span><span class=line><span class=cl><span class=n>candidates</span> <span class=o>=</span> <span class=n>retriever</span><span class=o>.</span><span class=n>retrieve</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>k</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Stage 2: Precise reranking</span>
</span></span><span class=line><span class=cl><span class=n>reranked</span> <span class=o>=</span> <span class=n>reranker</span><span class=o>.</span><span class=n>rerank</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>candidates</span><span class=p>,</span> <span class=n>k</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=batched-inference>Batched Inference<a hidden class=anchor aria-hidden=true href=#batched-inference>#</a></h3><p>For production workloads:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Batch queries for efficiency</span>
</span></span><span class=line><span class=cl><span class=n>queries</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34;query 1&#34;</span><span class=p>,</span> <span class=s2>&#34;query 2&#34;</span><span class=p>,</span> <span class=o>...</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>candidate_lists</span> <span class=o>=</span> <span class=p>[[</span><span class=n>docs</span><span class=o>...</span><span class=p>],</span> <span class=p>[</span><span class=n>docs</span><span class=o>...</span><span class=p>],</span> <span class=o>...</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>results</span> <span class=o>=</span> <span class=n>reranker</span><span class=o>.</span><span class=n>batch_rerank</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>queries</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>    <span class=n>candidate_lists</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>batch_size</span><span class=o>=</span><span class=mi>32</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>k</span><span class=o>=</span><span class=mi>10</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><h2 id=deployment-considerations>Deployment Considerations<a hidden class=anchor aria-hidden=true href=#deployment-considerations>#</a></h2><h3 id=hardware-requirements>Hardware Requirements<a hidden class=anchor aria-hidden=true href=#hardware-requirements>#</a></h3><ul><li><strong>Minimum</strong>: 4GB GPU memory</li><li><strong>Recommended</strong>: 8GB+ for batched inference</li><li><strong>CPU</strong>: Viable for low-throughput (&lt;10 QPS)</li></ul><h3 id=latency-optimization>Latency Optimization<a hidden class=anchor aria-hidden=true href=#latency-optimization>#</a></h3><ol><li><strong>Batching</strong>: Process multiple query-doc pairs together</li><li><strong>Quantization</strong>: INT8 reduces latency 40% with &lt;1% quality loss</li><li><strong>Early termination</strong>: Stop scoring when top-k is confident</li><li><strong>Caching</strong>: Cache scores for repeated query-document pairs</li></ol><h3 id=scaling>Scaling<a hidden class=anchor aria-hidden=true href=#scaling>#</a></h3><p>For high-throughput applications:</p><ul><li>Deploy multiple replicas behind load balancer</li><li>Use async inference with request queuing</li><li>Consider distilled smaller models for extreme latency requirements</li></ul><h2 id=model-release>Model Release<a hidden class=anchor aria-hidden=true href=#model-release>#</a></h2><p>The Zen Reranker is available under Apache 2.0:</p><ul><li><strong>Hugging Face</strong>: huggingface.co/zoo-labs/zen-reranker</li><li><strong>ONNX</strong>: Optimized for deployment</li><li><strong>TensorRT</strong>: NVIDIA-optimized variant</li></ul><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Two-stage retrieval with a quality reranker is the pragmatic choice for production search systems. The Zen Reranker provides state-of-the-art reranking in an efficient, easy-to-deploy package.</p><p>Fast first, then precise.</p><hr><p><em>Zach Kelling is a co-founder of Zoo Labs Foundation.</em></p></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://zenlm.org/>Zen LM</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>