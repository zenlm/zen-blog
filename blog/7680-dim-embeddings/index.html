<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta http-equiv=refresh content="5; url=https://qwen.ai/blog?id=7680-dim-embeddings"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>7680-Dimensional Embeddings: More Dimensions, Better Retrieval | Qwen</title><meta name=keywords content="Research,Embeddings,Retrieval"><meta name=description content="Why we trained embedding models with 7680 dimensions and what we learned about the relationship between dimensionality and retrieval quality."><meta name=author content="Zach Kelling"><link rel=canonical href=https://qwenlm.github.io/blog/7680-dim-embeddings/><link crossorigin=anonymous href=/assets/css/stylesheet.6c0865a4bc8dbcbd27d78777eb33b404568f7fbf3185cca7b978e5275a4dd049.css integrity="sha256-bAhlpLyNvL0n14d36zO0BFaPf78xhcynuXjlJ1pN0Ek=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/7680-dim-embeddings/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:title" content="7680-Dimensional Embeddings: More Dimensions, Better Retrieval"><meta property="og:description" content="Why we trained embedding models with 7680 dimensions and what we learned about the relationship between dimensionality and retrieval quality."><meta property="og:type" content="article"><meta property="og:url" content="https://qwenlm.github.io/blog/7680-dim-embeddings/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2022-12-05T10:00:00-08:00"><meta property="article:modified_time" content="2022-12-05T10:00:00-08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="7680-Dimensional Embeddings: More Dimensions, Better Retrieval"><meta name=twitter:description content="Why we trained embedding models with 7680 dimensions and what we learned about the relationship between dimensionality and retrieval quality."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/blog/"},{"@type":"ListItem","position":2,"name":"7680-Dimensional Embeddings: More Dimensions, Better Retrieval","item":"https://qwenlm.github.io/blog/7680-dim-embeddings/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"7680-Dimensional Embeddings: More Dimensions, Better Retrieval","name":"7680-Dimensional Embeddings: More Dimensions, Better Retrieval","description":"Why we trained embedding models with 7680 dimensions and what we learned about the relationship between dimensionality and retrieval quality.","keywords":["Research","Embeddings","Retrieval"],"articleBody":"Embedding dimensions have standardized around powers of two: 768, 1536, occasionally 4096. We asked a simple question: what happens if we go bigger? The answer surprised us.\nBackground: Why Dimensions Matter Text embeddings map variable-length sequences to fixed-dimensional vectors. These vectors enable semantic similarity search, clustering, and retrieval. The dimension count determines the vector space’s capacity.\nLower dimensions mean:\nSmaller storage requirements Faster similarity computations Potential information loss Higher dimensions mean:\nMore expressive capacity Larger memory footprint Computational overhead The conventional wisdom holds that returns diminish quickly past 1024-2048 dimensions. Our experiments challenge this.\nExperimental Setup We trained a series of embedding models with identical architectures except for output dimension:\nModel Dimensions Parameters Zen-Embed-S 768 110M Zen-Embed-M 1536 125M Zen-Embed-L 3072 155M Zen-Embed-XL 7680 230M Training data: 1.2B text pairs with contrastive learning objective.\nResults Retrieval Benchmarks BEIR (Benchmarking IR) results across 15 datasets:\nModel NDCG@10 Recall@100 MRR Zen-Embed-S 48.2 71.3 45.1 Zen-Embed-M 51.7 75.8 48.9 Zen-Embed-L 54.1 79.2 52.3 Zen-Embed-XL 57.3 83.6 55.8 The improvements continue well past conventional dimension counts.\nScaling Analysis Plotting performance against log(dimensions) reveals near-linear scaling:\n$$\\text{NDCG@10} \\approx 0.12 \\cdot \\log_2(d) + 37.4$$\nThis suggests embedding capacity remains a bottleneck even at high dimensions.\nPer-Domain Breakdown The benefits are not uniform across domains:\nDomain 768d 7680d Improvement Scientific 42.1 54.7 +30% Legal 38.9 51.2 +32% Conversational 52.3 55.1 +5% News 49.8 53.4 +7% Technical and specialized domains benefit most. Everyday conversational content sees smaller gains.\nInterpretability Higher dimensions don’t just improve metrics; they enable finer distinctions. Analysis of the 7680d space shows:\nCleaner clusters: Topic boundaries are sharper Preserved nuance: Similar but distinct concepts remain separable Hierarchical structure: Taxonomic relationships emerge naturally The Efficiency Question 7680 dimensions cost more to store and search. Is it worth it?\nStorage Dimensions Bytes per Vector 1M Vectors 768 3,072 2.9 GB 7680 30,720 29.3 GB 10x storage for higher dimensions. Significant but manageable with modern hardware.\nSearch Latency Exact search scales linearly with dimensions. But approximate methods (HNSW, IVF) show sublinear scaling:\nDimensions Exact (ms) HNSW (ms) IVF-PQ (ms) 768 12.3 0.8 0.3 7680 118.7 2.1 0.7 With appropriate indexing, 7680d search remains practical.\nCompression Quantization recovers much of the efficiency loss:\nINT8: 4x compression, \u003c1% quality loss Binary: 32x compression, 5% quality loss Product Quantization: 16x compression, 2% quality loss Practical Recommendations Based on our experiments:\nIf retrieval quality matters most: Use 7680d with HNSW indexing If storage is constrained: Use 7680d with INT8 quantization (still beats 768d float32) For conversational applications: 1536d is sufficient For technical/specialized domains: Higher dimensions provide outsized benefits Release We’re releasing the Zen-Embed family:\nZen-Embed-S (768d): Free, MIT license Zen-Embed-M (1536d): Free, MIT license Zen-Embed-L (3072d): Free, MIT license Zen-Embed-XL (7680d): Free, MIT license All models available on Hugging Face: huggingface.co/zoo-labs\nWhat This Means The embedding dimension race isn’t over. There’s room to improve retrieval quality by increasing capacity. As hardware improves and indexing methods advance, higher-dimensional embeddings become increasingly practical.\nMore dimensions, better retrieval. Sometimes the simple approach works.\nZach Kelling is a co-founder of Zoo Labs Foundation.\n","wordCount":"507","inLanguage":"en","datePublished":"2022-12-05T10:00:00-08:00","dateModified":"2022-12-05T10:00:00-08:00","author":{"@type":"Person","name":"Zach Kelling"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qwenlm.github.io/blog/7680-dim-embeddings/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://qwenlm.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><style>.modal-overlay{position:fixed;top:0;left:0;width:100%;height:100%;background-color:rgba(0,0,0,.5);display:flex;align-items:center;z-index:1000;animation:fadeIn .3s ease-in-out}.modal-container{margin-left:auto;margin-right:auto;background-color:var(--theme);border-radius:8px;box-shadow:0 4px 20px rgba(0,0,0,.15);width:90%;max-width:420px;height:fit-content;padding:30px;text-align:center;position:relative;animation:slideIn .4s ease-out}.modal-container a{color:var(--hero2)}.modal-icon{width:70px;height:70px;background-color:#f0f7ff;border-radius:50%;display:flex;align-items:center;justify-content:center;margin:0 auto 20px;color:#1a73e8;font-size:30px}.modal-title{font-size:1.5rem;font-weight:600;color:var(--primary);margin:0 0 15px}.modal-message{font-size:1rem;color:var(--secondary);line-height:1.5;margin:0 0 25px}.countdown{font-size:1.2rem;color:#666;margin:20px 0;font-weight:500}.modal-buttons{display:flex;justify-content:center;gap:15px;margin-top:25px}.modal-buttons .btn{padding:6px 16px;border-radius:8px;font-size:1.2rem;font-weight:500;cursor:pointer;transition:all .3s ease;border:none}.btn-primary{background-color:#1a73e8;color:#fff}.btn-primary:hover{background-color:#1557b0}.btn-secondary{background-color:#f1f3f4;color:#333}.btn-secondary:hover{background-color:#e0e0e0}@keyframes fadeIn{from{opacity:0}to{opacity:1}}@keyframes slideIn{from{opacity:0;transform:translateY(-50px)}to{opacity:1;transform:translateY(0)}}@media(max-width:480px){.modal-container{max-width:95%;width:calc(95vw - 40px);padding:20px}}</style><div class=modal-overlay><div class=modal-container><div class=modal-icon><svg xmlns="http://www.w3.org/2000/svg" width="36" height="36" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71"/></svg></div><h2 class=modal-title>We have a new blog!<br>View this page at <a href="https://qwen.ai/blog?id=7680-dim-embeddings">qwen.ai</a>.</h2><p class=modal-message>This page will automatically redirect in <span class=countdown id=countdown>5</span> seconds.</p><p class=modal-message>If you are not redirected automatically, please click the button below.</p><div class=modal-buttons><button class="btn btn-primary" onclick=redirectToPage()>Go Now</button></div></div></div><script>let countdown=5;const countdownElement=document.getElementById("countdown"),timer=setInterval(()=>{countdown--,countdownElement.textContent=countdown,countdown<=0&&clearInterval(timer)},1e3);function stayHere(){document.querySelector(".modal-overlay").style.display="none"}function redirectToPage(){window.location.href="https://qwen.ai/blog?id=7680-dim-embeddings"}</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://chat.qwen.ai title="Try Qwen Chat"><span>Try Qwen Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>7680-Dimensional Embeddings: More Dimensions, Better Retrieval</h1><div class=post-description>Why we trained embedding models with 7680 dimensions and what we learned about the relationship between dimensionality and retrieval quality.</div><div class=post-meta><span title='2022-12-05 10:00:00 -0800 -0800'>December 5, 2022</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;507 words&nbsp;·&nbsp;Zach Kelling</div></div></div><main class=main><article class=post-single><div class=post-content><p>Embedding dimensions have standardized around powers of two: 768, 1536, occasionally 4096. We asked a simple question: what happens if we go bigger? The answer surprised us.</p><h2 id=background-why-dimensions-matter>Background: Why Dimensions Matter<a hidden class=anchor aria-hidden=true href=#background-why-dimensions-matter>#</a></h2><p>Text embeddings map variable-length sequences to fixed-dimensional vectors. These vectors enable semantic similarity search, clustering, and retrieval. The dimension count determines the vector space&rsquo;s capacity.</p><p>Lower dimensions mean:</p><ul><li>Smaller storage requirements</li><li>Faster similarity computations</li><li>Potential information loss</li></ul><p>Higher dimensions mean:</p><ul><li>More expressive capacity</li><li>Larger memory footprint</li><li>Computational overhead</li></ul><p>The conventional wisdom holds that returns diminish quickly past 1024-2048 dimensions. Our experiments challenge this.</p><h2 id=experimental-setup>Experimental Setup<a hidden class=anchor aria-hidden=true href=#experimental-setup>#</a></h2><p>We trained a series of embedding models with identical architectures except for output dimension:</p><table><thead><tr><th>Model</th><th>Dimensions</th><th>Parameters</th></tr></thead><tbody><tr><td>Zen-Embed-S</td><td>768</td><td>110M</td></tr><tr><td>Zen-Embed-M</td><td>1536</td><td>125M</td></tr><tr><td>Zen-Embed-L</td><td>3072</td><td>155M</td></tr><tr><td>Zen-Embed-XL</td><td>7680</td><td>230M</td></tr></tbody></table><p>Training data: 1.2B text pairs with contrastive learning objective.</p><h2 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h2><h3 id=retrieval-benchmarks>Retrieval Benchmarks<a hidden class=anchor aria-hidden=true href=#retrieval-benchmarks>#</a></h3><p>BEIR (Benchmarking IR) results across 15 datasets:</p><table><thead><tr><th>Model</th><th>NDCG@10</th><th>Recall@100</th><th>MRR</th></tr></thead><tbody><tr><td>Zen-Embed-S</td><td>48.2</td><td>71.3</td><td>45.1</td></tr><tr><td>Zen-Embed-M</td><td>51.7</td><td>75.8</td><td>48.9</td></tr><tr><td>Zen-Embed-L</td><td>54.1</td><td>79.2</td><td>52.3</td></tr><tr><td>Zen-Embed-XL</td><td>57.3</td><td>83.6</td><td>55.8</td></tr></tbody></table><p>The improvements continue well past conventional dimension counts.</p><h3 id=scaling-analysis>Scaling Analysis<a hidden class=anchor aria-hidden=true href=#scaling-analysis>#</a></h3><p>Plotting performance against log(dimensions) reveals near-linear scaling:</p><p>$$\text{NDCG@10} \approx 0.12 \cdot \log_2(d) + 37.4$$</p><p>This suggests embedding capacity remains a bottleneck even at high dimensions.</p><h3 id=per-domain-breakdown>Per-Domain Breakdown<a hidden class=anchor aria-hidden=true href=#per-domain-breakdown>#</a></h3><p>The benefits are not uniform across domains:</p><table><thead><tr><th>Domain</th><th>768d</th><th>7680d</th><th>Improvement</th></tr></thead><tbody><tr><td>Scientific</td><td>42.1</td><td>54.7</td><td>+30%</td></tr><tr><td>Legal</td><td>38.9</td><td>51.2</td><td>+32%</td></tr><tr><td>Conversational</td><td>52.3</td><td>55.1</td><td>+5%</td></tr><tr><td>News</td><td>49.8</td><td>53.4</td><td>+7%</td></tr></tbody></table><p>Technical and specialized domains benefit most. Everyday conversational content sees smaller gains.</p><h3 id=interpretability>Interpretability<a hidden class=anchor aria-hidden=true href=#interpretability>#</a></h3><p>Higher dimensions don&rsquo;t just improve metrics; they enable finer distinctions. Analysis of the 7680d space shows:</p><ul><li><strong>Cleaner clusters</strong>: Topic boundaries are sharper</li><li><strong>Preserved nuance</strong>: Similar but distinct concepts remain separable</li><li><strong>Hierarchical structure</strong>: Taxonomic relationships emerge naturally</li></ul><h2 id=the-efficiency-question>The Efficiency Question<a hidden class=anchor aria-hidden=true href=#the-efficiency-question>#</a></h2><p>7680 dimensions cost more to store and search. Is it worth it?</p><h3 id=storage>Storage<a hidden class=anchor aria-hidden=true href=#storage>#</a></h3><table><thead><tr><th>Dimensions</th><th>Bytes per Vector</th><th>1M Vectors</th></tr></thead><tbody><tr><td>768</td><td>3,072</td><td>2.9 GB</td></tr><tr><td>7680</td><td>30,720</td><td>29.3 GB</td></tr></tbody></table><p>10x storage for higher dimensions. Significant but manageable with modern hardware.</p><h3 id=search-latency>Search Latency<a hidden class=anchor aria-hidden=true href=#search-latency>#</a></h3><p>Exact search scales linearly with dimensions. But approximate methods (HNSW, IVF) show sublinear scaling:</p><table><thead><tr><th>Dimensions</th><th>Exact (ms)</th><th>HNSW (ms)</th><th>IVF-PQ (ms)</th></tr></thead><tbody><tr><td>768</td><td>12.3</td><td>0.8</td><td>0.3</td></tr><tr><td>7680</td><td>118.7</td><td>2.1</td><td>0.7</td></tr></tbody></table><p>With appropriate indexing, 7680d search remains practical.</p><h3 id=compression>Compression<a hidden class=anchor aria-hidden=true href=#compression>#</a></h3><p>Quantization recovers much of the efficiency loss:</p><ul><li><strong>INT8</strong>: 4x compression, &lt;1% quality loss</li><li><strong>Binary</strong>: 32x compression, 5% quality loss</li><li><strong>Product Quantization</strong>: 16x compression, 2% quality loss</li></ul><h2 id=practical-recommendations>Practical Recommendations<a hidden class=anchor aria-hidden=true href=#practical-recommendations>#</a></h2><p>Based on our experiments:</p><ol><li><strong>If retrieval quality matters most</strong>: Use 7680d with HNSW indexing</li><li><strong>If storage is constrained</strong>: Use 7680d with INT8 quantization (still beats 768d float32)</li><li><strong>For conversational applications</strong>: 1536d is sufficient</li><li><strong>For technical/specialized domains</strong>: Higher dimensions provide outsized benefits</li></ol><h2 id=release>Release<a hidden class=anchor aria-hidden=true href=#release>#</a></h2><p>We&rsquo;re releasing the Zen-Embed family:</p><ul><li><strong>Zen-Embed-S</strong> (768d): Free, MIT license</li><li><strong>Zen-Embed-M</strong> (1536d): Free, MIT license</li><li><strong>Zen-Embed-L</strong> (3072d): Free, MIT license</li><li><strong>Zen-Embed-XL</strong> (7680d): Free, MIT license</li></ul><p>All models available on Hugging Face: huggingface.co/zoo-labs</p><h2 id=what-this-means>What This Means<a hidden class=anchor aria-hidden=true href=#what-this-means>#</a></h2><p>The embedding dimension race isn&rsquo;t over. There&rsquo;s room to improve retrieval quality by increasing capacity. As hardware improves and indexing methods advance, higher-dimensional embeddings become increasingly practical.</p><p>More dimensions, better retrieval. Sometimes the simple approach works.</p><hr><p><em>Zach Kelling is a co-founder of Zoo Labs Foundation.</em></p></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://qwenlm.github.io/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>