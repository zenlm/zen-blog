<!DOCTYPE html><!--qMVpAZcUAPsCZEMM19Q64--><html lang="en" class="dark"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/chunks/f2332aac77592f9d.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/e83606e8fa9cc796.js"/><script src="/_next/static/chunks/36d6595f0156cd7e.js" async=""></script><script src="/_next/static/chunks/040e9cea20a8d9c7.js" async=""></script><script src="/_next/static/chunks/c19fcbf6bf086438.js" async=""></script><script src="/_next/static/chunks/turbopack-7419f7f4f6b062de.js" async=""></script><script src="/_next/static/chunks/59d0ad1b64f8544e.js" async=""></script><script src="/_next/static/chunks/4d80e004cf4896dd.js" async=""></script><script src="/_next/static/chunks/350ee4303b732916.js" async=""></script><script src="/_next/static/chunks/36bfed0236ce2cf2.js" async=""></script><script src="/_next/static/chunks/e62b91212ee7f8ff.js" async=""></script><script src="/_next/static/chunks/2a98816c7d26bf58.js" async=""></script><script src="/_next/static/chunks/cb0a883bafeb6805.js" async=""></script><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#0A0A0A"/><meta name="theme-color" media="(prefers-color-scheme: light)" content="#fff"/><title>Introducing Qwen-VL — Zen LM Blog | Zen LM</title><meta name="description" content="Along with the rapid development of our large language model Qwen, we leveraged Qwen’s capabilities and unified multimodal pretraining to address the limitations of multimodal models in generalization, and we opensourced multimodal model Qwen-VL in Sep. 2023. Recently, the Qwen-VL series has undergone a significant upgrade with the launch of two enhanced versions, Qwen-VL-Plus and Qwen-VL-Max. The key technical advancements in these versions include:"/><meta property="og:title" content="Zen LM - Open Foundation Models"/><meta property="og:description" content="Zen AI model family by Hanzo AI. 14 frontier models from 4B to 1T+ parameters for code, reasoning, vision, and multimodal tasks."/><meta property="og:url" content="https://zenlm.org"/><meta property="og:site_name" content="Zen LM"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="@zenlmorg"/><meta name="twitter:creator" content="@zenlmorg"/><meta name="twitter:title" content="Zen LM - Open Foundation Models"/><meta name="twitter:description" content="Zen AI model family by Hanzo AI. 14 frontier models from 4B to 1T+ parameters for code, reasoning, vision, and multimodal tasks."/><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="antialiased"><div hidden=""><!--$--><!--/$--></div><script>((a,b,c,d,e,f,g,h)=>{let i=document.documentElement,j=["light","dark"];function k(b){var c;(Array.isArray(a)?a:[a]).forEach(a=>{let c="class"===a,d=c&&f?e.map(a=>f[a]||a):e;c?(i.classList.remove(...d),i.classList.add(f&&f[b]?f[b]:b)):i.setAttribute(a,b)}),c=b,h&&j.includes(c)&&(i.style.colorScheme=c)}if(d)k(d);else try{let a=localStorage.getItem(b)||c,d=g&&"system"===a?window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":a;k(d)}catch(a){}})("class","theme","system",null,["light","dark"],null,true,true)</script><!--$--><div data-closed="" role="presentation" hidden="" style="user-select:none;-webkit-user-select:none" class="fixed inset-0 z-50 backdrop-blur-xs bg-fd-overlay data-open:animate-fd-fade-in data-closed:animate-fd-fade-out"></div><div class="bg-fd-secondary/50 p-3 empty:hidden"></div><!--/$--><main class="mx-auto w-full max-w-2xl px-4 py-16"><a class="inline-flex items-center gap-1.5 text-sm text-fd-muted-foreground hover:text-fd-foreground mb-10 transition-colors" href="/blog">← Back to Blog</a><div class="mb-8"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">January 24, 2024</time><h1 class="text-3xl font-bold mt-2 mb-3">Introducing Qwen-VL</h1><p class="text-fd-muted-foreground text-lg mb-4">Along with the rapid development of our large language model Qwen, we leveraged Qwen’s capabilities and unified multimodal pretraining to address the limitations of multimodal models in generalization, and we opensourced multimodal model Qwen-VL in Sep. 2023. Recently, the Qwen-VL series has undergone a significant upgrade with the launch of two enhanced versions, Qwen-VL-Plus and Qwen-VL-Max. The key technical advancements in these versions include:</p><div class="flex items-center gap-3 pt-4 border-t border-fd-border"><span class="text-sm text-fd-muted-foreground">By <!-- -->Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></div><div class="prose dark:prose-invert max-w-none"><p>Along with the rapid development of our large language model Qwen, we leveraged Qwen’s capabilities and unified multimodal pretraining to address the limitations of multimodal models in generalization, and we opensourced multimodal model Qwen-VL in Sep. 2023. Recently, the Qwen-VL series has undergone a significant upgrade with the launch of two enhanced versions, Qwen-VL-Plus and Qwen-VL-Max. The key technical advancements in these versions include:</p>
<ul>
<li>Substantially boost in image-related reasoning capabilities;</li>
<li>Considerable enhancement in recognizing, extracting, and analyzing details within images and texts contained therein;</li>
<li>Support for high-definition images with resolutions above one million pixels and images of various aspect ratios.</li>
</ul>

















<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th><strong>Model Name</strong></th><th><strong>Model Description</strong></th></tr></thead><tbody><tr><td>qwen-vl-plus</td><td>Qwen&#x27;s <strong>Enhanced Large Visual Language Model</strong>. Significantly upgraded for detailed recognition capabilities and text recognition abilities, supporting ultra-high pixel resolutions up to millions of pixels and arbitrary aspect ratios for image input. It delivers significant performance across a broad range of visual tasks.</td></tr><tr><td>qwen-vl-max</td><td>Qwen&#x27;s <strong>Most Capable Large Visual Language Model</strong>. Compared to the enhanced version, further improvements have been made to visual reasoning and instruction-following capabilities, offering a higher level of visual perception and cognitive understanding. It delivers optimal performance on an even broader range of complex tasks.</td></tr></tbody></table></div>
<p>Compared to the open-source version of Qwen-VL, these two models perform on par with Gemini Ultra and GPT-4V in multiple text-image multimodal tasks, significantly surpassing the previous best results from open-source models.</p>
<p>Notably, Qwen-VL-Max outperforms both GPT-4V from OpenAI and Gemini from Google in tasks on Chinese question answering and Chinese text comprehension. This breakthrough underscores the model’s advanced capabilities and its potential to set new standards in the field of multimodal AI research and application.</p>
<p>Model| DocVQA<br/>
Document understanding| ChartQA<br/>
Chart understanding| AI2D<br/>
Science diagrams| TextVQA<br/>
Text reading| MMMU<br/>
College-level problems| MathVista<br/>
Mathematical reasoning| MM-Bench-CN<br/>
Natural image QA in Chinese<br/>
---|---|---|---|---|---|---|---<br/>
Other Best<br/>
Open-source LVLM| 81.6%<br/>
(CogAgent)| 68.4%<br/>
(CogAgent)| 73.7%<br/>
(Fuyu-Medium)| 76.1%<br/>
(CogAgent)| 45.9%<br/>
(Yi-VL-34B)| 36.7%<br/>
(SPHINX-V2)| 72.4%<br/>
(InternLM-XComposer-VL)<br/>
Gemini Pro| 88.1%| 74.1%| 73.9%| 74.6%| 47.9%| 45.2%| 74.3%<br/>
Gemini Ultra| 90.9%| 80.8% 1| 79.5% 1| 82.3% 1| 59.4% 1| 53.0% 1| -<br/>
GPT-4V| 88.4%| 78.5%| 78.2%| 78.0%| 56.8%| 49.9%| 73.9%<br/>
<strong>Qwen-VL-Plus</strong>|  91.4%| 78.1%| 75.9%| 78.9%| 45.2%| 43.3%| 68.0%<br/>
<strong>Qwen-VL-Max</strong>|  93.1% 1| 79.8% 2| 79.3% 2| 79.5% 2| 51.4% 3| 50.0% 2| 75.1% 1</p>
<p>New versions of the Qwen-VL-Plus and Qwen-VL-Max models not only exhibit exceptional benchmark performance but also demonstrate a marked improvement in problem-solving capabilities within real-world scenarios. These advanced models effortlessly engage in dialogue, identify celebrities and landmarks, generate text, and notably, have significantly enhanced their abilities to describe and interpret visual content.</p>
<p>Here we present some practical examples:</p>
<h3 class="flex scroll-m-28 flex-row items-center gap-2" id="1-basic-recognition-capabilities"><a data-card="" href="#1-basic-recognition-capabilities" class="peer">1. Basic Recognition Capabilities</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>
<p>The latest Qwen-VL models are now more adept at accurately describing and identifying complex information within images, as well as providing detailed background and answering related questions. For instance, Qwen-VL can recognize not only common objects, but also celebrities and landmarks. Qwen-VL can write poetry in various languages inspired by visuals, and analyze everyday screenshots.</p>
<p>Example: celebrity recognition Next</p>
<p>User</p>
<p>who are they?</p>
<p>Qwen-VL</p>
<p>These are some of the most famous and highest-paid celebrities in the world. From left to right, top to bottom:</p>
<ul>
<li>Justin Bieber (Canadian singer)</li>
<li>Lionel Messi (Argentine soccer player)</li>
<li>Cristiano Ronaldo (Portuguese soccer player)</li>
<li>Elon Musk (South African-born American entrepreneur and businessman)</li>
<li>Kylie Jenner (American media personality and socialite)</li>
<li>Taylor Swift (American singer-songwriter)</li>
<li>Robert Downey Jr. (American actor)</li>
<li>Scarlett Johansson (American actress)</li>
</ul>
<p>It’s worth noting that this is just a small sample of the many talented and successful celebrities out there, and that fame and wealth are not the only measures of success or importance.</p>
<p>User</p>
<p>their chinese name?</p>
<p>Qwen-VL</p>
<p>Here are the Chinese names of the celebrities mentioned:</p>
<ul>
<li>Justin Bieber: 贾斯汀·比伯</li>
<li>Lionel Messi: 莱昂内尔·梅西</li>
<li>Cristiano Ronaldo: 克里斯蒂亚诺·罗纳尔多</li>
<li>Elon Musk: 埃隆·马斯克</li>
<li>Kylie Jenner: 凯莉·詹娜</li>
<li>Taylor Swift: 泰勒·斯威夫特</li>
<li>Robert Downey Jr.: 小罗伯特·唐尼</li>
<li>Scarlett Johansson: 斯嘉丽·约翰逊</li>
</ul>
<p>Note that these are just one possible translation of their names into Chinese, and there may be other variations used in different contexts or regions.</p>
<p>Example: landmark recognition Next</p>
<p>User</p>
<p>Where is it in the picture? How did you recognize it?</p>
<p>Qwen-VL</p>
<p>The picture shows a busy street at night in New York City, USA. The Empire State Building can be seen in the background, which is a distinctive landmark of New York City. Therefore, I recognized it as New York City.</p>
<p>Example: poem writing Next</p>
<p>User</p>
<p>Look at pictures and write a poem.</p>
<p>Qwen-VL</p>
<p>A rocky crag by the sea, A place of solitude and mystery. The water calm and blue, Reflecting the sky above, anew.</p>
<p>The mountain stands tall and proud, Its peak reaching towards the clouds. A symbol of strength and resilience, Enduring through time’s relentless persistence.</p>
<p>The world around may change and shift, But this mountain remains steadfast and uplifted. A reminder to us all, To stand firm in our own personal call.</p>
<p>So let us take a moment to pause, And appreciate this natural cause. For in its beauty and grandeur, We find inspiration and renewal, forever.</p>
<p>Example: mobile app screenshot understanding Next</p>
<p>User</p>
<p>Describe the image.</p>
<p>Qwen-VL</p>
<p>The image is a screenshot of an app displaying a map of Beijing, China. The map is centered on the Summer Palace and Yuyuantan Park. The app provides directions for driving from Yuyuantan Park to the Summer Palace. It offers two options: one takes 22 minutes and the other takes 25 minutes. The app also indicates that there are local plate restrictions on the route.</p>
<h3 class="flex scroll-m-28 flex-row items-center gap-2" id="2-visual-agent-capability-the-essential-role-of-localization"><a data-card="" href="#2-visual-agent-capability-the-essential-role-of-localization" class="peer">2. Visual Agent Capability: The Essential Role of Localization</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>
<p>Beyond its fundamental capabilities in description and recognition, Qwen-VL also has impressive abilities to pinpoint and query specific elements. For instance, it can accurately highlight the black cars within an image. Moreover, Qwen-VL is also equipped to make judgments, deductions, and decisions based on the prevailing context of a scene.</p>
<p>Example: Locate Whatever You Desire Next</p>
<p>User</p>
<p>Locate the red car</p>
<p>Qwen-VL</p>
<p>The red car is located in the bottom right corner of the image.</p>
<p>Example: Contextually Anchored Image Description Generation Next</p>
<p>User</p>
<p>What does the pointed news talk about?</p>
<p>Qwen-VL</p>
<p>The pointed news is about OpenAI’s chatbot ChatGPT having personalized instructions to store your preferences.</p>
<p>Example: Autonomous Driving Next</p>
<p>User</p>
<p>The current vehicle speed is 150km/h. According to the picture, how should the vehicle operate? A. To accelerate B. Slow down C. Maintain the original speed D. Parking Choose an answer and give reasons.</p>
<p>Qwen-VL</p>
<p>B. Slow down</p>
<p>According to the traffic sign in the picture, the maximum speed limit for the current lane is 100 km/h. If the current vehicle speed is 150 km/h, it exceeds the speed limit and should slow down to comply with the traffic regulations.</p>
<h3 class="flex scroll-m-28 flex-row items-center gap-2" id="3-visual-reasoning-capability-to-solve-real-problems"><a data-card="" href="#3-visual-reasoning-capability-to-solve-real-problems" class="peer">3. Visual Reasoning Capability: To Solve Real Problems</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>
<p>One of the most notable advancements in the latest Qwen-VL is its capacity for complex reasoning based on visual inputs. This enhanced visual reasoning capability goes well beyond mere content description, extending to the comprehension and interpretation of intricate representations such as flowcharts, diagrams, and other symbolic systems. In the realms of problem-solving and reasoning, Qwen-VL-Plus/Max excels not only in mathematical problem-solving and information organization but also in conducting more profound interpretations and analyses of charts and graphs.</p>
<p>Example: Math Next</p>
<p>User</p>
<p>read the image and solve it step by step</p>
<p>Qwen-VL</p>
<p>The image shows two geometric figures, a rectangular prism and a cube, with their dimensions labeled. The problem asks to calculate the surface area and volume of these figures.</p>
<ol>
<li>For the rectangular prism:</li>
</ol>
<ul>
<li>Length (l) = 4 cm</li>
<li>Width (w) = 2 cm</li>
<li>Height (h) = 3 cm</li>
</ul>
<p>Surface Area (SA) of a rectangular prism = 2lw + 2lh + 2wh Volume (V) of a rectangular prism = lwh</p>
<p>Substituting the given values: SA = 2(4cm)(2cm) + 2(4cm)(3cm) + 2(2cm)(3cm) SA = 16cm² + 24cm² + 12cm² SA = 52cm²</p>
<p>V = (4cm)(2cm)(3cm) V = 24cm³</p>
<ol start="2">
<li>For the cube:</li>
</ol>
<ul>
<li>Side length (a) = 3 dm</li>
</ul>
<p>Surface Area (SA) of a cube = 6a² Volume (V) of a cube = a³</p>
<p>Substituting the given value: SA = 6(3dm)² SA = 6(9dm²) SA = 54dm²</p>
<p>V = (3dm)³ V = 27dm³</p>
<p>So, the surface area and volume for each figure are as follows:</p>
<p>For the rectangular prism:</p>
<ul>
<li>Surface Area = 52 cm²</li>
<li>Volume = 24 cm³</li>
</ul>
<p>For the cube:</p>
<ul>
<li>Surface Area = 54 dm²</li>
<li>Volume = 27 dm³</li>
</ul>
<p>Example: Code Next</p>
<p>User</p>
<p>convert the process into a python code program</p>
<p>Qwen-VL</p>
<p>The image shows a Scratch programming block, which can be converted into Python code as follows:</p>
<figure dir="ltr" class="my-4 bg-fd-card rounded-xl shiki relative border shadow-sm not-prose overflow-hidden text-sm shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e" tabindex="-1"><div class="empty:hidden absolute top-3 right-2 z-2 backdrop-blur-lg rounded-lg text-fd-muted-foreground"><button type="button" class="inline-flex items-center justify-center rounded-md text-sm font-medium transition-colors duration-100 disabled:pointer-events-none disabled:opacity-50 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-fd-ring p-1 [&amp;_svg]:size-4 hover:text-fd-accent-foreground data-checked:text-fd-accent-foreground" aria-label="Copy Text"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-clipboard" aria-hidden="true"><rect width="8" height="4" x="8" y="2" rx="1" ry="1"></rect><path d="M16 4h2a2 2 0 0 1 2 2v14a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V6a2 2 0 0 1 2-2h2"></path></svg></button></div><div role="region" tabindex="0" class="text-[0.8125rem] py-3.5 overflow-auto max-h-[600px] fd-scroll-container focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-inset focus-visible:ring-fd-ring" style="--padding-right:calc(var(--spacing) * 8)"><pre class="min-w-full w-max *:flex *:flex-col"><code><span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">    import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> random</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    </span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">    # Set up the variables</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    a </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> random.randint(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">50</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    b </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> random.randint(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">50</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    answer </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> a </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">+</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> b</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    </span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">    # Ask the user for their input</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    response </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF"> int</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">input</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">f</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">&quot;What is </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">a</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF"> + </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">b</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">}</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">? &quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    </span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">    # Check if the response matches the answer</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> response </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">==</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> answer:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">&quot;Correct!&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">    else</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">&quot;Incorrect!&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    </span></span></code></pre></div></figure>
<p>This Python code generates two random numbers between 1 and 50, adds them together to get the answer, and then asks the user for their response. It then checks if the response matches the answer, and prints out a message accordingly.</p>
<p>Example: Table Next</p>
<p>User</p>
<p>Spot a data point that stands out in these charts and what that implicates. Then produce a detailed markdown table for all the data shown.</p>
<p>Qwen-VL</p>
<p>The most striking data point in these charts is the high percentage of plastic waste that is mismanaged in the United States, which stands at 4%. This implies that a significant portion of plastic waste in the US is not being properly disposed of or recycled, leading to environmental pollution and potential harm to ecosystems and human health.</p>
<p>Here’s a detailed markdown table for all the data shown:</p>








































<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Region</th><th>Landfilled (2019)</th><th>Mismanaged (2019)</th><th>Incinerated (2019)</th><th>Recycled (2019)</th></tr></thead><tbody><tr><td>World</td><td>49%</td><td>22%</td><td>19%</td><td>9%</td></tr><tr><td>United States</td><td>73%</td><td>4%</td><td>19%</td><td>4%</td></tr><tr><td>Europe</td><td>44%</td><td>6%</td><td>38%</td><td>12%</td></tr><tr><td>Asia (excluding China and India)</td><td>39%</td><td>34%</td><td>19%</td><td>8%</td></tr></tbody></table></div>
<p>Note: The data source for this information is the OECD (Organisation for Economic Co-operation and Development), and the percentages represent the share of plastic waste that is recycled, landfilled, incinerated, and mismanaged in each region in 2019.</p>
<p>Example: Reasoning Next</p>
<p>User</p>
<p>Look at this sequence of three shapes. What shape should come as the fourth shape? Explain your reasoning with detailed descriptions of the first shapes.</p>
<p>Qwen-VL</p>
<p>The sequence of shapes is a triangle, square, and pentagon, followed by a question mark. This pattern suggests that the next shape in the sequence should be a hexagon. The pattern appears to be based on the number of sides in each shape, with each subsequent shape having one more side than the previous one. Therefore, the fourth shape should have six sides, making it a hexagon.</p>
<p>User</p>
<p>翻译成中文</p>
<p>Qwen-VL</p>
<p>看这个三个形状的序列。下一个形状应该是什么形状？解释你的推理过程。 形状序列是三角形、正方形和五边形，然后是一个问号。这表明下一个形状应该是六边形。这个序列似乎基于每个形状的边数，每个后续形状比前一个形状多一条边。因此，第四个形状应该有六条边，使它成为一个六边形。</p>
<p>Example: Model Flow Next</p>
<p>User</p>
<p>explain the picture</p>
<p>Qwen-VL</p>
<p>The image presents a three-stage training pipeline for the Qwen-VL series, a visual language model. Each stage is represented by a box labeled “QwenLM” and is accompanied by a smaller box indicating the type of data used in that stage.</p>
<p>In Stage 1, the model is pre-trained using image-text pairs. This stage involves learning from a large dataset of images and their corresponding text descriptions to understand the relationship between visual and linguistic information.</p>
<p>Stage 2 is multi-task pre-training with multi-task and interleaved VL data. Here, the model is trained on multiple tasks simultaneously, such as image captioning, visual question answering, and referring expression comprehension. The use of interleaved data helps the model learn to switch between tasks efficiently.</p>
<p>Finally, in Stage 3, the model undergoes supervised finetuning with chat-interleaved VL data. This stage involves refining the model’s performance on specific tasks using labeled data. The chat-interleaved data provides a more conversational context for the model to learn from, enhancing its ability to understand and respond to natural language queries.</p>
<p>Overall, this three-stage training pipeline aims to develop a robust visual language model capable of understanding and generating human-like responses to visual and linguistic inputs.</p>
<h3 class="flex scroll-m-28 flex-row items-center gap-2" id="4-text-information-recognition--processing"><a data-card="" href="#4-text-information-recognition--processing" class="peer">4. Text Information Recognition &amp; Processing</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>
<p>Text processing in images has also improved significantly, especially in terms of recognizing Chinese and English text. Qwen-VL-Plus/Max can now efficiently extract information from tables and documents and reformat this information to meet custom output requirements. In addition, it has an efficient mechanism for identifying and converting dense text, which is very effective in dealing with documents that contain a lot of information. It supports images with extreme aspect ratios, ensuring the flexibility to process diverse visual content.</p>
<p>Example: Information Retrieval Next</p>
<p>User</p>
<p>Please read the text in this image and return the information in the following JSON format (note xxx is placeholder, if the information is not available in the image, put “N/A” instead). {“class”: xxx, “DLN”: xxx, “DOB”: xxx, “Name”: xxx, “Address”: xxx, “EXP”: xxx, “ISS”: xxX, “SEX”: xxx, “HGT”: xxx, “WGT”: xxx, “EYES”: xxx, “HAIR”: xxX, “DONOR”: xxx}</p>
<p>Qwen-VL</p>
<p>{“class”: “C”, “DLN”: “I1234568”, “DOB”: “08/31/1977”, “Name”: “IMA CARDHOLDER”, “Address”: “2570 24TH STREET ANYTOWN, CA 95818”, “EXP”: “08/31/2014”, “ISS”: “08/31/2009”, “SEX”: “F”, “HGT”: “5’-05&quot;”, “WGT”: “125 lb”, “EYES”: “BRN”, “HAIR”: “BRN”, “DONOR”: “YES”}</p>
<p>Example: Text Information Recognition &amp; Processing Next</p>
<p>User</p>
<p>规范格式输出图中所有文字</p>
<p>Qwen-VL</p>
<p>图片中的文字如下：</p>
<blockquote>
<p>学习提示</p>
<p>郭沫若的诗集《女神》是中国新诗的代表作品，它以崭新的内容和形式，表达了“五四”时期狂飙突进的时代精神。《立在地球边上放号》是《女神》中富有代表性的一首诗。诗人设想站在地球边上全方位俯瞰地球，放声呼唤，纵情高歌，想象着那怒涌的白云、壮丽的北冰洋和狂暴的太平洋，意在赞美摧毁旧世界、创造新生活的雄强之力，体现了“五四”所焕发的自由宏阔、雄奇奔放的气概。阅读时要注意联系“五四”特定的时代氛围来理解这首诗的内涵与形式特征。</p>
<p>闻一多的《红烛》化用“蜡炬”这一古典意象，赋予它新的含义，赞美的红烛以“烧蜡成灰”来点亮世界的奉献精神。注意体会诗人如何借助与红烛的“对话”表达青春的困惑与希望，以及对理想的坚毅追求。洋溢在诗中的幻想和情绪渲染，叹词的回环使用，诗句长短错落形成的节奏美，也是欣赏时关注的。</p>
<p>昌耀的《峨日朵雪峰之侧》内蕴丰富，阅读时要注意发挥想象，体味“雪峰”、“太阳”、“蜘蛛”等意象营造出的凝重而又壮美的氛围，感受其中蕴含的谦卑而强劲的生命力量。</p>
<p>雪莱的《致云雀》意蕴深刻，比喻新颖，要注意体会诗人的情感，理解云雀的象征意义；把握作品节与节之间环环相扣的特点，欣赏诗人想象的奇特和运用修辞手法的大胆、灵活。</p>
<p>学习这组诗作，要多借助朗读，理清诗歌的抒情脉络，把握意象的内涵，感悟诗人抒发的情思，体会象征手法的运用。</p>
<p>第一单元<br/>
11</p>
</blockquote>
<p>请注意，图片中的文字可能需要进一步的排版和格式化，以便于阅读和理解。如果您需要帮助，请随时告诉我。</p>
<p>Example: Text Information Recognition &amp; Processing Next</p>
<p>User</p>
<p>A brief summary of the blog</p>
<p>Qwen-VL</p>
<p>The blog post announces several new updates and improvements to OpenAI’s platform, including:</p>
<ul>
<li>GPT-4 Turbo with a 128K context window, which is more capable and supports a 128K context window.</li>
<li>New Assistants API that makes it easier for developers to build their own assistive AI apps and tools.</li>
<li>New multimodal capabilities in the platform, including text-to-speech (TTS).</li>
<li>Improved instruction following and JSON mode.</li>
<li>Reproducible outputs and log probabilities.</li>
<li>Updated GPT-3.5 Turbo with a 16K context window by default.</li>
</ul>
<p>The post also mentions that these updates will be rolled out to all paying customers starting at 10am PT today.</p>
<h2 class="flex scroll-m-28 flex-row items-center gap-2" id="how-to-use"><a data-card="" href="#how-to-use" class="peer">How to Use</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>Now you can access Qwen-VL-Plus and Qwen-VL-Max through the Huggingface Spaces, the Qwen website, and Dashscope APIs.</p>
<ul>
<li>
<p>Try Qwen-VL-Plus (<a href="https://huggingface.co/spaces/Qwen/Qwen-VL-Plus" rel="noreferrer noopener" target="_blank">https://huggingface.co/spaces/Qwen/Qwen-VL-Plus</a>) and Qwen-VL-Max (<a href="https://huggingface.co/spaces/Qwen/Qwen-VL-Max" rel="noreferrer noopener" target="_blank">https://huggingface.co/spaces/Qwen/Qwen-VL-Max</a>) in the Huggingface Spaces</p>
</li>
<li>
<p>Log in to the QianWen web portal at <a href="https://tongyi.aliyun.com/qianwen" rel="noreferrer noopener" target="_blank">https://tongyi.aliyun.com/qianwen</a>, and switch to “Image Understanding” mode to harness the latest Qwen-VL-Max capabilities.</p>
</li>
<li>
<p>Access the powerful APIs of Qwen-VL-Plus and Qwen-VL-Max through the Dashscope platform (<a href="https://help.aliyun.com/zh/dashscope/developer-reference/vl-plus-quick-start" rel="noreferrer noopener" target="_blank">https://help.aliyun.com/zh/dashscope/developer-reference/vl-plus-quick-start</a>).</p>
</li>
</ul>
<h2 class="flex scroll-m-28 flex-row items-center gap-2" id="summary"><a data-card="" href="#summary" class="peer">Summary</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>Qwen-VL-Plus and Qwen-VL-Max make significant strides in enhancing high-resolution recognition, text analysis, and image reasoning capabilities. These models now match the performance of GPT4-v and Gemini, outperforming all other open-source and proprietary models in many tasks, such as MMMU, CMMMU, and MathVista. They achieve world-class results in document analysis (DocVQA) and Chinese language-related image understanding (MM-Bench-CN).</p>
<p>Our objective is to continually tap into and elevate the potential of Qwen-VL, enabling it to make a difference across a broader range of applications. We can envision Qwen-VL as an assistant equipped with superhuman visual and linguistic comprehension skills that can provide robust support in everyday dialogues as well as complex scenarios like driving environments and programming contexts.</p>
<p>While there is still a long way to go, we are confident that Qwen-VL will evolve to perceive and understand the world akin to human cognition through continuous optimization and expansion!</p>
<ul>
<li>Our Official Website: <a href="https://tongyi.aliyun.com/qianwen" rel="noreferrer noopener" target="_blank">https://tongyi.aliyun.com/qianwen</a></li>
<li>Github: <a href="https://github.com/QwenLM/Qwen-VL" rel="noreferrer noopener" target="_blank">https://github.com/QwenLM/Qwen-VL</a></li>
<li>Huggingface: <a href="http://huggingface.co/Qwen/Qwen-VL-Chat" rel="noreferrer noopener" target="_blank">http://huggingface.co/Qwen/Qwen-VL-Chat</a></li>
<li>ModelScope: <a href="https://modelscope.cn/studios/qwen/Qwen-VL-Chat-Demo" rel="noreferrer noopener" target="_blank">https://modelscope.cn/studios/qwen/Qwen-VL-Chat-Demo</a></li>
<li>API: <a href="https://help.aliyun.com/zh/dashscope/developer-reference/tongyi-qianwen-vl-plus-api" rel="noreferrer noopener" target="_blank">https://help.aliyun.com/zh/dashscope/developer-reference/tongyi-qianwen-vl-plus-api</a></li>
<li>Discord: <a href="https://discord.gg/CV4E9rpNSD" rel="noreferrer noopener" target="_blank">https://discord.gg/CV4E9rpNSD</a></li>
</ul></div></main><!--$--><!--/$--><script src="/_next/static/chunks/e83606e8fa9cc796.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[106,[\"/_next/static/chunks/59d0ad1b64f8544e.js\"],\"RootProvider\"]\n3:I[53113,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"default\"]\n4:I[73211,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"default\"]\n5:I[10086,[\"/_next/static/chunks/59d0ad1b64f8544e.js\",\"/_next/static/chunks/36bfed0236ce2cf2.js\",\"/_next/static/chunks/e62b91212ee7f8ff.js\",\"/_next/static/chunks/2a98816c7d26bf58.js\",\"/_next/static/chunks/cb0a883bafeb6805.js\"],\"\"]\n7:I[89923,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"OutletBoundary\"]\n8:\"$Sreact.suspense\"\na:I[89923,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"ViewportBoundary\"]\nc:I[89923,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"MetadataBoundary\"]\ne:I[6998,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"default\"]\n:HL[\"/_next/static/chunks/f2332aac77592f9d.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"qMVpAZcUAPsCZEMM19Q64\",\"c\":[\"\",\"blog\",\"qwen-vl\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"qwen-vl\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/f2332aac77592f9d.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/59d0ad1b64f8544e.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"dark\",\"suppressHydrationWarning\":true,\"children\":[\"$\",\"body\",null,{\"className\":\"antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"main\",null,{\"className\":\"flex min-h-screen flex-col items-center justify-center px-4 text-center\",\"children\":[[\"$\",\"div\",null,{\"className\":\"mb-8 opacity-20\",\"children\":[\"$\",\"svg\",null,{\"width\":\"120\",\"height\":\"120\",\"viewBox\":\"0 0 120 120\",\"fill\":\"none\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"circle\",null,{\"cx\":\"60\",\"cy\":\"60\",\"r\":\"50\",\"stroke\":\"currentColor\",\"strokeWidth\":\"3\",\"strokeLinecap\":\"round\",\"strokeDasharray\":\"280 40\"}]}]}],[\"$\",\"p\",null,{\"className\":\"text-xs font-mono uppercase tracking-[0.3em] text-fd-muted-foreground mb-4\",\"children\":\"404\"}],[\"$\",\"h1\",null,{\"className\":\"text-3xl font-semibold mb-3\",\"children\":\"Page not found\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground max-w-sm mb-10\",\"children\":\"This page doesn't exist, or it may have moved. Try the documentation or head home.\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-3 justify-center\",\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/\",\"className\":\"inline-flex items-center gap-2 rounded-lg bg-fd-primary text-fd-primary-foreground px-4 py-2 text-sm font-medium hover:opacity-90 transition\",\"children\":\"Go home\"}],[\"$\",\"$L5\",null,{\"href\":\"/docs\",\"className\":\"inline-flex items-center gap-2 rounded-lg border border-fd-border bg-fd-background px-4 py-2 text-sm font-medium hover:bg-fd-muted transition\",\"children\":\"Documentation\"}],[\"$\",\"$L5\",null,{\"href\":\"/docs/models\",\"className\":\"inline-flex items-center gap-2 rounded-lg border border-fd-border bg-fd-background px-4 py-2 text-sm font-medium hover:bg-fd-muted transition\",\"children\":\"Browse models\"}]]}],[\"$\",\"p\",null,{\"className\":\"mt-16 text-xs font-mono text-fd-muted-foreground opacity-50\",\"children\":\"zenlm.org\"}]]}],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[\"$L6\",[[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/36bfed0236ce2cf2.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-1\",{\"src\":\"/_next/static/chunks/e62b91212ee7f8ff.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-2\",{\"src\":\"/_next/static/chunks/2a98816c7d26bf58.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-3\",{\"src\":\"/_next/static/chunks/cb0a883bafeb6805.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"$L7\",null,{\"children\":[\"$\",\"$8\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@9\"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$La\",null,{\"children\":\"$Lb\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$Lc\",null,{\"children\":[\"$\",\"$8\",null,{\"name\":\"Next.Metadata\",\"children\":\"$Ld\"}]}]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$e\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"6:[\"$\",\"main\",null,{\"className\":\"mx-auto w-full max-w-2xl px-4 py-16\",\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog\",\"className\":\"inline-flex items-center gap-1.5 text-sm text-fd-muted-foreground hover:text-fd-foreground mb-10 transition-colors\",\"children\":\"← Back to Blog\"}],[\"$\",\"div\",null,{\"className\":\"mb-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"January 24, 2024\"}],[\"$\",\"h1\",null,{\"className\":\"text-3xl font-bold mt-2 mb-3\",\"children\":\"Introducing Qwen-VL\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-lg mb-4\",\"children\":\"Along with the rapid development of our large language model Qwen, we leveraged Qwen’s capabilities and unified multimodal pretraining to address the limitations of multimodal models in generalization, and we opensourced multimodal model Qwen-VL in Sep. 2023. Recently, the Qwen-VL series has undergone a significant upgrade with the launch of two enhanced versions, Qwen-VL-Plus and Qwen-VL-Max. The key technical advancements in these versions include:\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3 pt-4 border-t border-fd-border\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-sm text-fd-muted-foreground\",\"children\":[\"By \",\"Zen LM Team\"]}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"prose dark:prose-invert max-w-none\",\"children\":[[\"$\",\"p\",null,{\"children\":\"Along with the rapid development of our large language model Qwen, we leveraged Qwen’s capabilities and unified multimodal pretraining to address the limitations of multimodal models in generalization, and we opensourced multimodal model Qwen-VL in Sep. 2023. Recently, the Qwen-VL series has undergone a significant upgrade with the launch of two enhanced versions, Qwen-VL-Plus and Qwen-VL-Max. The key technical advancements in these versions include:\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Substantially boost in image-related reasoning capabilities;\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Considerable enhancement in recognizing, extracting, and analyzing details within images and texts contained therein;\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Support for high-definition images with resolutions above one million pixels and images of various aspect ratios.\"}],\"\\n\"]}],\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Model Name\"}]}],[\"$\",\"th\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Model Description\"}]}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"qwen-vl-plus\"}],[\"$\",\"td\",null,{\"children\":[\"Qwen's \",[\"$\",\"strong\",null,{\"children\":\"Enhanced Large Visual Language Model\"}],\". Significantly upgraded for detailed recognition capabilities and text recognition abilities, supporting ultra-high pixel resolutions up to millions of pixels and arbitrary aspect ratios for image input. It delivers significant performance across a broad range of visual tasks.\"]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"qwen-vl-max\"}],[\"$\",\"td\",null,{\"children\":[\"Qwen's \",[\"$\",\"strong\",null,{\"children\":\"Most Capable Large Visual Language Model\"}],\". Compared to the enhanced version, further improvements have been made to visual reasoning and instruction-following capabilities, offering a higher level of visual perception and cognitive understanding. It delivers optimal performance on an even broader range of complex tasks.\"]}]]}]]}]]}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Compared to the open-source version of Qwen-VL, these two models perform on par with Gemini Ultra and GPT-4V in multiple text-image multimodal tasks, significantly surpassing the previous best results from open-source models.\"}],\"\\n\",\"$Lf\",\"\\n\",\"$L10\",\"\\n\",\"$L11\",\"\\n\",\"$L12\",\"\\n\",\"$L13\",\"\\n\",\"$L14\",\"\\n\",\"$L15\",\"\\n\",\"$L16\",\"\\n\",\"$L17\",\"\\n\",\"$L18\",\"\\n\",\"$L19\",\"\\n\",\"$L1a\",\"\\n\",\"$L1b\",\"\\n\",\"$L1c\",\"\\n\",\"$L1d\",\"\\n\",\"$L1e\",\"\\n\",\"$L1f\",\"\\n\",\"$L20\",\"\\n\",\"$L21\",\"\\n\",\"$L22\",\"\\n\",\"$L23\",\"\\n\",\"$L24\",\"\\n\",\"$L25\",\"\\n\",\"$L26\",\"\\n\",\"$L27\",\"\\n\",\"$L28\",\"\\n\",\"$L29\",\"\\n\",\"$L2a\",\"\\n\",\"$L2b\",\"\\n\",\"$L2c\",\"\\n\",\"$L2d\",\"\\n\",\"$L2e\",\"\\n\",\"$L2f\",\"\\n\",\"$L30\",\"\\n\",\"$L31\",\"\\n\",\"$L32\",\"\\n\",\"$L33\",\"\\n\",\"$L34\",\"\\n\",\"$L35\",\"\\n\",\"$L36\",\"\\n\",\"$L37\",\"\\n\",\"$L38\",\"\\n\",\"$L39\",\"\\n\",\"$L3a\",\"\\n\",\"$L3b\",\"\\n\",\"$L3c\",\"\\n\",\"$L3d\",\"\\n\",\"$L3e\",\"\\n\",\"$L3f\",\"\\n\",\"$L40\",\"\\n\",\"$L41\",\"\\n\",\"$L42\",\"\\n\",\"$L43\",\"\\n\",\"$L44\",\"\\n\",\"$L45\",\"\\n\",\"$L46\",\"\\n\",\"$L47\",\"\\n\",\"$L48\",\"\\n\",\"$L49\",\"\\n\",\"$L4a\",\"\\n\",\"$L4b\",\"\\n\",\"$L4c\",\"\\n\",\"$L4d\",\"\\n\",\"$L4e\",\"\\n\",\"$L4f\",\"\\n\",\"$L50\",\"\\n\",\"$L51\",\"\\n\",\"$L52\",\"\\n\",\"$L53\",\"\\n\",\"$L54\",\"\\n\",\"$L55\",\"\\n\",\"$L56\",\"\\n\",\"$L57\",\"\\n\",\"$L58\",\"\\n\",\"$L59\",\"\\n\",\"$L5a\",\"\\n\",\"$L5b\",\"\\n\",\"$L5c\",\"\\n\",\"$L5d\",\"\\n\",\"$L5e\",\"\\n\",\"$L5f\",\"\\n\",\"$L60\",\"\\n\",\"$L61\",\"\\n\",\"$L62\",\"\\n\",\"$L63\",\"\\n\",\"$L64\",\"\\n\",\"$L65\",\"\\n\",\"$L66\",\"\\n\",\"$L67\",\"\\n\",\"$L68\",\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L69\",\"\\n\",\"$L6a\",\"\\n\",\"$L6b\",\"\\n\",\"$L6c\",\"\\n\",\"$L6d\",\"\\n\",\"$L6e\",\"\\n\",\"$L6f\",\"\\n\",\"$L70\",\"\\n\",\"$L71\",\"\\n\",\"$L72\",\"\\n\",\"$L73\",\"\\n\",\"$L74\",\"\\n\",\"$L75\",\"\\n\",\"$L76\",\"\\n\",\"$L77\",\"\\n\",\"$L78\",\"\\n\",\"$L79\",\"\\n\",\"$L7a\",\"\\n\",\"$L7b\",\"\\n\",\"$L7c\",\"\\n\",\"$L7d\",\"\\n\",\"$L7e\",\"\\n\",\"$L7f\",\"\\n\",\"$L80\",\"\\n\",\"$L81\",\"\\n\",\"$L82\",\"\\n\",\"$L83\",\"\\n\",\"$L84\",\"\\n\",\"$L85\",\"\\n\",\"$L86\",\"\\n\",\"$L87\",\"\\n\",\"$L88\",\"\\n\",\"$L89\",\"\\n\",\"$L8a\",\"\\n\",\"$L8b\",\"\\n\",\"$L8c\",\"\\n\",\"$L8d\",\"\\n\",\"$L8e\",\"\\n\",\"$L8f\",\"\\n\",\"$L90\",\"\\n\",\"$L91\",\"\\n\",\"$L92\",\"\\n\",\"$L93\",\"\\n\",\"$L94\",\"\\n\",\"$L95\",\"\\n\",\"$L96\",\"\\n\",\"$L97\",\"\\n\",\"$L98\",\"\\n\",\"$L99\"]}]]}]\n"])</script><script>self.__next_f.push([1,"9a:I[51504,[\"/_next/static/chunks/59d0ad1b64f8544e.js\",\"/_next/static/chunks/36bfed0236ce2cf2.js\",\"/_next/static/chunks/e62b91212ee7f8ff.js\",\"/_next/static/chunks/2a98816c7d26bf58.js\",\"/_next/static/chunks/cb0a883bafeb6805.js\"],\"CodeBlock\"]\n9c:I[51504,[\"/_next/static/chunks/59d0ad1b64f8544e.js\",\"/_next/static/chunks/36bfed0236ce2cf2.js\",\"/_next/static/chunks/e62b91212ee7f8ff.js\",\"/_next/static/chunks/2a98816c7d26bf58.js\",\"/_next/static/chunks/cb0a883bafeb6805.js\"],\"Pre\"]\naa:I[48068,[\"/_next/static/chunks/59d0ad1b64f8544e.js\",\"/_next/static/chunks/36bfed0236ce2cf2.js\",\"/_next/static/chunks/e62b91212ee7f8ff.js\",\"/_next/static/chunks/2a98816c7d26bf58.js\",\"/_next/static/chunks/cb0a883bafeb6805.js\"],\"default\"]\nf:[\"$\",\"p\",null,{\"children\":\"Notably, Qwen-VL-Max outperforms both GPT-4V from OpenAI and Gemini from Google in tasks on Chinese question answering and Chinese text comprehension. This breakthrough underscores the model’s advanced capabilities and its potential to set new standards in the field of multimodal AI research and application.\"}]\n10:[\"$\",\"p\",null,{\"children\":[\"Model| DocVQA\",[\"$\",\"br\",null,{}],\"\\nDocument understanding| ChartQA\",[\"$\",\"br\",null,{}],\"\\nChart understanding| AI2D\",[\"$\",\"br\",null,{}],\"\\nScience diagrams| TextVQA\",[\"$\",\"br\",null,{}],\"\\nText reading| MMMU\",[\"$\",\"br\",null,{}],\"\\nCollege-level problems| MathVista\",[\"$\",\"br\",null,{}],\"\\nMathematical reasoning| MM-Bench-CN\",[\"$\",\"br\",null,{}],\"\\nNatural image QA in Chinese\",[\"$\",\"br\",null,{}],\"\\n---|---|---|---|---|---|---|---\",[\"$\",\"br\",null,{}],\"\\nOther Best\",[\"$\",\"br\",null,{}],\"\\nOpen-source LVLM| 81.6%\",[\"$\",\"br\",null,{}],\"\\n(CogAgent)| 68.4%\",[\"$\",\"br\",null,{}],\"\\n(CogAgent)| 73.7%\",[\"$\",\"br\",null,{}],\"\\n(Fuyu-Medium)| 76.1%\",[\"$\",\"br\",null,{}],\"\\n(CogAgent)| 45.9%\",[\"$\",\"br\",null,{}],\"\\n(Yi-VL-34B)| 36.7%\",[\"$\",\"br\",null,{}],\"\\n(SPHINX-V2)| 72.4%\",[\"$\",\"br\",null,{}],\"\\n(InternLM-XComposer-VL)\",[\"$\",\"br\",null,{}],\"\\nGemini Pro| 88.1%| 74.1%| 73.9%| 74.6%| 47.9%| 45.2%| 74.3%\",[\"$\",\"br\",null,{}],\"\\nGemini Ultra| 90.9%| 80.8% 1| 79.5% 1| 82.3% 1| 59.4% 1| 53.0% 1| -\",[\"$\",\"br\",null,{}],\"\\nGPT-4V| 88.4%| 78.5%| 78.2%| 78.0%| 56.8%| 49.9%| 73.9%\",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"strong\",null,{\"children\":\"Qwen-VL-Plus\"}],\"|  91.4%| 78.1%| 75.9%| 78.9%| 45.2%| 43.3%| 68.0%\",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"strong\",null,{\"children\":\"Qwen-VL-Max\"}],\"|  93.1% 1| 79.8% 2| 79.3% 2| 79.5% 2| 51.4% 3| 50.0% 2| 75.1% 1\"]}]\n11:[\"$\",\"p\",null,{\"children\":\"New versions of the Qwen-VL-Plus and Qwen-VL-Max models not only exhibit exceptional benchmark performance but also demonstrate a marked improvement in problem-solving capabilities within real-world scenarios. These advanced models effortlessly engage in dialogue, identify celebrities and landmarks, generate text, and notably, have significantly enhanced their abilities to describe and interpret visual content.\"}]\n12:[\"$\",\"p\",null,{\"children\":\"Here we present some practical examples:\"}]\n13:[\"$\",\"h3\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"1-basic-recognition-capabilities\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#1-basic-recognition-capabilities\",\"className\":\"peer\",\"children\":\"1. Basic Recognition Capabilities\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n14:[\"$\",\"p\",null,{\"children\":\"The latest Qwen-VL models are now more adept at accurately describing and identifying complex information within images, as well as providing detailed background and answering related questions. For instance, Qwen-VL can recognize not only common objects, but also celebrit"])</script><script>self.__next_f.push([1,"ies and landmarks. Qwen-VL can write poetry in various languages inspired by visuals, and analyze everyday screenshots.\"}]\n15:[\"$\",\"p\",null,{\"children\":\"Example: celebrity recognition Next\"}]\n16:[\"$\",\"p\",null,{\"children\":\"User\"}]\n17:[\"$\",\"p\",null,{\"children\":\"who are they?\"}]\n18:[\"$\",\"p\",null,{\"children\":\"Qwen-VL\"}]\n19:[\"$\",\"p\",null,{\"children\":\"These are some of the most famous and highest-paid celebrities in the world. From left to right, top to bottom:\"}]\n1a:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Justin Bieber (Canadian singer)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Lionel Messi (Argentine soccer player)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Cristiano Ronaldo (Portuguese soccer player)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Elon Musk (South African-born American entrepreneur and businessman)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Kylie Jenner (American media personality and socialite)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Taylor Swift (American singer-songwriter)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Robert Downey Jr. (American actor)\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Scarlett Johansson (American actress)\"}],\"\\n\"]}]\n1b:[\"$\",\"p\",null,{\"children\":\"It’s worth noting that this is just a small sample of the many talented and successful celebrities out there, and that fame and wealth are not the only measures of success or importance.\"}]\n1c:[\"$\",\"p\",null,{\"children\":\"User\"}]\n1d:[\"$\",\"p\",null,{\"children\":\"their chinese name?\"}]\n1e:[\"$\",\"p\",null,{\"children\":\"Qwen-VL\"}]\n1f:[\"$\",\"p\",null,{\"children\":\"Here are the Chinese names of the celebrities mentioned:\"}]\n20:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Justin Bieber: 贾斯汀·比伯\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Lionel Messi: 莱昂内尔·梅西\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Cristiano Ronaldo: 克里斯蒂亚诺·罗纳尔多\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Elon Musk: 埃隆·马斯克\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Kylie Jenner: 凯莉·詹娜\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Taylor Swift: 泰勒·斯威夫特\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Robert Downey Jr.: 小罗伯特·唐尼\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Scarlett Johansson: 斯嘉丽·约翰逊\"}],\"\\n\"]}]\n21:[\"$\",\"p\",null,{\"children\":\"Note that these are just one possible translation of their names into Chinese, and there may be other variations used in different contexts or regions.\"}]\n22:[\"$\",\"p\",null,{\"children\":\"Example: landmark recognition Next\"}]\n23:[\"$\",\"p\",null,{\"children\":\"User\"}]\n24:[\"$\",\"p\",null,{\"children\":\"Where is it in the picture? How did you recognize it?\"}]\n25:[\"$\",\"p\",null,{\"children\":\"Qwen-VL\"}]\n26:[\"$\",\"p\",null,{\"children\":\"The picture shows a busy street at night in New York City, USA. The Empire State Building can be seen in the background, which is a distinctive landmark of New York City. Therefore, I recognized it as New York City.\"}]\n27:[\"$\",\"p\",null,{\"children\":\"Example: poem writing Next\"}]\n28:[\"$\",\"p\",null,{\"children\":\"User\"}]\n29:[\"$\",\"p\",null,{\"children\":\"Look at pictures and write a poem.\"}]\n2a:[\"$\",\"p\",null,{\"children\":\"Qwen-VL\"}]\n2b:[\"$\",\"p\",null,{\"children\":\"A rocky crag by the sea, A place of solitude and mystery. The water calm and blue, Reflecting the sky above, anew.\"}]\n2c:[\"$\",\"p\",null,{\"children\":\"The mountain stands tall and proud, Its peak reaching towards the clouds. A symbol of strength and resilience, Enduring through time’s relentless persistence.\"}]\n2d:[\"$\",\"p\",null,{\"children\":\"The world around may change and shift, But this mountain remains steadfast and uplifted. A reminder to us all, To stand firm in our own personal call.\"}]\n2e:[\"$\",\"p\",null,{\"children\":\"So let us take a moment to pause, And appreciate this natural cause. For in its beauty and grandeur, We find inspiration and renewal, forever.\"}]\n2f:[\"$\",\"p\",null,{\"children\":\"Example: mobile app screenshot understanding Next\"}]\n30:[\"$\",\"p\",null,{\"children\":\"User\"}]\n31:[\"$\",\"p\",null,{\"children\":\"Describe the image.\"}]\n32:[\"$\",\"p\",null,{\"children\":\"Qwen-VL\"}]\n33:[\"$\",\"p\",null,{\"children\":\"The image is a screenshot of an app displaying a map of Beijing"])</script><script>self.__next_f.push([1,", China. The map is centered on the Summer Palace and Yuyuantan Park. The app provides directions for driving from Yuyuantan Park to the Summer Palace. It offers two options: one takes 22 minutes and the other takes 25 minutes. The app also indicates that there are local plate restrictions on the route.\"}]\n34:[\"$\",\"h3\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"2-visual-agent-capability-the-essential-role-of-localization\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#2-visual-agent-capability-the-essential-role-of-localization\",\"className\":\"peer\",\"children\":\"2. Visual Agent Capability: The Essential Role of Localization\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n35:[\"$\",\"p\",null,{\"children\":\"Beyond its fundamental capabilities in description and recognition, Qwen-VL also has impressive abilities to pinpoint and query specific elements. For instance, it can accurately highlight the black cars within an image. Moreover, Qwen-VL is also equipped to make judgments, deductions, and decisions based on the prevailing context of a scene.\"}]\n36:[\"$\",\"p\",null,{\"children\":\"Example: Locate Whatever You Desire Next\"}]\n37:[\"$\",\"p\",null,{\"children\":\"User\"}]\n38:[\"$\",\"p\",null,{\"children\":\"Locate the red car\"}]\n39:[\"$\",\"p\",null,{\"children\":\"Qwen-VL\"}]\n3a:[\"$\",\"p\",null,{\"children\":\"The red car is located in the bottom right corner of the image.\"}]\n3b:[\"$\",\"p\",null,{\"children\":\"Example: Contextually Anchored Image Description Generation Next\"}]\n3c:[\"$\",\"p\",null,{\"children\":\"User\"}]\n3d:[\"$\",\"p\",null,{\"children\":\"What does the pointed news talk about?\"}]\n3e:[\"$\",\"p\",null,{\"children\":\"Qwen-VL\"}]\n3f:[\"$\",\"p\",null,{\"children\":\"The pointed news is about OpenAI’s chatbot ChatGPT having personalized instructions to store your preferences.\"}]\n40:[\"$\",\"p\",null,{\"children\":\"Example: Autonomous Driving Next\"}]\n41:[\"$\",\"p\",null,{\"children\":\"User\"}]\n42:[\"$\",\"p\",null,{\"children\":\"The current vehicle speed is 150km/h. According to the picture, how should the vehicle operate? A. To accelerate B. Slow down C. Maintain the original speed D. Parking Choose an answer and give reasons.\"}]\n43:[\"$\",\"p\",null,{\"children\":\"Qwen-VL\"}]\n44:[\"$\",\"p\",null,{\"children\":\"B. Slow down\"}]\n45:[\"$\",\"p\",null,{\"children\":\"According to the traffic sign in the picture, the maximum speed limit for the current lane is 100 km/h. If the current vehicle speed is 150 km/h, it exceeds the speed limit and should slow down to comply with the traffic regulations.\"}]\n46:[\"$\",\"h3\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"3-visual-reasoning-capability-to-solve-real-problems\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#3-visual-reasoning-capability-to-solve-real-problems\",\"className\":\"peer\",\"children\":\"3. Visual Reasoning Capability: To Solve Real Problems\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n47:[\"$\",\"p\",null,{\"children\":\"One of the most notable advancements in the latest Qwen-VL is its capacity for complex reasoning based on visual inputs. This enhanced visual"])</script><script>self.__next_f.push([1," reasoning capability goes well beyond mere content description, extending to the comprehension and interpretation of intricate representations such as flowcharts, diagrams, and other symbolic systems. In the realms of problem-solving and reasoning, Qwen-VL-Plus/Max excels not only in mathematical problem-solving and information organization but also in conducting more profound interpretations and analyses of charts and graphs.\"}]\n48:[\"$\",\"p\",null,{\"children\":\"Example: Math Next\"}]\n49:[\"$\",\"p\",null,{\"children\":\"User\"}]\n4a:[\"$\",\"p\",null,{\"children\":\"read the image and solve it step by step\"}]\n4b:[\"$\",\"p\",null,{\"children\":\"Qwen-VL\"}]\n4c:[\"$\",\"p\",null,{\"children\":\"The image shows two geometric figures, a rectangular prism and a cube, with their dimensions labeled. The problem asks to calculate the surface area and volume of these figures.\"}]\n4d:[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"For the rectangular prism:\"}],\"\\n\"]}]\n4e:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Length (l) = 4 cm\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Width (w) = 2 cm\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Height (h) = 3 cm\"}],\"\\n\"]}]\n4f:[\"$\",\"p\",null,{\"children\":\"Surface Area (SA) of a rectangular prism = 2lw + 2lh + 2wh Volume (V) of a rectangular prism = lwh\"}]\n50:[\"$\",\"p\",null,{\"children\":\"Substituting the given values: SA = 2(4cm)(2cm) + 2(4cm)(3cm) + 2(2cm)(3cm) SA = 16cm² + 24cm² + 12cm² SA = 52cm²\"}]\n51:[\"$\",\"p\",null,{\"children\":\"V = (4cm)(2cm)(3cm) V = 24cm³\"}]\n52:[\"$\",\"ol\",null,{\"start\":\"2\",\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"For the cube:\"}],\"\\n\"]}]\n53:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Side length (a) = 3 dm\"}],\"\\n\"]}]\n54:[\"$\",\"p\",null,{\"children\":\"Surface Area (SA) of a cube = 6a² Volume (V) of a cube = a³\"}]\n55:[\"$\",\"p\",null,{\"children\":\"Substituting the given value: SA = 6(3dm)² SA = 6(9dm²) SA = 54dm²\"}]\n56:[\"$\",\"p\",null,{\"children\":\"V = (3dm)³ V = 27dm³\"}]\n57:[\"$\",\"p\",null,{\"children\":\"So, the surface area and volume for each figure are as follows:\"}]\n58:[\"$\",\"p\",null,{\"children\":\"For the rectangular prism:\"}]\n59:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Surface Area = 52 cm²\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Volume = 24 cm³\"}],\"\\n\"]}]\n5a:[\"$\",\"p\",null,{\"children\":\"For the cube:\"}]\n5b:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"Surface Area = 54 dm²\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Volume = 27 dm³\"}],\"\\n\"]}]\n5c:[\"$\",\"p\",null,{\"children\":\"Example: Code Next\"}]\n5d:[\"$\",\"p\",null,{\"children\":\"User\"}]\n5e:[\"$\",\"p\",null,{\"children\":\"convert the process into a python code program\"}]\n5f:[\"$\",\"p\",null,{\"children\":\"Qwen-VL\"}]\n60:[\"$\",\"p\",null,{\"children\":\"The image shows a Scratch programming block, which can be converted into Python code as follows:\"}]\n9b:T5c0,"])</script><script>self.__next_f.push([1,"\u003csvg viewBox=\"0 0 24 24\"\u003e\u003cpath d=\"M14.25.18l.9.2.73.26.59.3.45.32.34.34.25.34.16.33.1.3.04.26.02.2-.01.13V8.5l-.05.63-.13.55-.21.46-.26.38-.3.31-.33.25-.35.19-.35.14-.33.1-.3.07-.26.04-.21.02H8.77l-.69.05-.59.14-.5.22-.41.27-.33.32-.27.35-.2.36-.15.37-.1.35-.07.32-.04.27-.02.21v3.06H3.17l-.21-.03-.28-.07-.32-.12-.35-.18-.36-.26-.36-.36-.35-.46-.32-.59-.28-.73-.21-.88-.14-1.05-.05-1.23.06-1.22.16-1.04.24-.87.32-.71.36-.57.4-.44.42-.33.42-.24.4-.16.36-.1.32-.05.24-.01h.16l.06.01h8.16v-.83H6.18l-.01-2.75-.02-.37.05-.34.11-.31.17-.28.25-.26.31-.23.38-.2.44-.18.51-.15.58-.12.64-.1.71-.06.77-.04.84-.02 1.27.05zm-6.3 1.98l-.23.33-.08.41.08.41.23.34.33.22.41.09.41-.09.33-.22.23-.34.08-.41-.08-.41-.23-.33-.33-.22-.41-.09-.41.09zm13.09 3.95l.28.06.32.12.35.18.36.27.36.35.35.47.32.59.28.73.21.88.14 1.04.05 1.23-.06 1.23-.16 1.04-.24.86-.32.71-.36.57-.4.45-.42.33-.42.24-.4.16-.36.09-.32.05-.24.02-.16-.01h-8.22v.82h5.84l.01 2.76.02.36-.05.34-.11.31-.17.29-.25.25-.31.24-.38.2-.44.17-.51.15-.58.13-.64.09-.71.07-.77.04-.84.01-1.27-.04-1.07-.14-.9-.2-.73-.25-.59-.3-.45-.33-.34-.34-.25-.34-.16-.33-.1-.3-.04-.25-.02-.2.01-.13v-5.34l.05-.64.13-.54.21-.46.26-.38.3-.32.33-.24.35-.2.35-.14.33-.1.3-.06.26-.04.21-.02.13-.01h5.84l.69-.05.59-.14.5-.21.41-.28.33-.32.27-.35.2-.36.15-.36.1-.35.07-.32.04-.28.02-.21V6.07h2.09l.14.01zm-6.47 14.25l-.23.33-.08.41.08.41.23.33.33.23.41.08.41-.08.33-.23.23-.33.08-.41-.08-.41-.23-.33-.33-.23-.41-.08-.41.08z\" fill=\"currentColor\" /\u003e\u003c/svg\u003e"])</script><script>self.__next_f.push([1,"61:[\"$\",\"$L9a\",null,{\"className\":\"shiki shiki-themes github-light github-dark\",\"style\":{\"--shiki-light\":\"#24292e\",\"--shiki-dark\":\"#e1e4e8\",\"--shiki-light-bg\":\"#fff\",\"--shiki-dark-bg\":\"#24292e\"},\"tabIndex\":\"0\",\"icon\":\"$9b\",\"children\":[\"$\",\"$L9c\",null,{\"children\":[\"$\",\"code\",null,{\"children\":[[\"$\",\"span\",null,{\"className\":\"line\"}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"    import\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\" random\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    \"}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#6A737D\",\"--shiki-dark\":\"#6A737D\"},\"children\":\"    # Set up the variables\"}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    a \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\" random.randint(\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#005CC5\",\"--shiki-dark\":\"#79B8FF\"},\"children\":\"1\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\", \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#005CC5\",\"--shiki-dark\":\"#79B8FF\"},\"children\":\"50\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\")\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    b \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\" random.randint(\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#005CC5\",\"--shiki-dark\":\"#79B8FF\"},\"children\":\"1\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\", \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#005CC5\",\"--shiki-dark\":\"#79B8FF\"},\"children\":\"50\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\")\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    answer \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"=\"}],\"$L9d\",\"$L9e\",\"$L9f\"]}],\"\\n\",\"$La0\",\"\\n\",\"$La1\",\"\\n\",\"$La2\",\"\\n\",\"$La3\",\"\\n\",\"$La4\",\"\\n\",\"$La5\",\"\\n\",\"$La6\",\"\\n\",\"$La7\",\"\\n\",\"$La8\",\"\\n\",\"$La9\"]}]}]}]\n"])</script><script>self.__next_f.push([1,"62:[\"$\",\"p\",null,{\"children\":\"This Python code generates two random numbers between 1 and 50, adds them together to get the answer, and then asks the user for their response. It then checks if the response matches the answer, and prints out a message accordingly.\"}]\n63:[\"$\",\"p\",null,{\"children\":\"Example: Table Next\"}]\n64:[\"$\",\"p\",null,{\"children\":\"User\"}]\n65:[\"$\",\"p\",null,{\"children\":\"Spot a data point that stands out in these charts and what that implicates. Then produce a detailed markdown table for all the data shown.\"}]\n66:[\"$\",\"p\",null,{\"children\":\"Qwen-VL\"}]\n67:[\"$\",\"p\",null,{\"children\":\"The most striking data point in these charts is the high percentage of plastic waste that is mismanaged in the United States, which stands at 4%. This implies that a significant portion of plastic waste in the US is not being properly disposed of or recycled, leading to environmental pollution and potential harm to ecosystems and human health.\"}]\n68:[\"$\",\"p\",null,{\"children\":\"Here’s a detailed markdown table for all the data shown:\"}]\n69:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Region\"}],[\"$\",\"th\",null,{\"children\":\"Landfilled (2019)\"}],[\"$\",\"th\",null,{\"children\":\"Mismanaged (2019)\"}],[\"$\",\"th\",null,{\"children\":\"Incinerated (2019)\"}],[\"$\",\"th\",null,{\"children\":\"Recycled (2019)\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"World\"}],[\"$\",\"td\",null,{\"children\":\"49%\"}],[\"$\",\"td\",null,{\"children\":\"22%\"}],[\"$\",\"td\",null,{\"children\":\"19%\"}],[\"$\",\"td\",null,{\"children\":\"9%\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"United States\"}],[\"$\",\"td\",null,{\"children\":\"73%\"}],[\"$\",\"td\",null,{\"children\":\"4%\"}],[\"$\",\"td\",null,{\"children\":\"19%\"}],[\"$\",\"td\",null,{\"children\":\"4%\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Europe\"}],[\"$\",\"td\",null,{\"children\":\"44%\"}],[\"$\",\"td\",null,{\"children\":\"6%\"}],[\"$\",\"td\",null,{\"children\":\"38%\"}],[\"$\",\"td\",null,{\"children\":\"12%\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Asia (excluding China and India)\"}],[\"$\",\"td\",null,{\"children\":\"39%\"}],[\"$\",\"td\",null,{\"children\":\"34%\"}],[\"$\",\"td\",null,{\"children\":\"19%\"}],[\"$\",\"td\",null,{\"children\":\"8%\"}]]}]]}]]}]}]\n6a:[\"$\",\"p\",null,{\"children\":\"Note: The data source for this information is the OECD (Organisation for Economic Co-operation and Development), and the percentages represent the share of plastic waste that is recycled, landfilled, incinerated, and mismanaged in each region in 2019.\"}]\n6b:[\"$\",\"p\",null,{\"children\":\"Example: Reasoning Next\"}]\n6c:[\"$\",\"p\",null,{\"children\":\"User\"}]\n6d:[\"$\",\"p\",null,{\"children\":\"Look at this sequence of three shapes. What shape should come as the fourth shape? Explain your reasoning with detailed descriptions of the first shapes.\"}]\n6e:[\"$\",\"p\",null,{\"children\":\"Qwen-VL\"}]\n6f:[\"$\",\"p\",null,{\"children\":\"The sequence of shapes is a triangle, square, and pentagon, followed by a question mark. This pattern suggests that the next shape in the sequence should be a hexagon. The pattern appears to be based on the number of sides in each shape, with each subsequent shape having one more side than the previous one. Therefore, the fourth shape should have six sides, making it a hexagon.\"}]\n70:[\"$\",\"p\",null,{\"children\":\"User\"}]\n71:[\"$\",\"p\",null,{\"children\":\"翻译成中文\"}]\n72:[\"$\",\"p\",null,{\"children\":\"Qwen-VL\"}]\n73:[\"$\",\"p\",null,{\"children\":\"看这个三个形状的序列。下一个形状应该是什么形状？解释你的推理过程。 形状序列是三角形、正方形和五边形，然后是一个问号。这表明下一个形状应该是六边形。这个序列似乎基于每个形状的边数，每个后续形状比前一个形状多一条边。因此，第四个形状应该有六条边，使它成为一个六边形。\"}]\n74:[\"$\",\"p\",null,{\"children\":\"Example: Model Flow Next\"}]\n75:[\"$\",\"p\",null,{\"children\":\"User\"}]\n76:[\"$\",\"p\",null,{\"children\":\"explain the picture\"}]\n77:[\"$\",\""])</script><script>self.__next_f.push([1,"p\",null,{\"children\":\"Qwen-VL\"}]\n78:[\"$\",\"p\",null,{\"children\":\"The image presents a three-stage training pipeline for the Qwen-VL series, a visual language model. Each stage is represented by a box labeled “QwenLM” and is accompanied by a smaller box indicating the type of data used in that stage.\"}]\n79:[\"$\",\"p\",null,{\"children\":\"In Stage 1, the model is pre-trained using image-text pairs. This stage involves learning from a large dataset of images and their corresponding text descriptions to understand the relationship between visual and linguistic information.\"}]\n7a:[\"$\",\"p\",null,{\"children\":\"Stage 2 is multi-task pre-training with multi-task and interleaved VL data. Here, the model is trained on multiple tasks simultaneously, such as image captioning, visual question answering, and referring expression comprehension. The use of interleaved data helps the model learn to switch between tasks efficiently.\"}]\n7b:[\"$\",\"p\",null,{\"children\":\"Finally, in Stage 3, the model undergoes supervised finetuning with chat-interleaved VL data. This stage involves refining the model’s performance on specific tasks using labeled data. The chat-interleaved data provides a more conversational context for the model to learn from, enhancing its ability to understand and respond to natural language queries.\"}]\n7c:[\"$\",\"p\",null,{\"children\":\"Overall, this three-stage training pipeline aims to develop a robust visual language model capable of understanding and generating human-like responses to visual and linguistic inputs.\"}]\n7d:[\"$\",\"h3\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"4-text-information-recognition--processing\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#4-text-information-recognition--processing\",\"className\":\"peer\",\"children\":\"4. Text Information Recognition \u0026 Processing\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n7e:[\"$\",\"p\",null,{\"children\":\"Text processing in images has also improved significantly, especially in terms of recognizing Chinese and English text. Qwen-VL-Plus/Max can now efficiently extract information from tables and documents and reformat this information to meet custom output requirements. In addition, it has an efficient mechanism for identifying and converting dense text, which is very effective in dealing with documents that contain a lot of information. It supports images with extreme aspect ratios, ensuring the flexibility to process diverse visual content.\"}]\n7f:[\"$\",\"p\",null,{\"children\":\"Example: Information Retrieval Next\"}]\n80:[\"$\",\"p\",null,{\"children\":\"User\"}]\n81:[\"$\",\"p\",null,{\"children\":\"Please read the text in this image and return the information in the following JSON format (note xxx is placeholder, if the information is not available in the image, put “N/A” instead). {“class”: xxx, “DLN”: xxx, “DOB”: xxx, “Name”: xxx, “Address”: xxx, “EXP”: xxx, “ISS”: xxX, “SEX”: xxx, “HGT”: xxx, “WGT”: xxx, “EYES”: xxx, “HAIR”: xxX, “DONOR”: xxx}\"}]\n82:[\"$\",\"p\",null,{\"children\":\"Qwen-VL\"}]\n83:[\"$\",\"p\",null,{\"children\":\"{“class”: “C”, “DLN”: “I1234568”, “DOB”: “08/31/1977”, “Name”: “IMA CARDHOLDER”, “Address”: “2570 24TH STREET ANYTOWN, CA 95818”, “EXP”: “08/31/2014”, “ISS”: “08/31/2009”, “SEX”: “F”, “HGT”: “5’-05\\\"”, “WGT”: “125 lb”, “EYES”: “BRN”, “HAIR”: “BRN”, “DONOR”: “YES”}\"}]\n84:[\"$\",\"p\",null,{\"children\":\"Example: Text Information Recognition \u0026 Processing Next\"}]\n85:[\"$\",\"p\",nul"])</script><script>self.__next_f.push([1,"l,{\"children\":\"User\"}]\n86:[\"$\",\"p\",null,{\"children\":\"规范格式输出图中所有文字\"}]\n87:[\"$\",\"p\",null,{\"children\":\"Qwen-VL\"}]\n88:[\"$\",\"p\",null,{\"children\":\"图片中的文字如下：\"}]\n89:[\"$\",\"blockquote\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"学习提示\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"郭沫若的诗集《女神》是中国新诗的代表作品，它以崭新的内容和形式，表达了“五四”时期狂飙突进的时代精神。《立在地球边上放号》是《女神》中富有代表性的一首诗。诗人设想站在地球边上全方位俯瞰地球，放声呼唤，纵情高歌，想象着那怒涌的白云、壮丽的北冰洋和狂暴的太平洋，意在赞美摧毁旧世界、创造新生活的雄强之力，体现了“五四”所焕发的自由宏阔、雄奇奔放的气概。阅读时要注意联系“五四”特定的时代氛围来理解这首诗的内涵与形式特征。\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"闻一多的《红烛》化用“蜡炬”这一古典意象，赋予它新的含义，赞美的红烛以“烧蜡成灰”来点亮世界的奉献精神。注意体会诗人如何借助与红烛的“对话”表达青春的困惑与希望，以及对理想的坚毅追求。洋溢在诗中的幻想和情绪渲染，叹词的回环使用，诗句长短错落形成的节奏美，也是欣赏时关注的。\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"昌耀的《峨日朵雪峰之侧》内蕴丰富，阅读时要注意发挥想象，体味“雪峰”、“太阳”、“蜘蛛”等意象营造出的凝重而又壮美的氛围，感受其中蕴含的谦卑而强劲的生命力量。\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"雪莱的《致云雀》意蕴深刻，比喻新颖，要注意体会诗人的情感，理解云雀的象征意义；把握作品节与节之间环环相扣的特点，欣赏诗人想象的奇特和运用修辞手法的大胆、灵活。\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"学习这组诗作，要多借助朗读，理清诗歌的抒情脉络，把握意象的内涵，感悟诗人抒发的情思，体会象征手法的运用。\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"第一单元\",[\"$\",\"br\",null,{}],\"\\n11\"]}],\"\\n\"]}]\n8a:[\"$\",\"p\",null,{\"children\":\"请注意，图片中的文字可能需要进一步的排版和格式化，以便于阅读和理解。如果您需要帮助，请随时告诉我。\"}]\n8b:[\"$\",\"p\",null,{\"children\":\"Example: Text Information Recognition \u0026 Processing Next\"}]\n8c:[\"$\",\"p\",null,{\"children\":\"User\"}]\n8d:[\"$\",\"p\",null,{\"children\":\"A brief summary of the blog\"}]\n8e:[\"$\",\"p\",null,{\"children\":\"Qwen-VL\"}]\n8f:[\"$\",\"p\",null,{\"children\":\"The blog post announces several new updates and improvements to OpenAI’s platform, including:\"}]\n90:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":\"GPT-4 Turbo with a 128K context window, which is more capable and supports a 128K context window.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"New Assistants API that makes it easier for developers to build their own assistive AI apps and tools.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"New multimodal capabilities in the platform, including text-to-speech (TTS).\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Improved instruction following and JSON mode.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Reproducible outputs and log probabilities.\"}],\"\\n\",[\"$\",\"li\",null,{\"children\":\"Updated GPT-3.5 Turbo with a 16K context window by default.\"}],\"\\n\"]}]\n91:[\"$\",\"p\",null,{\"children\":\"The post also mentions that these updates will be rolled out to all paying customers starting at 10am PT today.\"}]\n92:[\"$\",\"h2\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"how-to-use\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#how-to-use\",\"className\":\"peer\",\"children\":\"How to Use\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-"])</script><script>self.__next_f.push([1,"opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n93:[\"$\",\"p\",null,{\"children\":\"Now you can access Qwen-VL-Plus and Qwen-VL-Max through the Huggingface Spaces, the Qwen website, and Dashscope APIs.\"}]\n94:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Try Qwen-VL-Plus (\",[\"$\",\"$Laa\",null,{\"href\":\"https://huggingface.co/spaces/Qwen/Qwen-VL-Plus\",\"children\":\"https://huggingface.co/spaces/Qwen/Qwen-VL-Plus\"}],\") and Qwen-VL-Max (\",[\"$\",\"$Laa\",null,{\"href\":\"https://huggingface.co/spaces/Qwen/Qwen-VL-Max\",\"children\":\"https://huggingface.co/spaces/Qwen/Qwen-VL-Max\"}],\") in the Huggingface Spaces\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Log in to the QianWen web portal at \",[\"$\",\"$Laa\",null,{\"href\":\"https://tongyi.aliyun.com/qianwen\",\"children\":\"https://tongyi.aliyun.com/qianwen\"}],\", and switch to “Image Understanding” mode to harness the latest Qwen-VL-Max capabilities.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Access the powerful APIs of Qwen-VL-Plus and Qwen-VL-Max through the Dashscope platform (\",[\"$\",\"$Laa\",null,{\"href\":\"https://help.aliyun.com/zh/dashscope/developer-reference/vl-plus-quick-start\",\"children\":\"https://help.aliyun.com/zh/dashscope/developer-reference/vl-plus-quick-start\"}],\").\"]}],\"\\n\"]}],\"\\n\"]}]\n95:[\"$\",\"h2\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"summary\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#summary\",\"className\":\"peer\",\"children\":\"Summary\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n96:[\"$\",\"p\",null,{\"children\":\"Qwen-VL-Plus and Qwen-VL-Max make significant strides in enhancing high-resolution recognition, text analysis, and image reasoning capabilities. These models now match the performance of GPT4-v and Gemini, outperforming all other open-source and proprietary models in many tasks, such as MMMU, CMMMU, and MathVista. They achieve world-class results in document analysis (DocVQA) and Chinese language-related image understanding (MM-Bench-CN).\"}]\n97:[\"$\",\"p\",null,{\"children\":\"Our objective is to continually tap into and elevate the potential of Qwen-VL, enabling it to make a difference across a broader range of applications. We can envision Qwen-VL as an assistant equipped with superhuman visual and linguistic comprehension skills that can provide robust support in everyday dialogues as well as complex scenarios like driving environments and programming contexts.\"}]\n98:[\"$\",\"p\",null,{\"children\":\"While there is still a long way to go, we are confident that Qwen-VL will evolve to perceive and understand the world akin to human cognition through continuous optimization and expansion!\"}]\n99:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Our Official Website: \",[\"$\",\"$Laa\",null,{\"href\":\"https://tongyi.aliyun.com/qianwen\",\"children\":\"https://tongyi.aliyun.com/qianwen\"}]]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Github: \",[\"$\",\"$Laa\",null,{\"href\":\"https://github.com/QwenLM/Qwen-VL\",\"children\":\"https://github.com/QwenLM/Qwen-VL\"}]]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Huggingface: \",[\"$\",\"$Laa\",null,{\"href\":\"http://huggingface.co/Qwen/Qwen-VL-Chat\",\"children\":\"http://huggingface.co/Qwen/Qwen-VL-Chat\"}]]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"ModelScope: \",[\"$\",\"$Laa\",null,{\"href\":\"https://modelscope.cn"])</script><script>self.__next_f.push([1,"/studios/qwen/Qwen-VL-Chat-Demo\",\"children\":\"https://modelscope.cn/studios/qwen/Qwen-VL-Chat-Demo\"}]]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"API: \",[\"$\",\"$Laa\",null,{\"href\":\"https://help.aliyun.com/zh/dashscope/developer-reference/tongyi-qianwen-vl-plus-api\",\"children\":\"https://help.aliyun.com/zh/dashscope/developer-reference/tongyi-qianwen-vl-plus-api\"}]]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"Discord: \",[\"$\",\"$Laa\",null,{\"href\":\"https://discord.gg/CV4E9rpNSD\",\"children\":\"https://discord.gg/CV4E9rpNSD\"}]]}],\"\\n\"]}]\n"])</script><script>self.__next_f.push([1,"9d:[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\" a \"}]\n9e:[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"+\"}]\n9f:[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\" b\"}]\na0:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    \"}]}]\na1:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#6A737D\",\"--shiki-dark\":\"#6A737D\"},\"children\":\"    # Ask the user for their input\"}]}]\n"])</script><script>self.__next_f.push([1,"a2:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    response \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#005CC5\",\"--shiki-dark\":\"#79B8FF\"},\"children\":\" int\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"(\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#005CC5\",\"--shiki-dark\":\"#79B8FF\"},\"children\":\"input\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"(\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"f\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\"\\\"What is \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#005CC5\",\"--shiki-dark\":\"#79B8FF\"},\"children\":\"{\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"a\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#005CC5\",\"--shiki-dark\":\"#79B8FF\"},\"children\":\"}\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\" + \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#005CC5\",\"--shiki-dark\":\"#79B8FF\"},\"children\":\"{\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"b\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#005CC5\",\"--shiki-dark\":\"#79B8FF\"},\"children\":\"}\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\"? \\\"\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"))\"}]]}]\n"])</script><script>self.__next_f.push([1,"a3:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    \"}]}]\na4:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#6A737D\",\"--shiki-dark\":\"#6A737D\"},\"children\":\"    # Check if the response matches the answer\"}]}]\na5:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"    if\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\" response \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"==\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\" answer:\"}]]}]\na6:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#005CC5\",\"--shiki-dark\":\"#79B8FF\"},\"children\":\"        print\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"(\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\"\\\"Correct!\\\"\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\")\"}]]}]\na7:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"    else\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\":\"}]]}]\na8:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#005CC5\",\"--shiki-dark\":\"#79B8FF\"},\"children\":\"        print\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"(\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\"\\\"Incorrect!\\\"\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\")\"}]]}]\na9:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    \"}]}]\n"])</script><script>self.__next_f.push([1,"b:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"2\",{\"name\":\"theme-color\",\"media\":\"(prefers-color-scheme: dark)\",\"content\":\"#0A0A0A\"}],[\"$\",\"meta\",\"3\",{\"name\":\"theme-color\",\"media\":\"(prefers-color-scheme: light)\",\"content\":\"#fff\"}]]\n"])</script><script>self.__next_f.push([1,"9:null\n"])</script><script>self.__next_f.push([1,"d:[[\"$\",\"title\",\"0\",{\"children\":\"Introducing Qwen-VL — Zen LM Blog | Zen LM\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Along with the rapid development of our large language model Qwen, we leveraged Qwen’s capabilities and unified multimodal pretraining to address the limitations of multimodal models in generalization, and we opensourced multimodal model Qwen-VL in Sep. 2023. Recently, the Qwen-VL series has undergone a significant upgrade with the launch of two enhanced versions, Qwen-VL-Plus and Qwen-VL-Max. The key technical advancements in these versions include:\"}],[\"$\",\"meta\",\"2\",{\"property\":\"og:title\",\"content\":\"Zen LM - Open Foundation Models\"}],[\"$\",\"meta\",\"3\",{\"property\":\"og:description\",\"content\":\"Zen AI model family by Hanzo AI. 14 frontier models from 4B to 1T+ parameters for code, reasoning, vision, and multimodal tasks.\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:url\",\"content\":\"https://zenlm.org\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:site_name\",\"content\":\"Zen LM\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"7\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"8\",{\"name\":\"twitter:site\",\"content\":\"@zenlmorg\"}],[\"$\",\"meta\",\"9\",{\"name\":\"twitter:creator\",\"content\":\"@zenlmorg\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:title\",\"content\":\"Zen LM - Open Foundation Models\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:description\",\"content\":\"Zen AI model family by Hanzo AI. 14 frontier models from 4B to 1T+ parameters for code, reasoning, vision, and multimodal tasks.\"}]]\n"])</script></body></html>