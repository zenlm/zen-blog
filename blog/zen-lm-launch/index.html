<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Introducing Zen LM: Open Frontier Models from Hanzo AI and Zoo Labs | Zen LM</title><meta name=keywords content="Announcement,Models,Zen"><meta name=description content="Announcing the Zen model family: 94+ open models built on Zen MoDE architecture, co-developed by Hanzo AI and Zoo Labs Foundation."><meta name=author content="Zen LM Team"><link rel=canonical href=https://zenlm.org/blog/zen-lm-launch/><link crossorigin=anonymous href=/assets/css/stylesheet.6c0865a4bc8dbcbd27d78777eb33b404568f7fbf3185cca7b978e5275a4dd049.css integrity="sha256-bAhlpLyNvL0n14d36zO0BFaPf78xhcynuXjlJ1pN0Ek=" rel="preload stylesheet" as=style><link rel=icon href=https://zenlm.org/favicon.png><link rel=apple-touch-icon href=https://zenlm.org/favicon.png><link rel=manifest href=https://zenlm.org/site.webmanifest><meta name=theme-color content="#615CED"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script><meta property="og:title" content="Introducing Zen LM: Open Frontier Models from Hanzo AI and Zoo Labs"><meta property="og:description" content="Announcing the Zen model family: 94+ open models built on Zen MoDE architecture, co-developed by Hanzo AI and Zoo Labs Foundation."><meta property="og:type" content="article"><meta property="og:url" content="https://zenlm.org/blog/zen-lm-launch/"><meta property="og:image" content="https://zenlm.org/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2026-01-15T09:00:00-08:00"><meta property="article:modified_time" content="2026-01-15T09:00:00-08:00"><meta property="og:site_name" content="Zen LM"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://zenlm.org/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Introducing Zen LM: Open Frontier Models from Hanzo AI and Zoo Labs"><meta name=twitter:description content="Announcing the Zen model family: 94+ open models built on Zen MoDE architecture, co-developed by Hanzo AI and Zoo Labs Foundation."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://zenlm.org/blog/"},{"@type":"ListItem","position":2,"name":"Introducing Zen LM: Open Frontier Models from Hanzo AI and Zoo Labs","item":"https://zenlm.org/blog/zen-lm-launch/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Introducing Zen LM: Open Frontier Models from Hanzo AI and Zoo Labs","name":"Introducing Zen LM: Open Frontier Models from Hanzo AI and Zoo Labs","description":"Announcing the Zen model family: 94+ open models built on Zen MoDE architecture, co-developed by Hanzo AI and Zoo Labs Foundation.","keywords":["Announcement","Models","Zen"],"articleBody":"GITHUB HUGGING FACE TRY ZEN CHAT\nToday we are announcing Zen LM — a family of open frontier models co-developed by Hanzo AI and Zoo Labs Foundation. This release marks the public launch of the Zen model catalog: 94+ models spanning text, vision, audio, and code, all built on our Zen MoDE (Mixture of Distilled Experts) architecture.\nWhy We Built It Modern AI infrastructure concentrates capability in a small number of proprietary systems. The models that power the most demanding production workloads are closed, expensive, and opaque. We believe frontier capability should be accessible — not as a service you pay per token, but as open weights you can run, inspect, fine-tune, and deploy on your own infrastructure.\nHanzo AI has spent years building AI infrastructure used by thousands of developers. Zoo Labs Foundation has led decentralized AI research through ZIPs (Zoo Improvement Proposals) and the PoAI (Proof of AI) protocol. Zen LM is the convergence of that work: production-grade models, open weights, governed by the community.\nThe Zen MoDE Architecture Zen MoDE stands for Mixture of Distilled Experts. Every Zen model is built on this architecture:\nExpert routing: Each token is processed by a small subset of specialized expert networks Distillation from large teachers: Experts are initialized from, and continuously refined against, larger teacher models Efficient inference: Only a fraction of total parameters activate per token, making large models economically viable The result: Zen models deliver capability competitive with much larger dense models at a fraction of the inference cost.\nFor a deep technical treatment see our architecture post: Zen MoDE Architecture.\nThe Catalog The Zen family covers every major use case:\nModel Parameters Use Case Zen4 Ultra 480B (35B active) Maximum capability, frontier tasks Zen Max 72B General enterprise use Zen4 Pro 32B (22B active) Balanced capability/cost Zen4 Flash 7B (3B active) Low-latency production Zen4 Coder 480B (35B active) Code generation, agentic coding Zen Omni 32B Vision + text + audio Zen VL 72B Image understanding, OCR Zen Nano 0.6B On-device inference Zen Embedding 7680-dim Semantic search, RAG Zen Guard 3B Safety classification All general-purpose models are released under Apache-2.0. Safety models use a more restrictive license to prevent adversarial use.\nHow We Train Zen models are trained on the Zoo Compute Network — a decentralized GPU cluster funded through the Zoo Labs Foundation treasury and governed by ZIPs. Training details:\nData: 15T+ tokens, high-quality filtered corpus with documented provenance Compute: Distributed across heterogeneous H100/A100 clusters Alignment: GRPO (Group Relative Policy Optimization) for instruction following Continuous improvement: ASO (Active Semantic Optimization, HIP-002) feeds production signals back into training Open Weights Commitment We release weights, not just APIs. Every Zen model is available on Hugging Face under hanzoai/. You can:\nDownload and run locally with transformers, vLLM, or llama.cpp Fine-tune on your own data Deploy on any infrastructure Inspect weights, tokenizer, and training configuration We believe open weights is the only credible commitment to openness. APIs can be taken down. Weights are permanent.\nWhat’s Next The Zen4 generation is live today. We are already training Zen5, with improvements to:\nLong-context reasoning (targeting 2M token context) Multimodal integration (tighter vision-text-audio coupling) Agentic reliability (reduced hallucination in tool-use chains) Distillation efficiency (more capability per active parameter) Follow along at zenlm.org, on GitHub, and on Hugging Face.\nZen LM is a joint initiative of Hanzo AI Inc. (Techstars ‘17) and Zoo Labs Foundation (501c3).\n","wordCount":"562","inLanguage":"en","datePublished":"2026-01-15T09:00:00-08:00","dateModified":"2026-01-15T09:00:00-08:00","author":{"@type":"Person","name":"Zen LM Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://zenlm.org/blog/zen-lm-launch/"},"publisher":{"@type":"Organization","name":"Zen LM","logo":{"@type":"ImageObject","url":"https://zenlm.org/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Zen LM (Alt + H)"><img src=https://zenlm.org/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://zeekay.blog title=zeekay><span>zeekay</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://hanzo.blog title=hanzo.blog><span>hanzo.blog</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://hanzo.ai/chat title="Try Zen Chat"><span>Try Zen Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://blog.zoo.ngo title="zoo blog"><span>zoo blog</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Introducing Zen LM: Open Frontier Models from Hanzo AI and Zoo Labs</h1><div class=post-description>Announcing the Zen model family: 94+ open models built on Zen MoDE architecture, co-developed by Hanzo AI and Zoo Labs Foundation.</div><div class=post-meta><span title='2026-01-15 09:00:00 -0800 -0800'>January 15, 2026</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;562 words&nbsp;·&nbsp;Zen LM Team</div></div></div><main class=main><article class=post-single><div class=post-content><p><a href=https://github.com/hanzoai class="btn external" target=_blank>GITHUB</a>
<a href=https://huggingface.co/hanzoai class="btn external" target=_blank>HUGGING FACE</a>
<a href=https://hanzo.ai/chat class="btn external" target=_blank>TRY ZEN CHAT</a></p><p>Today we are announcing <strong>Zen LM</strong> — a family of open frontier models co-developed by Hanzo AI and Zoo Labs Foundation. This release marks the public launch of the Zen model catalog: 94+ models spanning text, vision, audio, and code, all built on our <strong>Zen MoDE</strong> (Mixture of Distilled Experts) architecture.</p><h2 id=why-we-built-it>Why We Built It<a hidden class=anchor aria-hidden=true href=#why-we-built-it>#</a></h2><p>Modern AI infrastructure concentrates capability in a small number of proprietary systems. The models that power the most demanding production workloads are closed, expensive, and opaque. We believe frontier capability should be accessible — not as a service you pay per token, but as open weights you can run, inspect, fine-tune, and deploy on your own infrastructure.</p><p>Hanzo AI has spent years building AI infrastructure used by thousands of developers. Zoo Labs Foundation has led decentralized AI research through ZIPs (Zoo Improvement Proposals) and the PoAI (Proof of AI) protocol. Zen LM is the convergence of that work: production-grade models, open weights, governed by the community.</p><h2 id=the-zen-mode-architecture>The Zen MoDE Architecture<a hidden class=anchor aria-hidden=true href=#the-zen-mode-architecture>#</a></h2><p>Zen MoDE stands for <strong>Mixture of Distilled Experts</strong>. Every Zen model is built on this architecture:</p><ul><li><strong>Expert routing</strong>: Each token is processed by a small subset of specialized expert networks</li><li><strong>Distillation from large teachers</strong>: Experts are initialized from, and continuously refined against, larger teacher models</li><li><strong>Efficient inference</strong>: Only a fraction of total parameters activate per token, making large models economically viable</li></ul><p>The result: Zen models deliver capability competitive with much larger dense models at a fraction of the inference cost.</p><p>For a deep technical treatment see our architecture post: <a href=/blog/zen-mode-architecture/>Zen MoDE Architecture</a>.</p><h2 id=the-catalog>The Catalog<a hidden class=anchor aria-hidden=true href=#the-catalog>#</a></h2><p>The Zen family covers every major use case:</p><table><thead><tr><th>Model</th><th>Parameters</th><th>Use Case</th></tr></thead><tbody><tr><td>Zen4 Ultra</td><td>480B (35B active)</td><td>Maximum capability, frontier tasks</td></tr><tr><td>Zen Max</td><td>72B</td><td>General enterprise use</td></tr><tr><td>Zen4 Pro</td><td>32B (22B active)</td><td>Balanced capability/cost</td></tr><tr><td>Zen4 Flash</td><td>7B (3B active)</td><td>Low-latency production</td></tr><tr><td>Zen4 Coder</td><td>480B (35B active)</td><td>Code generation, agentic coding</td></tr><tr><td>Zen Omni</td><td>32B</td><td>Vision + text + audio</td></tr><tr><td>Zen VL</td><td>72B</td><td>Image understanding, OCR</td></tr><tr><td>Zen Nano</td><td>0.6B</td><td>On-device inference</td></tr><tr><td>Zen Embedding</td><td>7680-dim</td><td>Semantic search, RAG</td></tr><tr><td>Zen Guard</td><td>3B</td><td>Safety classification</td></tr></tbody></table><p>All general-purpose models are released under <strong>Apache-2.0</strong>. Safety models use a more restrictive license to prevent adversarial use.</p><h2 id=how-we-train>How We Train<a hidden class=anchor aria-hidden=true href=#how-we-train>#</a></h2><p>Zen models are trained on the Zoo Compute Network — a decentralized GPU cluster funded through the Zoo Labs Foundation treasury and governed by ZIPs. Training details:</p><ul><li><strong>Data</strong>: 15T+ tokens, high-quality filtered corpus with documented provenance</li><li><strong>Compute</strong>: Distributed across heterogeneous H100/A100 clusters</li><li><strong>Alignment</strong>: GRPO (Group Relative Policy Optimization) for instruction following</li><li><strong>Continuous improvement</strong>: ASO (Active Semantic Optimization, HIP-002) feeds production signals back into training</li></ul><h2 id=open-weights-commitment>Open Weights Commitment<a hidden class=anchor aria-hidden=true href=#open-weights-commitment>#</a></h2><p>We release weights, not just APIs. Every Zen model is available on Hugging Face under <code>hanzoai/</code>. You can:</p><ul><li>Download and run locally with <code>transformers</code>, <code>vLLM</code>, or <code>llama.cpp</code></li><li>Fine-tune on your own data</li><li>Deploy on any infrastructure</li><li>Inspect weights, tokenizer, and training configuration</li></ul><p>We believe open weights is the only credible commitment to openness. APIs can be taken down. Weights are permanent.</p><h2 id=whats-next>What&rsquo;s Next<a hidden class=anchor aria-hidden=true href=#whats-next>#</a></h2><p>The Zen4 generation is live today. We are already training Zen5, with improvements to:</p><ul><li>Long-context reasoning (targeting 2M token context)</li><li>Multimodal integration (tighter vision-text-audio coupling)</li><li>Agentic reliability (reduced hallucination in tool-use chains)</li><li>Distillation efficiency (more capability per active parameter)</li></ul><p>Follow along at <a href=https://zenlm.org>zenlm.org</a>, on <a href=https://github.com/hanzoai>GitHub</a>, and on <a href=https://huggingface.co/hanzoai>Hugging Face</a>.</p><hr><p><em>Zen LM is a joint initiative of Hanzo AI Inc. (Techstars &lsquo;17) and Zoo Labs Foundation (501c3).</em></p></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://zenlm.org/>Zen LM</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>