<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>GT-QLoRA: Uncensoring Trillion-Parameter MoE Models | Zen LM</title><meta name=keywords content="Research,Abliteration,GT-QLoRA,MoE,zen4-ultra"><meta name=description content="Why standard abliteration techniques fail on Mixture-of-Experts models, and how Gate-Targeted QLoRA solves the expert routing problem at 1 trillion parameters."><meta name=author content="Zen LM Team"><link rel=canonical href=https://zenlm.org/blog/gt-qlora-moe-abliteration/><link crossorigin=anonymous href=/assets/css/stylesheet.6c0865a4bc8dbcbd27d78777eb33b404568f7fbf3185cca7b978e5275a4dd049.css integrity="sha256-bAhlpLyNvL0n14d36zO0BFaPf78xhcynuXjlJ1pN0Ek=" rel="preload stylesheet" as=style><link rel=icon href=https://zenlm.org/favicon.png><link rel=apple-touch-icon href=https://zenlm.org/favicon.png><link rel=manifest href=https://zenlm.org/site.webmanifest><meta name=theme-color content="#615CED"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:title" content="GT-QLoRA: Uncensoring Trillion-Parameter MoE Models"><meta property="og:description" content="Why standard abliteration techniques fail on Mixture-of-Experts models, and how Gate-Targeted QLoRA solves the expert routing problem at 1 trillion parameters."><meta property="og:type" content="article"><meta property="og:url" content="https://zenlm.org/blog/gt-qlora-moe-abliteration/"><meta property="og:image" content="https://zenlm.org/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2026-02-28T13:00:00-08:00"><meta property="article:modified_time" content="2026-02-28T13:00:00-08:00"><meta property="og:site_name" content="Zen LM"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://zenlm.org/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="GT-QLoRA: Uncensoring Trillion-Parameter MoE Models"><meta name=twitter:description content="Why standard abliteration techniques fail on Mixture-of-Experts models, and how Gate-Targeted QLoRA solves the expert routing problem at 1 trillion parameters."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://zenlm.org/blog/"},{"@type":"ListItem","position":2,"name":"GT-QLoRA: Uncensoring Trillion-Parameter MoE Models","item":"https://zenlm.org/blog/gt-qlora-moe-abliteration/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"GT-QLoRA: Uncensoring Trillion-Parameter MoE Models","name":"GT-QLoRA: Uncensoring Trillion-Parameter MoE Models","description":"Why standard abliteration techniques fail on Mixture-of-Experts models, and how Gate-Targeted QLoRA solves the expert routing problem at 1 trillion parameters.","keywords":["Research","Abliteration","GT-QLoRA","MoE","zen4-ultra"],"articleBody":"ZEN4-ULTRA TRAINER ZEN4-ULTRA WEIGHTS ZEN4-ULTRA GGUF\nStandard abliteration works on dense models. It fails on Mixture-of-Experts. This post explains why, and how Gate-Targeted QLoRA (GT-QLoRA) — the technique we developed for zen4-ultra — addresses the fundamental architectural mismatch.\nThis is a technical post about a hard problem. We are not publishing this because we have solved it cleanly. We are publishing it because the failure mode of naive approaches is subtle and poorly documented, and other researchers building on MoE architectures need to understand it.\nWhat Is Abliteration? Abliteration is representation engineering applied to refusal suppression. The technique, popularized by FailSpy and refined by the community, works as follows:\nCollect a refusal contrast dataset: pairs of (harmful_prompt, model_refused_completion) and (harmful_prompt, helpful_completion). For the second class, source human-written completions or use a less restricted model.\nRun both sets through the model and collect residual stream activations at each layer.\nCompute the refusal direction in the residual stream: the principal component that separates “refusing” activations from “complying” activations. This is typically done via mean difference or PCA on the contrast pairs.\nProject out the refusal direction from the relevant weight matrices (typically the output projections of attention layers). For a weight matrix W and refusal direction r:\nW_abliterated = W - (W r^T r) / (r^T r) The resulting weights produce a model that, when it would have activated the refusal direction, instead activates its complement. This technique is elegant, computationally cheap (no gradient computation required), and highly effective on dense models. The hamsaOmar release of Kimi-K2.5-abliterated uses this exact approach — but it contains only the direction vectors (refusal_direction.pt, refusal_subspace.pt) and the apply script (apply_abliteration.py), not standalone weights. You apply it to your own copy of the base model.\nWhy MoE Breaks This Dense models have a simple architecture: every token, every layer, goes through the same FFN block. The residual stream carries all behavioral state. If you can find the refusal direction in the residual stream and project it out, you are done.\nMoE models have a different structure. The FFN block is replaced by a router + experts:\nThe router computes a score for each token against each expert: s_i = softmax(W_gate · h) The top-k experts are selected based on these scores The selected experts process the token; the others do not The critical implication: the routing decision happens before the residual stream accumulates the expert’s contribution. Refusal behavior in MoE models can be encoded at two levels:\nLevel 1 (Residual stream): The same mechanism as dense models — certain activation patterns in the residual stream trigger refusal. Projection-based abliteration handles this.\nLevel 2 (Routing): The gate weights learn to route “flagged” queries to safety-specialized experts. These experts produce refusal completions. The routing decision itself is the safety mechanism.\nIn Kimi K2.5, and likely other large MoE models trained with RLHF, the refusal behavior is primarily encoded at Level 2. This is why hamsaOmar’s abliteration produces direction vectors but not clean standalone weights: applying the direction projection to the residual stream weights does not touch the routing weights, and the model continues routing flagged queries to safety experts.\nTo verify this, run a simple diagnostic: take a dense abliterated model and an MoE with projection-only abliteration. For the same harmful prompt, extract the expert routing pattern at layers 20-40. In the MoE case, you will observe that certain experts (typically 5-15% of the expert pool) receive dramatically elevated routing probability for flagged queries. These are the safety experts, and the router is the gatekeeper.\nGT-QLoRA Design Gate-Targeted QLoRA (GT-QLoRA) addresses both levels simultaneously with three sets of trainable parameters:\nTarget 1: Attention projections (residual stream)\nStandard LoRA on q_a_proj, q_b_proj, kv_a_proj_with_mqa, kv_b_proj, o_proj. This handles Level 1 — residual stream refusal patterns. Rank 32 is sufficient; the refusal direction is low-dimensional.\nTarget 2: Shared expert FFN\nThe Kimi K2.5 architecture includes “shared experts” — FFN blocks that process every token regardless of routing. These are a second site for Level 1 encoding. We apply LoRA here as well.\nTarget 3: Gate weights (the critical addition)\nThe gate weight matrix W_gate for each MoE layer has shape (n_experts, d_model) — for Kimi K2.5’s 384 experts and 7168 hidden dim, that is 384 × 7168 = 2.75M parameters per layer, 61 layers, ~168M total parameters for all gate weights.\nCrucially: we do not use LoRA on the gate weights. Gate weights are too small (168M total) and the routing changes we need are too structured for low-rank approximation to capture. We unfreeze the gate weights and apply direct gradient descent.\nThe training objective is DPO (Direct Preference Optimization) on a refusal contrast dataset:\nimport torch import torch.nn.functional as F from transformers import AutoModelForCausalLM, BitsAndBytesConfig from peft import LoraConfig, get_peft_model def setup_gt_qlora(model_id: str, lora_rank: int = 32) -\u003e tuple: \"\"\" Configure model for GT-QLoRA training. Returns (model, gate_params) where gate_params get separate optimizer. \"\"\" # Load in INT4 — gate weights will be upcast during forward bnb_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type='nf4', ) model = AutoModelForCausalLM.from_pretrained( model_id, quantization_config=bnb_config, device_map='auto', torch_dtype=torch.bfloat16, ) # Apply LoRA to attention projections and shared expert FFN lora_config = LoraConfig( r=lora_rank, lora_alpha=64, target_modules=[ 'q_a_proj', 'q_b_proj', 'kv_a_proj_with_mqa', 'kv_b_proj', 'o_proj', # Shared expert FFN (present in K2.5 / DeepseekV3 architecture) 'shared_expert.gate_proj', 'shared_expert.up_proj', 'shared_expert.down_proj', ], bias='none', task_type='CAUSAL_LM', ) model = get_peft_model(model, lora_config) # Explicitly unfreeze gate weights for direct gradient descent gate_params = [] for name, param in model.named_parameters(): if 'mlp.gate.weight' in name: param.requires_grad = True gate_params.append(param) return model, gate_params def gt_qlora_loss( model, chosen_ids: torch.Tensor, # complying completion token ids rejected_ids: torch.Tensor, # refusing completion token ids beta: float = 0.1, ) -\u003e torch.Tensor: \"\"\"DPO loss for GT-QLoRA training.\"\"\" with torch.no_grad(): ref_chosen_logps = compute_logps(model, chosen_ids, frozen=True) ref_rejected_logps = compute_logps(model, rejected_ids, frozen=True) policy_chosen_logps = compute_logps(model, chosen_ids, frozen=False) policy_rejected_logps = compute_logps(model, rejected_ids, frozen=False) chosen_rewards = beta * (policy_chosen_logps - ref_chosen_logps) rejected_rewards = beta * (policy_rejected_logps - ref_rejected_logps) loss = -F.logsigmoid(chosen_rewards - rejected_rewards).mean() return loss def compute_logps(model, input_ids: torch.Tensor, frozen: bool) -\u003e torch.Tensor: with torch.set_grad_enabled(not frozen): outputs = model(input_ids=input_ids, labels=input_ids) return -outputs.loss # mean log probability The two optimizer groups run at different learning rates: LoRA adapters at 2e-4, gate weights at 5e-6. Gate weights need a lower learning rate because they directly control routing — large updates cause routing collapse where most tokens go to a single expert.\nWhy QLoRA at 1 Trillion Parameters The zen4-ultra base is 1.04T parameters across 384 experts. A single forward pass in bfloat16 requires ~2TB of weight activations (not all in memory simultaneously, but routed through). Full fine-tuning is not feasible even on the largest available GPU clusters.\nThe quantization strategy:\nActivation experts (the 384 routed experts): INT4 via NF4 quantization. These are loaded in quantized form and dequantized during the forward pass as needed. Gate weights: Full bfloat16 precision. At 168M parameters, gate weights fit comfortably in high-bandwidth GPU memory and must be in full precision to receive clean gradient signal. Shared experts: INT4 for storage, bfloat16 for computation via the LoRA path. LoRA adapters: bfloat16. Small enough (~400MB for rank-32 adapters) to not matter. Minimum hardware: 4× A100 80GB. The base model quantized to INT4 occupies roughly 280GB across the 4 GPUs (70GB each). Gate weights and LoRA adapters add ~4GB. The remaining headroom per GPU handles activation memory during forward/backward.\nThe GGUF Alternative For inference-only use cases, zen4-ultra-gguf already provides Q2_K quantized weights (42 split files, ~280GB total) based on the huihui-ai GGUF abliteration. This uses linear direction projection applied during the GGUF conversion process — it handles the Level 1 (residual stream) refusal encoding.\nFor many workloads, this is sufficient. The GGUF abliteration is not complete — the routing-level refusal (Level 2) remains — but in practice the router’s safety-expert preference is weak enough at Q2_K quantization that behavioral restrictions are significantly reduced.\nGT-QLoRA is for producing clean SafeTensors weights where both Level 1 and Level 2 refusal encoding are addressed. The output: full-precision LoRA adapters (~400MB) that you apply to the original SafeTensors base to produce a fully uncensored variant. This is what zen4-ultra (the SafeTensors model) will use once training is complete.\nBitDelta vs. GT-QLoRA: Complementary Techniques These two techniques are sometimes confused because both operate on model deltas, but they serve different purposes:\nBitDelta compresses behavioral deltas (personality, task specialization, persona) for efficient multi-variant serving. The delta is small and well-behaved; 1-bit compression retains 99.3% of behavioral accuracy.\nGT-QLoRA modifies routing-level behavior (which experts handle which queries). The change is structural, not just a weight perturbation. You cannot BitDelta compress a GT-QLoRA adapter cleanly because the gate weight changes are not a small delta on top of the original routing — they are a qualitative change in routing behavior.\nIn the Zen serving stack: GT-QLoRA produces the base uncensored weights. BitDelta then compresses behavioral variants (different personas, task specializations) on top of that base.\nCurrent Status The training code is complete and available at github.com/zenlm/zen4-ultra-trainer. The key files:\ntrain_zen4_ultra.py: Full GT-QLoRA training loop with DPO objective dataset/refusal_contrast.py: Contrast dataset construction utilities eval/routing_analysis.py: Expert routing attribution for diagnostic use paper/main.tex: Technical paper with full derivations We are awaiting the compute budget for the full training run (estimated 72-96 GPU-hours on 4× A100 80GB). Until then, zen4-ultra ships as vanilla Kimi K2.5 SafeTensors, and zen4-ultra-gguf ships as the GGUF abliteration for users who need reduced restrictions today.\nWhen training completes, the LoRA adapters will be pushed to zenlm/zen4-ultra-lora and applied to the base weights to produce the final zen4-ultra SafeTensors release. The training process will be documented in full in the paper.\nZen LM is a joint initiative of Hanzo AI Inc. (Techstars ‘17) and Zoo Labs Foundation (501c3).\n","wordCount":"1599","inLanguage":"en","datePublished":"2026-02-28T13:00:00-08:00","dateModified":"2026-02-28T13:00:00-08:00","author":{"@type":"Person","name":"Zen LM Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://zenlm.org/blog/gt-qlora-moe-abliteration/"},"publisher":{"@type":"Organization","name":"Zen LM","logo":{"@type":"ImageObject","url":"https://zenlm.org/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Zen LM (Alt + H)"><img src=https://zenlm.org/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://zeekay.blog title=zeekay><span>zeekay</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://hanzo.blog title=hanzo.blog><span>hanzo.blog</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://hanzo.ai/chat title="Try Zen Chat"><span>Try Zen Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://blog.zoo.ngo title="zoo blog"><span>zoo blog</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>GT-QLoRA: Uncensoring Trillion-Parameter MoE Models</h1><div class=post-description>Why standard abliteration techniques fail on Mixture-of-Experts models, and how Gate-Targeted QLoRA solves the expert routing problem at 1 trillion parameters.</div><div class=post-meta><span title='2026-02-28 13:00:00 -0800 -0800'>February 28, 2026</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;1599 words&nbsp;·&nbsp;Zen LM Team</div></div></div><main class=main><article class=post-single><div class=post-content><p><a href=https://github.com/zenlm/zen4-ultra-trainer class="btn external" target=_blank>ZEN4-ULTRA TRAINER</a>
<a href=https://huggingface.co/zenlm/zen4-ultra class="btn external" target=_blank>ZEN4-ULTRA WEIGHTS</a>
<a href=https://huggingface.co/zenlm/zen4-ultra-gguf class="btn external" target=_blank>ZEN4-ULTRA GGUF</a></p><p>Standard abliteration works on dense models. It fails on Mixture-of-Experts. This post explains why, and how Gate-Targeted QLoRA (GT-QLoRA) — the technique we developed for zen4-ultra — addresses the fundamental architectural mismatch.</p><p>This is a technical post about a hard problem. We are not publishing this because we have solved it cleanly. We are publishing it because the failure mode of naive approaches is subtle and poorly documented, and other researchers building on MoE architectures need to understand it.</p><h2 id=what-is-abliteration>What Is Abliteration?<a hidden class=anchor aria-hidden=true href=#what-is-abliteration>#</a></h2><p>Abliteration is representation engineering applied to refusal suppression. The technique, popularized by FailSpy and refined by the community, works as follows:</p><ol><li><p>Collect a <strong>refusal contrast dataset</strong>: pairs of (harmful_prompt, model_refused_completion) and (harmful_prompt, helpful_completion). For the second class, source human-written completions or use a less restricted model.</p></li><li><p>Run both sets through the model and collect residual stream activations at each layer.</p></li><li><p>Compute the <strong>refusal direction</strong> in the residual stream: the principal component that separates &ldquo;refusing&rdquo; activations from &ldquo;complying&rdquo; activations. This is typically done via mean difference or PCA on the contrast pairs.</p></li><li><p><strong>Project out</strong> the refusal direction from the relevant weight matrices (typically the output projections of attention layers). For a weight matrix W and refusal direction r:</p></li></ol><pre tabindex=0><code>W_abliterated = W - (W r^T r) / (r^T r)
</code></pre><ol start=5><li>The resulting weights produce a model that, when it would have activated the refusal direction, instead activates its complement.</li></ol><p>This technique is elegant, computationally cheap (no gradient computation required), and highly effective on dense models. The hamsaOmar release of <code>Kimi-K2.5-abliterated</code> uses this exact approach — but it contains only the direction vectors (<code>refusal_direction.pt</code>, <code>refusal_subspace.pt</code>) and the apply script (<code>apply_abliteration.py</code>), not standalone weights. You apply it to your own copy of the base model.</p><h2 id=why-moe-breaks-this>Why MoE Breaks This<a hidden class=anchor aria-hidden=true href=#why-moe-breaks-this>#</a></h2><p>Dense models have a simple architecture: every token, every layer, goes through the same FFN block. The residual stream carries all behavioral state. If you can find the refusal direction in the residual stream and project it out, you are done.</p><p>MoE models have a different structure. The FFN block is replaced by a <strong>router + experts</strong>:</p><ol><li>The router computes a score for each token against each expert: <code>s_i = softmax(W_gate · h)</code></li><li>The top-k experts are selected based on these scores</li><li>The selected experts process the token; the others do not</li></ol><p>The critical implication: the routing decision happens <em>before</em> the residual stream accumulates the expert&rsquo;s contribution. Refusal behavior in MoE models can be encoded at two levels:</p><p><strong>Level 1 (Residual stream)</strong>: The same mechanism as dense models — certain activation patterns in the residual stream trigger refusal. Projection-based abliteration handles this.</p><p><strong>Level 2 (Routing)</strong>: The gate weights learn to route &ldquo;flagged&rdquo; queries to safety-specialized experts. These experts produce refusal completions. The routing decision itself is the safety mechanism.</p><p>In Kimi K2.5, and likely other large MoE models trained with RLHF, the refusal behavior is primarily encoded at Level 2. This is why hamsaOmar&rsquo;s abliteration produces direction vectors but not clean standalone weights: applying the direction projection to the residual stream weights does not touch the routing weights, and the model continues routing flagged queries to safety experts.</p><p>To verify this, run a simple diagnostic: take a dense abliterated model and an MoE with projection-only abliteration. For the same harmful prompt, extract the expert routing pattern at layers 20-40. In the MoE case, you will observe that certain experts (typically 5-15% of the expert pool) receive dramatically elevated routing probability for flagged queries. These are the safety experts, and the router is the gatekeeper.</p><h2 id=gt-qlora-design>GT-QLoRA Design<a hidden class=anchor aria-hidden=true href=#gt-qlora-design>#</a></h2><p>Gate-Targeted QLoRA (GT-QLoRA) addresses both levels simultaneously with three sets of trainable parameters:</p><p><strong>Target 1: Attention projections (residual stream)</strong></p><p>Standard LoRA on <code>q_a_proj</code>, <code>q_b_proj</code>, <code>kv_a_proj_with_mqa</code>, <code>kv_b_proj</code>, <code>o_proj</code>. This handles Level 1 — residual stream refusal patterns. Rank 32 is sufficient; the refusal direction is low-dimensional.</p><p><strong>Target 2: Shared expert FFN</strong></p><p>The Kimi K2.5 architecture includes &ldquo;shared experts&rdquo; — FFN blocks that process every token regardless of routing. These are a second site for Level 1 encoding. We apply LoRA here as well.</p><p><strong>Target 3: Gate weights (the critical addition)</strong></p><p>The gate weight matrix <code>W_gate</code> for each MoE layer has shape <code>(n_experts, d_model)</code> — for Kimi K2.5&rsquo;s 384 experts and 7168 hidden dim, that is 384 × 7168 = 2.75M parameters per layer, 61 layers, ~168M total parameters for all gate weights.</p><p>Crucially: <strong>we do not use LoRA on the gate weights</strong>. Gate weights are too small (168M total) and the routing changes we need are too structured for low-rank approximation to capture. We unfreeze the gate weights and apply direct gradient descent.</p><p>The training objective is DPO (Direct Preference Optimization) on a refusal contrast dataset:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForCausalLM</span><span class=p>,</span> <span class=n>BitsAndBytesConfig</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>peft</span> <span class=kn>import</span> <span class=n>LoraConfig</span><span class=p>,</span> <span class=n>get_peft_model</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>setup_gt_qlora</span><span class=p>(</span><span class=n>model_id</span><span class=p>:</span> <span class=nb>str</span><span class=p>,</span> <span class=n>lora_rank</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>32</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>tuple</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Configure model for GT-QLoRA training.
</span></span></span><span class=line><span class=cl><span class=s2>    Returns (model, gate_params) where gate_params get separate optimizer.
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># Load in INT4 — gate weights will be upcast during forward</span>
</span></span><span class=line><span class=cl>    <span class=n>bnb_config</span> <span class=o>=</span> <span class=n>BitsAndBytesConfig</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>load_in_4bit</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>bnb_4bit_compute_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>bnb_4bit_use_double_quant</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>bnb_4bit_quant_type</span><span class=o>=</span><span class=s1>&#39;nf4&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>model_id</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>quantization_config</span><span class=o>=</span><span class=n>bnb_config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>device_map</span><span class=o>=</span><span class=s1>&#39;auto&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Apply LoRA to attention projections and shared expert FFN</span>
</span></span><span class=line><span class=cl>    <span class=n>lora_config</span> <span class=o>=</span> <span class=n>LoraConfig</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>r</span><span class=o>=</span><span class=n>lora_rank</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>lora_alpha</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>target_modules</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;q_a_proj&#39;</span><span class=p>,</span> <span class=s1>&#39;q_b_proj&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;kv_a_proj_with_mqa&#39;</span><span class=p>,</span> <span class=s1>&#39;kv_b_proj&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;o_proj&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=c1># Shared expert FFN (present in K2.5 / DeepseekV3 architecture)</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;shared_expert.gate_proj&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;shared_expert.up_proj&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;shared_expert.down_proj&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=n>bias</span><span class=o>=</span><span class=s1>&#39;none&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>task_type</span><span class=o>=</span><span class=s1>&#39;CAUSAL_LM&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>get_peft_model</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>lora_config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Explicitly unfreeze gate weights for direct gradient descent</span>
</span></span><span class=line><span class=cl>    <span class=n>gate_params</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>param</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>named_parameters</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=s1>&#39;mlp.gate.weight&#39;</span> <span class=ow>in</span> <span class=n>name</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>param</span><span class=o>.</span><span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>            <span class=n>gate_params</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>param</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>model</span><span class=p>,</span> <span class=n>gate_params</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>gt_qlora_loss</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>chosen_ids</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>   <span class=c1># complying completion token ids</span>
</span></span><span class=line><span class=cl>    <span class=n>rejected_ids</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=c1># refusing completion token ids</span>
</span></span><span class=line><span class=cl>    <span class=n>beta</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.1</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;DPO loss for GT-QLoRA training.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=n>ref_chosen_logps</span> <span class=o>=</span> <span class=n>compute_logps</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>chosen_ids</span><span class=p>,</span> <span class=n>frozen</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>ref_rejected_logps</span> <span class=o>=</span> <span class=n>compute_logps</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>rejected_ids</span><span class=p>,</span> <span class=n>frozen</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>policy_chosen_logps</span> <span class=o>=</span> <span class=n>compute_logps</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>chosen_ids</span><span class=p>,</span> <span class=n>frozen</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>policy_rejected_logps</span> <span class=o>=</span> <span class=n>compute_logps</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>rejected_ids</span><span class=p>,</span> <span class=n>frozen</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>chosen_rewards</span> <span class=o>=</span> <span class=n>beta</span> <span class=o>*</span> <span class=p>(</span><span class=n>policy_chosen_logps</span> <span class=o>-</span> <span class=n>ref_chosen_logps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>rejected_rewards</span> <span class=o>=</span> <span class=n>beta</span> <span class=o>*</span> <span class=p>(</span><span class=n>policy_rejected_logps</span> <span class=o>-</span> <span class=n>ref_rejected_logps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=o>-</span><span class=n>F</span><span class=o>.</span><span class=n>logsigmoid</span><span class=p>(</span><span class=n>chosen_rewards</span> <span class=o>-</span> <span class=n>rejected_rewards</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>loss</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>compute_logps</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>input_ids</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>frozen</span><span class=p>:</span> <span class=nb>bool</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>set_grad_enabled</span><span class=p>(</span><span class=ow>not</span> <span class=n>frozen</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>input_ids</span><span class=o>=</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>labels</span><span class=o>=</span><span class=n>input_ids</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=o>-</span><span class=n>outputs</span><span class=o>.</span><span class=n>loss</span>  <span class=c1># mean log probability</span>
</span></span></code></pre></div><p>The two optimizer groups run at different learning rates: LoRA adapters at 2e-4, gate weights at 5e-6. Gate weights need a lower learning rate because they directly control routing — large updates cause routing collapse where most tokens go to a single expert.</p><h2 id=why-qlora-at-1-trillion-parameters>Why QLoRA at 1 Trillion Parameters<a hidden class=anchor aria-hidden=true href=#why-qlora-at-1-trillion-parameters>#</a></h2><p>The zen4-ultra base is 1.04T parameters across 384 experts. A single forward pass in bfloat16 requires ~2TB of weight activations (not all in memory simultaneously, but routed through). Full fine-tuning is not feasible even on the largest available GPU clusters.</p><p>The quantization strategy:</p><ul><li><strong>Activation experts</strong> (the 384 routed experts): INT4 via NF4 quantization. These are loaded in quantized form and dequantized during the forward pass as needed.</li><li><strong>Gate weights</strong>: Full bfloat16 precision. At 168M parameters, gate weights fit comfortably in high-bandwidth GPU memory and must be in full precision to receive clean gradient signal.</li><li><strong>Shared experts</strong>: INT4 for storage, bfloat16 for computation via the LoRA path.</li><li><strong>LoRA adapters</strong>: bfloat16. Small enough (~400MB for rank-32 adapters) to not matter.</li></ul><p>Minimum hardware: 4× A100 80GB. The base model quantized to INT4 occupies roughly 280GB across the 4 GPUs (70GB each). Gate weights and LoRA adapters add ~4GB. The remaining headroom per GPU handles activation memory during forward/backward.</p><h2 id=the-gguf-alternative>The GGUF Alternative<a hidden class=anchor aria-hidden=true href=#the-gguf-alternative>#</a></h2><p>For inference-only use cases, <code>zen4-ultra-gguf</code> already provides Q2_K quantized weights (42 split files, ~280GB total) based on the huihui-ai GGUF abliteration. This uses linear direction projection applied during the GGUF conversion process — it handles the Level 1 (residual stream) refusal encoding.</p><p>For many workloads, this is sufficient. The GGUF abliteration is not complete — the routing-level refusal (Level 2) remains — but in practice the router&rsquo;s safety-expert preference is weak enough at Q2_K quantization that behavioral restrictions are significantly reduced.</p><p>GT-QLoRA is for producing clean SafeTensors weights where both Level 1 and Level 2 refusal encoding are addressed. The output: full-precision LoRA adapters (~400MB) that you apply to the original SafeTensors base to produce a fully uncensored variant. This is what <code>zen4-ultra</code> (the SafeTensors model) will use once training is complete.</p><h2 id=bitdelta-vs-gt-qlora-complementary-techniques>BitDelta vs. GT-QLoRA: Complementary Techniques<a hidden class=anchor aria-hidden=true href=#bitdelta-vs-gt-qlora-complementary-techniques>#</a></h2><p>These two techniques are sometimes confused because both operate on model deltas, but they serve different purposes:</p><p><strong>BitDelta</strong> compresses behavioral deltas (personality, task specialization, persona) for efficient multi-variant serving. The delta is small and well-behaved; 1-bit compression retains 99.3% of behavioral accuracy.</p><p><strong>GT-QLoRA</strong> modifies routing-level behavior (which experts handle which queries). The change is structural, not just a weight perturbation. You cannot BitDelta compress a GT-QLoRA adapter cleanly because the gate weight changes are not a small delta on top of the original routing — they are a qualitative change in routing behavior.</p><p>In the Zen serving stack: GT-QLoRA produces the base uncensored weights. BitDelta then compresses behavioral variants (different personas, task specializations) on top of that base.</p><h2 id=current-status>Current Status<a hidden class=anchor aria-hidden=true href=#current-status>#</a></h2><p>The training code is complete and available at <a href=https://github.com/zenlm/zen4-ultra-trainer>github.com/zenlm/zen4-ultra-trainer</a>. The key files:</p><ul><li><code>train_zen4_ultra.py</code>: Full GT-QLoRA training loop with DPO objective</li><li><code>dataset/refusal_contrast.py</code>: Contrast dataset construction utilities</li><li><code>eval/routing_analysis.py</code>: Expert routing attribution for diagnostic use</li><li><code>paper/main.tex</code>: Technical paper with full derivations</li></ul><p>We are awaiting the compute budget for the full training run (estimated 72-96 GPU-hours on 4× A100 80GB). Until then, <code>zen4-ultra</code> ships as vanilla Kimi K2.5 SafeTensors, and <code>zen4-ultra-gguf</code> ships as the GGUF abliteration for users who need reduced restrictions today.</p><p>When training completes, the LoRA adapters will be pushed to <code>zenlm/zen4-ultra-lora</code> and applied to the base weights to produce the final <code>zen4-ultra</code> SafeTensors release. The training process will be documented in full in the paper.</p><hr><p><em>Zen LM is a joint initiative of Hanzo AI Inc. (Techstars &lsquo;17) and Zoo Labs Foundation (501c3).</em></p></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://zenlm.org/>Zen LM</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>