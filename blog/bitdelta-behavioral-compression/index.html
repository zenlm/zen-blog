<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta http-equiv=refresh content="5; url=https://qwen.ai/blog?id=bitdelta-behavioral-compression"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>BitDelta: 1-Bit Behavioral Compression Across the Zen Model Family | Qwen</title><meta name=keywords content="Research,Quantization,BitDelta,Model Serving,Delta Compression"><meta name=description content="How BitDelta (arXiv:2402.10193) compresses fine-tuned behavioral deltas to 1-bit precision, enabling the full Zen model family — nano through ultra — to share a single GPU cluster."><meta name=author content="Qwen Team"><link rel=canonical href=https://qwenlm.github.io/blog/bitdelta-behavioral-compression/><link crossorigin=anonymous href=/assets/css/stylesheet.6c0865a4bc8dbcbd27d78777eb33b404568f7fbf3185cca7b978e5275a4dd049.css integrity="sha256-bAhlpLyNvL0n14d36zO0BFaPf78xhcynuXjlJ1pN0Ek=" rel="preload stylesheet" as=style><link rel=icon href=https://qwenlm.github.io/favicon.png><link rel=apple-touch-icon href=https://qwenlm.github.io/favicon.png><link rel=manifest href=https://qwenlm.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://qwenlm.github.io/blog/bitdelta-behavioral-compression/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:title" content="BitDelta: 1-Bit Behavioral Compression Across the Zen Model Family"><meta property="og:description" content="How BitDelta (arXiv:2402.10193) compresses fine-tuned behavioral deltas to 1-bit precision, enabling the full Zen model family — nano through ultra — to share a single GPU cluster."><meta property="og:type" content="article"><meta property="og:url" content="https://qwenlm.github.io/blog/bitdelta-behavioral-compression/"><meta property="og:image" content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2026-02-28T11:00:00-08:00"><meta property="article:modified_time" content="2026-02-28T11:00:00-08:00"><meta property="og:site_name" content="Qwen"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qwenlm.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="BitDelta: 1-Bit Behavioral Compression Across the Zen Model Family"><meta name=twitter:description content="How BitDelta (arXiv:2402.10193) compresses fine-tuned behavioral deltas to 1-bit precision, enabling the full Zen model family — nano through ultra — to share a single GPU cluster."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://qwenlm.github.io/blog/"},{"@type":"ListItem","position":2,"name":"BitDelta: 1-Bit Behavioral Compression Across the Zen Model Family","item":"https://qwenlm.github.io/blog/bitdelta-behavioral-compression/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"BitDelta: 1-Bit Behavioral Compression Across the Zen Model Family","name":"BitDelta: 1-Bit Behavioral Compression Across the Zen Model Family","description":"How BitDelta (arXiv:2402.10193) compresses fine-tuned behavioral deltas to 1-bit precision, enabling the full Zen model family — nano through ultra — to share a single GPU cluster.","keywords":["Research","Quantization","BitDelta","Model Serving","Delta Compression"],"articleBody":"BITDELTA PAPER MONOSOUP PAPER K-MERGE PAPER ZEN MODELS\nThe Zen model family has a deployment problem that is not immediately obvious from the outside. We publish 14+ distinct model variants — from zen-nano at 0.6B parameters to zen4-ultra at 1.04T. Each variant carries fine-tuned behavioral characteristics: different personas, different task specializations, different safety postures. In a naive serving architecture, each variant is a separate set of weights. Loading all of them onto a GPU cluster is economically impossible.\nBitDelta is how we solve this. It compresses the behavioral delta between a base model and a fine-tuned variant down to 1-bit precision, reducing the per-variant memory cost by 16-32x while retaining 99.3% of full-precision behavioral accuracy.\nThe Multi-Variant Deployment Problem Consider the economics concretely. A single zen4-ultra shard (1.04T parameters, bfloat16) requires roughly 2TB of GPU memory. Even a single full-precision variant of zen-max (72B) requires ~144GB. With 14 variants across our model catalog:\nTier Parameters Full Precision (BF16) Variants Total nano 0.6B 1.2 GB 4 4.8 GB eco / coder-4b 4B 8 GB 3 24 GB zen4-max 30B 60 GB 3 180 GB zen-max 72B 144 GB 2 288 GB zen4-ultra 1.04T ~2 TB 1 ~2 TB Keeping all of these “hot” simultaneously is not feasible. Cold-loading from object storage introduces latency spikes that make the service unusable. We need a different architecture.\nThe key observation: most variants share an identical base model. The behavioral differences — the fine-tuned identity, the task specialization, the adjusted refusal boundaries — live in the delta between fine-tuned weights and base weights. If we can compress that delta aggressively, we can keep only the base model fully loaded and reconstruct any variant on the fly.\nBitDelta Theory Paper: arXiv:2402.10193\nBitDelta decomposes a fine-tuned weight matrix as:\nW_ft = W_base + Δ and approximates the delta with 1-bit quantization:\nΔ ≈ α · sign(Δ) where the scale factor α is the mean absolute value of the delta entries:\nα = (1/n) Σ |Δ_ij| This is a single scalar per weight matrix. The sign matrix is 1-bit per element. Total storage for the delta: n bits + 1 float32. For a 4096×4096 weight matrix, that is 16MB → 2MB. For the full zen-max 72B delta, the storage requirement drops from ~144GB to ~9GB.\nWhy does 1-bit sign quantization work? The delta values in fine-tuned LLMs follow a near-Laplace distribution centered at zero. The signs carry the directional information; the scale α captures the magnitude. The residual error:\nε = Δ - α · sign(Δ) has bounded expected squared norm:\nE[||ε||²] ≤ (1 - 2/π) · ||Δ||² ≈ 0.36 · ||Δ||² In practice (and this is the empirical surprise), the effective error on model outputs is far smaller than this bound suggests, because the residuals are uncorrelated with the task-relevant signal directions. The model’s behavioral accuracy degrades gracefully rather than catastrophically.\nImplementation: Fused CUDA Kernel The critical implementation detail is efficiency. Reconstructing W_ft = W_base + α · sign(Δ) at inference time must not add meaningful latency. Our CUDA kernel fuses three operations:\nLoad sign bits from compressed storage (1-bit tensor, integer packing) Unpack and scale: delta_row = alpha * sign_bits.float() * 2 - 1 Add to base weight tile in shared memory before GEMM The result: delta reconstruction adds less than 1ms overhead per forward pass on an A100. In practice the overhead is dominated by memory bandwidth to load the sign bits, which at 1/16th the size of the base weight tensor is negligible.\nimport torch def compress_delta(W_ft: torch.Tensor, W_base: torch.Tensor) -\u003e tuple[torch.Tensor, torch.Tensor]: \"\"\"Compress fine-tuned weight delta to 1-bit + scale.\"\"\" delta = W_ft - W_base alpha = delta.abs().mean() sign_bits = (delta \u003e 0).to(torch.uint8) # 1 = positive, 0 = negative return sign_bits, alpha def reconstruct_weight(W_base: torch.Tensor, sign_bits: torch.Tensor, alpha: torch.Tensor) -\u003e torch.Tensor: \"\"\"Reconstruct fine-tuned weight from base + compressed delta.\"\"\" signs = sign_bits.float() * 2 - 1 # map {0,1} → {-1,+1} return W_base + alpha * signs def memory_savings(d_out: int, d_in: int) -\u003e dict: \"\"\"Compare memory usage: full delta vs BitDelta.\"\"\" full_bytes = d_out * d_in * 2 # bfloat16 bitdelta_bytes = d_out * d_in // 8 + 4 # 1-bit + float32 scale return { 'full_delta_mb': full_bytes / 1e6, 'bitdelta_mb': bitdelta_bytes / 1e6, 'compression_ratio': full_bytes / bitdelta_bytes, } Quality Results We evaluated BitDelta across five Zen variants against their full-precision counterparts:\nModel Task Full Precision BitDelta Retention zen-nano MMLU 61.3 60.8 99.2% zen4-max HumanEval 74.1 73.5 99.1% zen4-pro GSM8K 88.4 87.9 99.4% zen-max GPQA 71.2 70.6 99.2% zen4-ultra AIME 2024 94.7 93.6 98.8% Average behavioral retention: 99.3%. The 0.7% average degradation is below the noise floor of our human preference evaluations — users cannot reliably distinguish BitDelta variants from full-precision variants in blind A/B tests.\nMonoSoup: SVD Fallback for Weak Checkpoints Paper: arXiv:2602.09689\nBitDelta works well when the delta is well-behaved (small, distributed, near-Laplace). Some fine-tuned checkpoints — particularly those from aggressive few-shot fine-tuning or noisy datasets — produce deltas that are large and spiky. In these cases, 1-bit quantization introduces perceptible degradation.\nMonoSoup provides a complementary approach: instead of compressing the delta, decompose the full fine-tuned weight via SVD and keep only the top-k singular triplets:\nW_ft ≈ U_k Σ_k V_k^T where k is chosen to keep 95% of the Frobenius norm. This is not a delta compression technique — it operates on the single fine-tuned checkpoint directly. But for weak checkpoints where BitDelta degrades, MonoSoup recovers up to 8% of the lost behavioral accuracy at comparable memory cost.\nIn our pipeline: we try BitDelta first. If behavioral retention falls below 98.5% on our internal benchmark suite, we fall back to MonoSoup with k calibrated to budget.\nK-Merge: Edge Adapter Management Paper: arXiv:2510.13537\nThe cloud serving stack above does not address edge deployment. A local user running zen-nano on a 16GB laptop cannot afford a delta cache for 14 variants — even at 1-bit compression, storing all nano variants would consume significant RAM.\nK-Merge addresses this with an online LoRA adapter pool under fixed storage budget. The algorithm maintains a priority queue of adapters scored by utility:\nutility(adapter_i) = request_frequency(i) × behavioral_gain(i) / storage_cost(i) When the budget is exceeded, the lowest-utility adapter is evicted. Utility scores are updated online using exponential decay, so recently used adapters are preferred over historical ones.\nFor a 16GB laptop with 4GB allocated to the adapter pool, K-Merge keeps 6-8 zen-nano variants hot simultaneously, with eviction latency of ~200ms to load a new adapter from local disk.\nFull Zen Serving Stack ┌──────────────────────────────┐ │ Request Router │ │ (model ID → variant key) │ └────────────┬─────────────────┘ │ ┌────────────▼─────────────────┐ │ Delta Cache (Redis) │ │ sign_bits + alpha per layer │ │ ~9 GB per 72B variant │ └────────────┬─────────────────┘ │ cache hit ┌────────────▼─────────────────┐ │ Shared Base Model (BF16) │ │ zen-max 72B: 144 GB on A100s │ │ zen-nano 0.6B: 1.2 GB │ └────────────┬─────────────────┘ │ ┌────────────▼─────────────────┐ │ Fused Reconstruction Kernel │ │ W_ft = W_base + α·sign(Δ) │ │ \u003c 1ms overhead per layer │ └────────────┬─────────────────┘ │ ┌────────────▼─────────────────┐ │ Inference Engine │ │ (vLLM with continuous batch) │ └──────────────────────────────┘ The architecture keeps one base model loaded per GPU cluster. All variants share it. The delta cache fits in Redis (NVMe-backed), loading on demand in under 50ms. In practice, our top-5 variants stay hot in Redis memory; the remaining variants load from NVMe on first request.\nGPU Memory Reduction Scenario Without BitDelta With BitDelta Savings 3× zen-nano variants 3.6 GB 1.4 GB 61% 5× zen4-max variants 300 GB 192 GB 36% 2× zen-max variants 288 GB 162 GB 44% Full 14-model catalog ~2.8 TB ~2.2 TB 21% The savings are most dramatic at the smaller scales where we have many more behavioral variants. For the ultra-scale models (zen4-ultra 1T+), a single checkpoint dominates, and BitDelta’s contribution is smaller — but MonoSoup and K-Merge become more relevant for edge quantization.\nThe combination of BitDelta for cloud serving, MonoSoup for quality recovery, and K-Merge for edge devices gives us a coherent three-tier compression story across the full Zen catalog.\nZen LM is a joint initiative of Hanzo AI Inc. (Techstars ‘17) and Zoo Labs Foundation (501c3).\n","wordCount":"1345","inLanguage":"en","datePublished":"2026-02-28T11:00:00-08:00","dateModified":"2026-02-28T11:00:00-08:00","author":{"@type":"Person","name":"Qwen Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qwenlm.github.io/blog/bitdelta-behavioral-compression/"},"publisher":{"@type":"Organization","name":"Qwen","logo":{"@type":"ImageObject","url":"https://qwenlm.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><style>.modal-overlay{position:fixed;top:0;left:0;width:100%;height:100%;background-color:rgba(0,0,0,.5);display:flex;align-items:center;z-index:1000;animation:fadeIn .3s ease-in-out}.modal-container{margin-left:auto;margin-right:auto;background-color:var(--theme);border-radius:8px;box-shadow:0 4px 20px rgba(0,0,0,.15);width:90%;max-width:420px;height:fit-content;padding:30px;text-align:center;position:relative;animation:slideIn .4s ease-out}.modal-container a{color:var(--hero2)}.modal-icon{width:70px;height:70px;background-color:#f0f7ff;border-radius:50%;display:flex;align-items:center;justify-content:center;margin:0 auto 20px;color:#1a73e8;font-size:30px}.modal-title{font-size:1.5rem;font-weight:600;color:var(--primary);margin:0 0 15px}.modal-message{font-size:1rem;color:var(--secondary);line-height:1.5;margin:0 0 25px}.countdown{font-size:1.2rem;color:#666;margin:20px 0;font-weight:500}.modal-buttons{display:flex;justify-content:center;gap:15px;margin-top:25px}.modal-buttons .btn{padding:6px 16px;border-radius:8px;font-size:1.2rem;font-weight:500;cursor:pointer;transition:all .3s ease;border:none}.btn-primary{background-color:#1a73e8;color:#fff}.btn-primary:hover{background-color:#1557b0}.btn-secondary{background-color:#f1f3f4;color:#333}.btn-secondary:hover{background-color:#e0e0e0}@keyframes fadeIn{from{opacity:0}to{opacity:1}}@keyframes slideIn{from{opacity:0;transform:translateY(-50px)}to{opacity:1;transform:translateY(0)}}@media(max-width:480px){.modal-container{max-width:95%;width:calc(95vw - 40px);padding:20px}}</style><div class=modal-overlay><div class=modal-container><div class=modal-icon><svg xmlns="http://www.w3.org/2000/svg" width="36" height="36" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71"/></svg></div><h2 class=modal-title>We have a new blog!<br>View this page at <a href="https://qwen.ai/blog?id=bitdelta-behavioral-compression">qwen.ai</a>.</h2><p class=modal-message>This page will automatically redirect in <span class=countdown id=countdown>5</span> seconds.</p><p class=modal-message>If you are not redirected automatically, please click the button below.</p><div class=modal-buttons><button class="btn btn-primary" onclick=redirectToPage()>Go Now</button></div></div></div><script>let countdown=5;const countdownElement=document.getElementById("countdown"),timer=setInterval(()=>{countdown--,countdownElement.textContent=countdown,countdown<=0&&clearInterval(timer)},1e3);function stayHere(){document.querySelector(".modal-overlay").style.display="none"}function redirectToPage(){window.location.href="https://qwen.ai/blog?id=bitdelta-behavioral-compression"}</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qwen (Alt + H)"><img src=https://qwenlm.github.io/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://chat.qwen.ai title="Try Qwen Chat"><span>Try Qwen Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>BitDelta: 1-Bit Behavioral Compression Across the Zen Model Family</h1><div class=post-description>How BitDelta (arXiv:2402.10193) compresses fine-tuned behavioral deltas to 1-bit precision, enabling the full Zen model family — nano through ultra — to share a single GPU cluster.</div><div class=post-meta><span title='2026-02-28 11:00:00 -0800 -0800'>February 28, 2026</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;1345 words&nbsp;·&nbsp;Qwen Team</div></div></div><main class=main><article class=post-single><div class=post-content><p><a href=https://arxiv.org/abs/2402.10193 class="btn external" target=_blank>BITDELTA PAPER</a>
<a href=https://arxiv.org/abs/2602.09689 class="btn external" target=_blank>MONOSOUP PAPER</a>
<a href=https://arxiv.org/abs/2510.13537 class="btn external" target=_blank>K-MERGE PAPER</a>
<a href=https://huggingface.co/zenlm class="btn external" target=_blank>ZEN MODELS</a></p><p>The Zen model family has a deployment problem that is not immediately obvious from the outside. We publish 14+ distinct model variants — from zen-nano at 0.6B parameters to zen4-ultra at 1.04T. Each variant carries fine-tuned behavioral characteristics: different personas, different task specializations, different safety postures. In a naive serving architecture, each variant is a separate set of weights. Loading all of them onto a GPU cluster is economically impossible.</p><p>BitDelta is how we solve this. It compresses the behavioral delta between a base model and a fine-tuned variant down to 1-bit precision, reducing the per-variant memory cost by 16-32x while retaining 99.3% of full-precision behavioral accuracy.</p><h2 id=the-multi-variant-deployment-problem>The Multi-Variant Deployment Problem<a hidden class=anchor aria-hidden=true href=#the-multi-variant-deployment-problem>#</a></h2><p>Consider the economics concretely. A single zen4-ultra shard (1.04T parameters, bfloat16) requires roughly 2TB of GPU memory. Even a single full-precision variant of zen-max (72B) requires ~144GB. With 14 variants across our model catalog:</p><table><thead><tr><th>Tier</th><th>Parameters</th><th>Full Precision (BF16)</th><th>Variants</th><th>Total</th></tr></thead><tbody><tr><td>nano</td><td>0.6B</td><td>1.2 GB</td><td>4</td><td>4.8 GB</td></tr><tr><td>eco / coder-4b</td><td>4B</td><td>8 GB</td><td>3</td><td>24 GB</td></tr><tr><td>zen4-max</td><td>30B</td><td>60 GB</td><td>3</td><td>180 GB</td></tr><tr><td>zen-max</td><td>72B</td><td>144 GB</td><td>2</td><td>288 GB</td></tr><tr><td>zen4-ultra</td><td>1.04T</td><td>~2 TB</td><td>1</td><td>~2 TB</td></tr></tbody></table><p>Keeping all of these &ldquo;hot&rdquo; simultaneously is not feasible. Cold-loading from object storage introduces latency spikes that make the service unusable. We need a different architecture.</p><p>The key observation: most variants share an identical base model. The behavioral differences — the fine-tuned identity, the task specialization, the adjusted refusal boundaries — live in the <strong>delta</strong> between fine-tuned weights and base weights. If we can compress that delta aggressively, we can keep only the base model fully loaded and reconstruct any variant on the fly.</p><h2 id=bitdelta-theory>BitDelta Theory<a hidden class=anchor aria-hidden=true href=#bitdelta-theory>#</a></h2><p><strong>Paper</strong>: arXiv:2402.10193</p><p>BitDelta decomposes a fine-tuned weight matrix as:</p><pre tabindex=0><code>W_ft = W_base + Δ
</code></pre><p>and approximates the delta with 1-bit quantization:</p><pre tabindex=0><code>Δ ≈ α · sign(Δ)
</code></pre><p>where the scale factor α is the mean absolute value of the delta entries:</p><pre tabindex=0><code>α = (1/n) Σ |Δ_ij|
</code></pre><p>This is a single scalar per weight matrix. The sign matrix is 1-bit per element. Total storage for the delta: n bits + 1 float32. For a 4096×4096 weight matrix, that is 16MB → 2MB. For the full zen-max 72B delta, the storage requirement drops from ~144GB to ~9GB.</p><p>Why does 1-bit sign quantization work? The delta values in fine-tuned LLMs follow a near-Laplace distribution centered at zero. The signs carry the directional information; the scale α captures the magnitude. The residual error:</p><pre tabindex=0><code>ε = Δ - α · sign(Δ)
</code></pre><p>has bounded expected squared norm:</p><pre tabindex=0><code>E[||ε||²] ≤ (1 - 2/π) · ||Δ||²  ≈ 0.36 · ||Δ||²
</code></pre><p>In practice (and this is the empirical surprise), the effective error on model outputs is far smaller than this bound suggests, because the residuals are uncorrelated with the task-relevant signal directions. The model&rsquo;s behavioral accuracy degrades gracefully rather than catastrophically.</p><h2 id=implementation-fused-cuda-kernel>Implementation: Fused CUDA Kernel<a hidden class=anchor aria-hidden=true href=#implementation-fused-cuda-kernel>#</a></h2><p>The critical implementation detail is efficiency. Reconstructing <code>W_ft = W_base + α · sign(Δ)</code> at inference time must not add meaningful latency. Our CUDA kernel fuses three operations:</p><ol><li>Load sign bits from compressed storage (1-bit tensor, integer packing)</li><li>Unpack and scale: <code>delta_row = alpha * sign_bits.float() * 2 - 1</code></li><li>Add to base weight tile in shared memory before GEMM</li></ol><p>The result: delta reconstruction adds less than 1ms overhead per forward pass on an A100. In practice the overhead is dominated by memory bandwidth to load the sign bits, which at 1/16th the size of the base weight tensor is negligible.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>compress_delta</span><span class=p>(</span><span class=n>W_ft</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>W_base</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Compress fine-tuned weight delta to 1-bit + scale.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>delta</span> <span class=o>=</span> <span class=n>W_ft</span> <span class=o>-</span> <span class=n>W_base</span>
</span></span><span class=line><span class=cl>    <span class=n>alpha</span> <span class=o>=</span> <span class=n>delta</span><span class=o>.</span><span class=n>abs</span><span class=p>()</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>sign_bits</span> <span class=o>=</span> <span class=p>(</span><span class=n>delta</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>uint8</span><span class=p>)</span>  <span class=c1># 1 = positive, 0 = negative</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>sign_bits</span><span class=p>,</span> <span class=n>alpha</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>reconstruct_weight</span><span class=p>(</span><span class=n>W_base</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>sign_bits</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>alpha</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Reconstruct fine-tuned weight from base + compressed delta.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>signs</span> <span class=o>=</span> <span class=n>sign_bits</span><span class=o>.</span><span class=n>float</span><span class=p>()</span> <span class=o>*</span> <span class=mi>2</span> <span class=o>-</span> <span class=mi>1</span>  <span class=c1># map {0,1} → {-1,+1}</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>W_base</span> <span class=o>+</span> <span class=n>alpha</span> <span class=o>*</span> <span class=n>signs</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>memory_savings</span><span class=p>(</span><span class=n>d_out</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>d_in</span><span class=p>:</span> <span class=nb>int</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Compare memory usage: full delta vs BitDelta.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>full_bytes</span> <span class=o>=</span> <span class=n>d_out</span> <span class=o>*</span> <span class=n>d_in</span> <span class=o>*</span> <span class=mi>2</span>   <span class=c1># bfloat16</span>
</span></span><span class=line><span class=cl>    <span class=n>bitdelta_bytes</span> <span class=o>=</span> <span class=n>d_out</span> <span class=o>*</span> <span class=n>d_in</span> <span class=o>//</span> <span class=mi>8</span> <span class=o>+</span> <span class=mi>4</span>  <span class=c1># 1-bit + float32 scale</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;full_delta_mb&#39;</span><span class=p>:</span> <span class=n>full_bytes</span> <span class=o>/</span> <span class=mf>1e6</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;bitdelta_mb&#39;</span><span class=p>:</span> <span class=n>bitdelta_bytes</span> <span class=o>/</span> <span class=mf>1e6</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;compression_ratio&#39;</span><span class=p>:</span> <span class=n>full_bytes</span> <span class=o>/</span> <span class=n>bitdelta_bytes</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span></code></pre></div><h2 id=quality-results>Quality Results<a hidden class=anchor aria-hidden=true href=#quality-results>#</a></h2><p>We evaluated BitDelta across five Zen variants against their full-precision counterparts:</p><table><thead><tr><th>Model</th><th>Task</th><th>Full Precision</th><th>BitDelta</th><th>Retention</th></tr></thead><tbody><tr><td>zen-nano</td><td>MMLU</td><td>61.3</td><td>60.8</td><td>99.2%</td></tr><tr><td>zen4-max</td><td>HumanEval</td><td>74.1</td><td>73.5</td><td>99.1%</td></tr><tr><td>zen4-pro</td><td>GSM8K</td><td>88.4</td><td>87.9</td><td>99.4%</td></tr><tr><td>zen-max</td><td>GPQA</td><td>71.2</td><td>70.6</td><td>99.2%</td></tr><tr><td>zen4-ultra</td><td>AIME 2024</td><td>94.7</td><td>93.6</td><td>98.8%</td></tr></tbody></table><p>Average behavioral retention: <strong>99.3%</strong>. The 0.7% average degradation is below the noise floor of our human preference evaluations — users cannot reliably distinguish BitDelta variants from full-precision variants in blind A/B tests.</p><h2 id=monosoup-svd-fallback-for-weak-checkpoints>MonoSoup: SVD Fallback for Weak Checkpoints<a hidden class=anchor aria-hidden=true href=#monosoup-svd-fallback-for-weak-checkpoints>#</a></h2><p><strong>Paper</strong>: arXiv:2602.09689</p><p>BitDelta works well when the delta is well-behaved (small, distributed, near-Laplace). Some fine-tuned checkpoints — particularly those from aggressive few-shot fine-tuning or noisy datasets — produce deltas that are large and spiky. In these cases, 1-bit quantization introduces perceptible degradation.</p><p>MonoSoup provides a complementary approach: instead of compressing the delta, decompose the full fine-tuned weight via SVD and keep only the top-k singular triplets:</p><pre tabindex=0><code>W_ft ≈ U_k Σ_k V_k^T
</code></pre><p>where k is chosen to keep 95% of the Frobenius norm. This is not a delta compression technique — it operates on the single fine-tuned checkpoint directly. But for weak checkpoints where BitDelta degrades, MonoSoup recovers up to 8% of the lost behavioral accuracy at comparable memory cost.</p><p>In our pipeline: we try BitDelta first. If behavioral retention falls below 98.5% on our internal benchmark suite, we fall back to MonoSoup with k calibrated to budget.</p><h2 id=k-merge-edge-adapter-management>K-Merge: Edge Adapter Management<a hidden class=anchor aria-hidden=true href=#k-merge-edge-adapter-management>#</a></h2><p><strong>Paper</strong>: arXiv:2510.13537</p><p>The cloud serving stack above does not address edge deployment. A local user running zen-nano on a 16GB laptop cannot afford a delta cache for 14 variants — even at 1-bit compression, storing all nano variants would consume significant RAM.</p><p>K-Merge addresses this with an <strong>online LoRA adapter pool under fixed storage budget</strong>. The algorithm maintains a priority queue of adapters scored by utility:</p><pre tabindex=0><code>utility(adapter_i) = request_frequency(i) × behavioral_gain(i) / storage_cost(i)
</code></pre><p>When the budget is exceeded, the lowest-utility adapter is evicted. Utility scores are updated online using exponential decay, so recently used adapters are preferred over historical ones.</p><p>For a 16GB laptop with 4GB allocated to the adapter pool, K-Merge keeps 6-8 zen-nano variants hot simultaneously, with eviction latency of ~200ms to load a new adapter from local disk.</p><h2 id=full-zen-serving-stack>Full Zen Serving Stack<a hidden class=anchor aria-hidden=true href=#full-zen-serving-stack>#</a></h2><pre tabindex=0><code>                    ┌──────────────────────────────┐
                    │      Request Router           │
                    │  (model ID → variant key)     │
                    └────────────┬─────────────────┘
                                 │
                    ┌────────────▼─────────────────┐
                    │     Delta Cache (Redis)       │
                    │  sign_bits + alpha per layer  │
                    │  ~9 GB per 72B variant        │
                    └────────────┬─────────────────┘
                                 │ cache hit
                    ┌────────────▼─────────────────┐
                    │  Shared Base Model (BF16)     │
                    │  zen-max 72B: 144 GB on A100s │
                    │  zen-nano 0.6B: 1.2 GB        │
                    └────────────┬─────────────────┘
                                 │
                    ┌────────────▼─────────────────┐
                    │  Fused Reconstruction Kernel  │
                    │  W_ft = W_base + α·sign(Δ)   │
                    │  &lt; 1ms overhead per layer     │
                    └────────────┬─────────────────┘
                                 │
                    ┌────────────▼─────────────────┐
                    │       Inference Engine        │
                    │  (vLLM with continuous batch) │
                    └──────────────────────────────┘
</code></pre><p>The architecture keeps one base model loaded per GPU cluster. All variants share it. The delta cache fits in Redis (NVMe-backed), loading on demand in under 50ms. In practice, our top-5 variants stay hot in Redis memory; the remaining variants load from NVMe on first request.</p><h2 id=gpu-memory-reduction>GPU Memory Reduction<a hidden class=anchor aria-hidden=true href=#gpu-memory-reduction>#</a></h2><table><thead><tr><th>Scenario</th><th>Without BitDelta</th><th>With BitDelta</th><th>Savings</th></tr></thead><tbody><tr><td>3× zen-nano variants</td><td>3.6 GB</td><td>1.4 GB</td><td>61%</td></tr><tr><td>5× zen4-max variants</td><td>300 GB</td><td>192 GB</td><td>36%</td></tr><tr><td>2× zen-max variants</td><td>288 GB</td><td>162 GB</td><td>44%</td></tr><tr><td>Full 14-model catalog</td><td>~2.8 TB</td><td>~2.2 TB</td><td>21%</td></tr></tbody></table><p>The savings are most dramatic at the smaller scales where we have many more behavioral variants. For the ultra-scale models (zen4-ultra 1T+), a single checkpoint dominates, and BitDelta&rsquo;s contribution is smaller — but MonoSoup and K-Merge become more relevant for edge quantization.</p><p>The combination of BitDelta for cloud serving, MonoSoup for quality recovery, and K-Merge for edge devices gives us a coherent three-tier compression story across the full Zen catalog.</p><hr><p><em>Zen LM is a joint initiative of Hanzo AI Inc. (Techstars &lsquo;17) and Zoo Labs Foundation (501c3).</em></p></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://qwenlm.github.io/>Qwen</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>