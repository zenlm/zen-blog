1:"$Sreact.fragment"
2:I[10086,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/36bfed0236ce2cf2.js","/_next/static/chunks/e62b91212ee7f8ff.js","/_next/static/chunks/2a98816c7d26bf58.js","/_next/static/chunks/cb0a883bafeb6805.js"],""]
3:I[48068,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/36bfed0236ce2cf2.js","/_next/static/chunks/e62b91212ee7f8ff.js","/_next/static/chunks/2a98816c7d26bf58.js","/_next/static/chunks/cb0a883bafeb6805.js"],"default"]
1c:I[51504,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/36bfed0236ce2cf2.js","/_next/static/chunks/e62b91212ee7f8ff.js","/_next/static/chunks/2a98816c7d26bf58.js","/_next/static/chunks/cb0a883bafeb6805.js"],"CodeBlock"]
1d:I[51504,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/36bfed0236ce2cf2.js","/_next/static/chunks/e62b91212ee7f8ff.js","/_next/static/chunks/2a98816c7d26bf58.js","/_next/static/chunks/cb0a883bafeb6805.js"],"Pre"]
1e:I[89923,["/_next/static/chunks/4d80e004cf4896dd.js","/_next/static/chunks/350ee4303b732916.js"],"OutletBoundary"]
1f:"$Sreact.suspense"
0:{"buildId":"qMVpAZcUAPsCZEMM19Q64","rsc":["$","$1","c",{"children":[["$","main",null,{"className":"mx-auto w-full max-w-2xl px-4 py-16","children":[["$","$L2",null,{"href":"/blog","className":"inline-flex items-center gap-1.5 text-sm text-fd-muted-foreground hover:text-fd-foreground mb-10 transition-colors","children":"← Back to Blog"}],["$","div",null,{"className":"mb-8","children":[["$","time",null,{"className":"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider","children":"January 22, 2024"}],["$","h1",null,{"className":"text-3xl font-bold mt-2 mb-3","children":"Introducing Qwen"}],["$","p",null,{"className":"text-fd-muted-foreground text-lg mb-4","children":"4 months after our first release of Qwen-7B, which is the starting point of our opensource journey of large language models (LLM), we now provide an introduction to the Qwen series to give you a whole picture of our work as well as our objectives. Below are important links to our opensource projects and community."}],["$","div",null,{"className":"flex items-center gap-3 pt-4 border-t border-fd-border","children":[["$","span",null,{"className":"text-sm text-fd-muted-foreground","children":["By ","Zen LM Team"]}],["$","div",null,{"className":"flex gap-1.5 ml-auto","children":[]}]]}]]}],["$","div",null,{"className":"prose dark:prose-invert max-w-none","children":[["$","p",null,{"children":"4 months after our first release of Qwen-7B, which is the starting point of our opensource journey of large language models (LLM), we now provide an introduction to the Qwen series to give you a whole picture of our work as well as our objectives. Below are important links to our opensource projects and community."}],"\n",["$","p",null,{"children":[["$","$L3",null,{"href":"https://arxiv.org/abs/2309.16609","children":"PAPER"}]," ",["$","$L3",null,{"href":"https://github.com/QwenLM/Qwen","children":"GITHUB"}]," ",["$","$L3",null,{"href":"https://huggingface.co/Qwen","children":"HUGGING FACE"}]," ",["$","$L3",null,{"href":"https://modelscope.cn/organization/qwen","children":"MODELSCOPE"}]," ",["$","$L3",null,{"href":"https://discord.gg/CV4E9rpNSD","children":"DISCORD"}]]}],"\n",["$","p",null,{"children":"Additionally, we have WeChat groups for chatting and we invite you to join the groups through the provided link in our GitHub readme."}],"\n",["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"overview","children":[["$","a",null,{"data-card":"","href":"#overview","className":"peer","children":"Overview"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}],"\n",["$","p",null,{"children":"In general, Qwen is more than a language model but a project towards AGI which for now consists of LLM and LMM. The following figure shows the main components of Qwen:"}],"\n",["$","p",null,{"children":"where Qwen refers to the base language model, while Qwen-Chat refers to the chat model trained with techniques like SFT and RLHF. We also have models specialized for domains and tasks, such as Code-Qwen for coding and Math-Qwen for mathematics. LLM can be extended to multimodality with modality alignment, and thus we have vision-language model Qwen-VL as well as audio-language model Qwen-Audio. Note that this blog mainly serves for introducing the language model. As to the large multimodal models (LMM), such as Qwen-VL and Qwen-Audio, please refer to the respective blog."}],"\n","$L4","\n","$L5","\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","$L6","\n","$L7","\n","$L8","\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","$L9","\n","$La","\n","$Lb","\n","$Lc","\n","$Ld","\n","$Le","\n","$Lf","\n","$L10","\n","$L11","\n","$L12","\n","$L13","\n","$L14","\n","$L15","\n","$L16"]}]]}],["$L17","$L18","$L19","$L1a"],"$L1b"]}],"loading":null,"isPartial":false}
4:["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"base-model-a-good-starting-point-for-alignment","children":[["$","a",null,{"data-card":"","href":"#base-model-a-good-starting-point-for-alignment","className":"peer","children":"Base Model: A Good Starting Point for Alignment"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
5:["$","p",null,{"children":"The general procedure of building an assistant model includes pretraining and post-training, where the latter mostly consists of SFT and RLHF. As to pretraining, similar to previous LLM, GPT-3, Llama, Qwen is a Transformer-based language model pretrained by the task of next token prediction. For simplicity and stability, we did not introduce more tasks for the language model but focus on model size scaling and data scaling. For now, we have developed 5 models of different sizes, 4 of which are opensourced. Specially, we now release Qwen-1.8B, Qwen-7B, Qwen-14B, and Qwen-72B."}]
6:["$","div",null,{"className":"relative overflow-auto prose-no-margin my-6","children":["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"Model"}],["$","th",null,{"children":"Release Date"}],["$","th",null,{"children":"Max Length"}],["$","th",null,{"children":"System Prompt Enhancement"}],["$","th",null,{"children":"# of Pretrained Tokens"}],["$","th",null,{"children":"Minimum GPU Memory Usage of Finetuning (Q-Lora)"}],["$","th",null,{"children":"Minimum GPU Usage of Generating 2048 Tokens (Int4)"}],["$","th",null,{"children":"Tool Usage"}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":"Qwen-1.8B"}],["$","td",null,{"children":"23.11.30"}],["$","td",null,{"children":"32K"}],["$","td",null,{"children":"✔"}],["$","td",null,{"children":"2.2T"}],["$","td",null,{"children":"5.8GB"}],["$","td",null,{"children":"2.9GB"}],["$","td",null,{"children":"✔"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"Qwen-7B"}],["$","td",null,{"children":"23.08.03"}],["$","td",null,{"children":"32K"}],["$","td",null,{"children":"✘"}],["$","td",null,{"children":"2.4T"}],["$","td",null,{"children":"11.5GB"}],["$","td",null,{"children":"8.2GB"}],["$","td",null,{"children":"✔"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"Qwen-14B"}],["$","td",null,{"children":"23.09.25"}],["$","td",null,{"children":"8K"}],["$","td",null,{"children":"✘"}],["$","td",null,{"children":"3.0T"}],["$","td",null,{"children":"18.7GB"}],["$","td",null,{"children":"13.0GB"}],["$","td",null,{"children":"✔"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"Qwen-72B"}],["$","td",null,{"children":"23.11.30"}],["$","td",null,{"children":"32K"}],["$","td",null,{"children":"✔"}],["$","td",null,{"children":"3.0T"}],["$","td",null,{"children":"61.4GB"}],["$","td",null,{"children":"48.9GB"}],["$","td",null,{"children":"✔"}]]}]]}]]}]}]
7:["$","p",null,{"children":"Models are sufficiently trained with 2-3 trillion tokens. The pretraining data are multilingual, and thus Qwen is essentially a multilingual model instead of a model of a single language or bilingual. Note that due to the limitations of our pretraining data, the model is strongly capable of English and Chinese and also capable of other languages, such as Spanish, French, and Japanese. To extend its multilingual capabilities, we applied a tokenizer with high efficiency in encoding information from different languages. In comparison with other tokenizers, ours demonstrates high compression rate in a series of languages."}]
8:["$","p",null,{"children":"Another focus of our pretraining is the extension of context length. We directly apply continual pretraining with longer context length and larger base value for RoPE. Additionally, we find that.this method is also effective in extrapolation. Now our opensourced models mostly support a context length of 32K tokens, and they were evaluated through L-Eval and “Needle in a Haystack”."}]
9:["$","div",null,{"className":"relative overflow-auto prose-no-margin my-6","children":["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"Model"}],["$","th",null,{"children":"Input Length"}],["$","th",null,{"children":"Average"}],["$","th",null,{"children":"Coursera"}],["$","th",null,{"children":"GSM"}],["$","th",null,{"children":"QuALITY"}],["$","th",null,{"children":"TOEFL"}],["$","th",null,{"children":"CodeU"}],["$","th",null,{"children":"SFcition"}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":"ChatGPT-3.5-16k"}],["$","td",null,{"children":"16K"}],["$","td",null,{"children":"60.73"}],["$","td",null,{"children":"63.51"}],["$","td",null,{"children":"84.00"}],["$","td",null,{"children":"61.38"}],["$","td",null,{"children":"78.43"}],["$","td",null,{"children":"12.22"}],["$","td",null,{"children":"64.84"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"Qwen-72B-Chat"}],["$","td",null,{"children":"32K"}],["$","td",null,{"children":"62.30"}],["$","td",null,{"children":"58.13"}],["$","td",null,{"children":"76.00"}],["$","td",null,{"children":"77.22"}],["$","td",null,{"children":"86.24"}],["$","td",null,{"children":"6.66"}],["$","td",null,{"children":"69.53"}]]}]]}]]}]}]
a:["$","p",null,{"children":"Benchmark evaluation shows that our largest opensourced model Qwen-72B as well as the largest proprietary shows competitive performance against Llama 2, GPT-3.5 and GPT-4."}]
b:["$","p",null,{"children":"Note that this is an evaluation of base language model. This only reflects that we might have a good starting point for post-training, i.e., SFT and RLHF."}]
c:["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"alignment","children":[["$","a",null,{"data-card":"","href":"#alignment","className":"peer","children":"Alignment"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
d:["$","p",null,{"children":"We refer both techniques to the word “alignment” in post-training. Currently, it is consensus that we can obtain a chat model with a relatively small amount of finetuning data. We focus on improving the diversity and complexity (instag and tulu 2) of the SFT data and strictly control the quality by manual checking and automatic evaluation."}]
e:["$","p",null,{"children":"Based on a good SFT model, we can then explore the effects of RLHF. It is difficult to train RLHF, specifically PPO-based method, Besides the training instabilities of PPO, another key to the final performance is the quality of reward model. Therefore, we have spent efforts in building a reliable reward model by reward model pretraining on large-scale comparison data and finetuning on carefully labeled comparison data of high quality. In comparison with the SFT model, we find that the RLHF model is more creative and follows the instructions better, and thus its generated responses are more preferred by human annotators."}]
f:["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"tool-use-and-agent","children":[["$","a",null,{"data-card":"","href":"#tool-use-and-agent","className":"peer","children":"Tool Use and Agent"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
10:["$","p",null,{"children":"One of the most amazing parts of today’s LLMs is the capabilities of tool use and agent playing. We directly label data of ReAct formats in order to endow the abilities of generating thought and action and generating responses based on previous steps and observations. Also, the model directly learns the in-context learning ability and thus it then can use unseen tool through understanding instructions and demonstrations."}]
11:["$","p",null,{"children":"We currently support function calling, code interpreter, and hugging face agent, which respectively serves for tool use, data analysis and using AI models for different outputs, say image generation. Furthermore, based on our agent framework, we further build a project called AgentFabric, following GPTs, which allows you to build a specialzed AI agent for yourself simply by chatting with our model for configuration."}]
12:["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"summary","children":[["$","a",null,{"data-card":"","href":"#summary","className":"peer","children":"Summary"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
13:["$","p",null,{"children":"We release the Qwen series, and in this blog, we provide a simple introduction to the Qwen language models Now, we are still following the recipes of pretraining, SFT, and RLHF and we are figuring out a path towards scaling model and data. We hope that our opensource is contributive to the research and application communities."}]
14:["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"citation","children":[["$","a",null,{"data-card":"","href":"#citation","className":"peer","children":"Citation"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
15:["$","p",null,{"children":"If you find our work helpful, feel free to give us a cite!"}]
16:["$","$L1c",null,{"className":"shiki shiki-themes github-light github-dark","style":{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},"tabIndex":"0","icon":"<svg viewBox=\"0 0 24 24\"><path d=\"M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z\" fill=\"currentColor\" /></svg>","children":["$","$L1d",null,{"children":["$","code",null,{"children":[["$","span",null,{"className":"line","children":["$","span",null,{}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"    @article{qwen,"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"      title={Qwen Technical Report},"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"      author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"      journal={arXiv preprint arXiv:2309.16609},"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"      year={2023}"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"    }"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"    "}]}]]}]}]}]
17:["$","script","script-0",{"src":"/_next/static/chunks/36bfed0236ce2cf2.js","async":true}]
18:["$","script","script-1",{"src":"/_next/static/chunks/e62b91212ee7f8ff.js","async":true}]
19:["$","script","script-2",{"src":"/_next/static/chunks/2a98816c7d26bf58.js","async":true}]
1a:["$","script","script-3",{"src":"/_next/static/chunks/cb0a883bafeb6805.js","async":true}]
1b:["$","$L1e",null,{"children":["$","$1f",null,{"name":"Next.MetadataOutlet","children":"$@20"}]}]
20:null
