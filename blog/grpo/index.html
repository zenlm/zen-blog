<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>GRPO: Group Relative Policy Optimization | Zen LM</title><meta name=keywords content="Research,Alignment,Training"><meta name=description content="Introducing GRPO, a new approach to reinforcement learning from human feedback that improves sample efficiency and alignment stability."><meta name=author content="Zach Kelling"><link rel=canonical href=https://zenlm.org/blog/grpo/><link crossorigin=anonymous href=/assets/css/stylesheet.6c0865a4bc8dbcbd27d78777eb33b404568f7fbf3185cca7b978e5275a4dd049.css integrity="sha256-bAhlpLyNvL0n14d36zO0BFaPf78xhcynuXjlJ1pN0Ek=" rel="preload stylesheet" as=style><link rel=icon href=https://zenlm.org/favicon.png><link rel=apple-touch-icon href=https://zenlm.org/favicon.png><link rel=manifest href=https://zenlm.org/site.webmanifest><meta name=theme-color content="#615CED"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:title" content="GRPO: Group Relative Policy Optimization"><meta property="og:description" content="Introducing GRPO, a new approach to reinforcement learning from human feedback that improves sample efficiency and alignment stability."><meta property="og:type" content="article"><meta property="og:url" content="https://zenlm.org/blog/grpo/"><meta property="og:image" content="https://zenlm.org/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2022-09-19T09:00:00-08:00"><meta property="article:modified_time" content="2022-09-19T09:00:00-08:00"><meta property="og:site_name" content="Zen LM"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://zenlm.org/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="GRPO: Group Relative Policy Optimization"><meta name=twitter:description content="Introducing GRPO, a new approach to reinforcement learning from human feedback that improves sample efficiency and alignment stability."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://zenlm.org/blog/"},{"@type":"ListItem","position":2,"name":"GRPO: Group Relative Policy Optimization","item":"https://zenlm.org/blog/grpo/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"GRPO: Group Relative Policy Optimization","name":"GRPO: Group Relative Policy Optimization","description":"Introducing GRPO, a new approach to reinforcement learning from human feedback that improves sample efficiency and alignment stability.","keywords":["Research","Alignment","Training"],"articleBody":"Reinforcement learning from human feedback (RLHF) has become central to aligning language models with human preferences. But current methods like PPO are sample-inefficient and unstable. Today we introduce Group Relative Policy Optimization (GRPO), a new approach that addresses these limitations.\nThe RLHF Challenge Standard RLHF follows three steps:\nTrain a reward model on human preference data Use the reward model to provide training signal Optimize the policy with reinforcement learning (typically PPO) Step 3 is problematic. PPO requires careful hyperparameter tuning, extensive sampling, and still often produces unstable training dynamics.\nKey Insight: Relative Comparisons Humans naturally make relative judgments. “Response A is better than B” comes more easily than “Response A scores 7.3.” Yet reward models output absolute scores that discard this relational structure.\nGRPO preserves relative comparisons throughout training.\nThe GRPO Algorithm Instead of optimizing absolute rewards, GRPO optimizes within groups of sampled responses.\nSampling For each prompt $x$, sample a group of $k$ responses:\n$$G = {y_1, y_2, …, y_k} \\sim \\pi_\\theta(y|x)$$\nRanking Rank responses within the group using the reward model:\n$$r_i = R(x, y_i)$$ $$\\text{rank}(y_i) = |{j : r_j \u003e r_i}| + 1$$\nRelative Advantage Compute advantages relative to the group:\n$$A_i = \\frac{r_i - \\mu_G}{\\sigma_G}$$\nwhere $\\mu_G$ and $\\sigma_G$ are the group mean and standard deviation.\nPolicy Update Update the policy to increase probability of high-ranked responses:\n$$\\mathcal{L}(\\theta) = -\\mathbb{E}{y \\sim G}\\left[\\min\\left(\\frac{\\pi\\theta(y|x)}{\\pi_{\\text{old}}(y|x)} A, \\text{clip}\\left(\\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{old}}(y|x)}, 1-\\epsilon, 1+\\epsilon\\right) A\\right)\\right]$$\nThe clipping stabilizes training, similar to PPO, but the relative advantages provide better signal.\nWhy GRPO Works Robust to Reward Scale Absolute reward values vary across prompts and reward model calibration. Normalization within groups removes this sensitivity.\nBetter Gradient Signal Groups provide multiple comparison points per prompt. This reduces variance and improves sample efficiency.\nNatural Curriculum Hard prompts where all responses score poorly still provide useful gradients. The best-in-group response gets positive advantage even if absolute rewards are low.\nReduced Reward Hacking Optimizing relative rankings is harder to game than optimizing absolute scores. The model must genuinely improve, not find reward model exploits.\nExperimental Results We compared GRPO to PPO on alignment benchmarks:\nMethod Helpfulness Harmlessness Honesty Samples Required PPO 78.2 82.1 75.4 100K GRPO 81.7 84.3 79.1 35K GRPO achieves better alignment with 3x fewer samples.\nTraining dynamics also improve significantly:\nStability: GRPO loss curves show less variance Convergence: Reaches final performance 2x faster Robustness: Less sensitive to learning rate choice Implementation Details Key hyperparameters:\nGroup size ($k$): 8-16 works well Clipping ($\\epsilon$): 0.1-0.2 KL penalty: Lower than PPO (0.01 vs 0.1) The larger group size compared to PPO’s typical 2-response setup is essential. More comparisons mean better gradient estimates.\nCode Release We’re releasing our GRPO implementation integrated with the Zen training framework:\nfrom zen.alignment import GRPOTrainer trainer = GRPOTrainer( model=model, reward_model=reward_model, group_size=12, clip_epsilon=0.15, ) trainer.train(prompts) Full documentation and examples at github.com/zoo-labs/zen-align.\nWhat’s Next GRPO opens several research directions:\nMulti-objective GRPO: Separate groups for different alignment dimensions Online preference learning: Update reward model during training Constitutional GRPO: Use principles instead of learned rewards Alignment is the central challenge of AI development. GRPO is one step toward making it more reliable.\nZach Kelling is a co-founder of Zoo Labs Foundation.\n","wordCount":"522","inLanguage":"en","datePublished":"2022-09-19T09:00:00-08:00","dateModified":"2022-09-19T09:00:00-08:00","author":{"@type":"Person","name":"Zach Kelling"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://zenlm.org/blog/grpo/"},"publisher":{"@type":"Organization","name":"Zen LM","logo":{"@type":"ImageObject","url":"https://zenlm.org/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Zen LM (Alt + H)"><img src=https://zenlm.org/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://zeekay.blog title=zeekay><span>zeekay</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://hanzo.blog title=hanzo.blog><span>hanzo.blog</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://hanzo.ai/chat title="Try Zen Chat"><span>Try Zen Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://blog.zoo.ngo title="zoo blog"><span>zoo blog</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>GRPO: Group Relative Policy Optimization</h1><div class=post-description>Introducing GRPO, a new approach to reinforcement learning from human feedback that improves sample efficiency and alignment stability.</div><div class=post-meta><span title='2022-09-19 09:00:00 -0800 -0800'>September 19, 2022</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;522 words&nbsp;·&nbsp;Zach Kelling</div></div></div><main class=main><article class=post-single><div class=post-content><p>Reinforcement learning from human feedback (RLHF) has become central to aligning language models with human preferences. But current methods like PPO are sample-inefficient and unstable. Today we introduce Group Relative Policy Optimization (GRPO), a new approach that addresses these limitations.</p><h2 id=the-rlhf-challenge>The RLHF Challenge<a hidden class=anchor aria-hidden=true href=#the-rlhf-challenge>#</a></h2><p>Standard RLHF follows three steps:</p><ol><li>Train a reward model on human preference data</li><li>Use the reward model to provide training signal</li><li>Optimize the policy with reinforcement learning (typically PPO)</li></ol><p>Step 3 is problematic. PPO requires careful hyperparameter tuning, extensive sampling, and still often produces unstable training dynamics.</p><h2 id=key-insight-relative-comparisons>Key Insight: Relative Comparisons<a hidden class=anchor aria-hidden=true href=#key-insight-relative-comparisons>#</a></h2><p>Humans naturally make relative judgments. &ldquo;Response A is better than B&rdquo; comes more easily than &ldquo;Response A scores 7.3.&rdquo; Yet reward models output absolute scores that discard this relational structure.</p><p>GRPO preserves relative comparisons throughout training.</p><h2 id=the-grpo-algorithm>The GRPO Algorithm<a hidden class=anchor aria-hidden=true href=#the-grpo-algorithm>#</a></h2><p>Instead of optimizing absolute rewards, GRPO optimizes within groups of sampled responses.</p><h3 id=sampling>Sampling<a hidden class=anchor aria-hidden=true href=#sampling>#</a></h3><p>For each prompt $x$, sample a group of $k$ responses:</p><p>$$G = {y_1, y_2, &mldr;, y_k} \sim \pi_\theta(y|x)$$</p><h3 id=ranking>Ranking<a hidden class=anchor aria-hidden=true href=#ranking>#</a></h3><p>Rank responses within the group using the reward model:</p><p>$$r_i = R(x, y_i)$$
$$\text{rank}(y_i) = |{j : r_j > r_i}| + 1$$</p><h3 id=relative-advantage>Relative Advantage<a hidden class=anchor aria-hidden=true href=#relative-advantage>#</a></h3><p>Compute advantages relative to the group:</p><p>$$A_i = \frac{r_i - \mu_G}{\sigma_G}$$</p><p>where $\mu_G$ and $\sigma_G$ are the group mean and standard deviation.</p><h3 id=policy-update>Policy Update<a hidden class=anchor aria-hidden=true href=#policy-update>#</a></h3><p>Update the policy to increase probability of high-ranked responses:</p><p>$$\mathcal{L}(\theta) = -\mathbb{E}<em>{y \sim G}\left[\min\left(\frac{\pi</em>\theta(y|x)}{\pi_{\text{old}}(y|x)} A, \text{clip}\left(\frac{\pi_\theta(y|x)}{\pi_{\text{old}}(y|x)}, 1-\epsilon, 1+\epsilon\right) A\right)\right]$$</p><p>The clipping stabilizes training, similar to PPO, but the relative advantages provide better signal.</p><h2 id=why-grpo-works>Why GRPO Works<a hidden class=anchor aria-hidden=true href=#why-grpo-works>#</a></h2><h3 id=robust-to-reward-scale>Robust to Reward Scale<a hidden class=anchor aria-hidden=true href=#robust-to-reward-scale>#</a></h3><p>Absolute reward values vary across prompts and reward model calibration. Normalization within groups removes this sensitivity.</p><h3 id=better-gradient-signal>Better Gradient Signal<a hidden class=anchor aria-hidden=true href=#better-gradient-signal>#</a></h3><p>Groups provide multiple comparison points per prompt. This reduces variance and improves sample efficiency.</p><h3 id=natural-curriculum>Natural Curriculum<a hidden class=anchor aria-hidden=true href=#natural-curriculum>#</a></h3><p>Hard prompts where all responses score poorly still provide useful gradients. The best-in-group response gets positive advantage even if absolute rewards are low.</p><h3 id=reduced-reward-hacking>Reduced Reward Hacking<a hidden class=anchor aria-hidden=true href=#reduced-reward-hacking>#</a></h3><p>Optimizing relative rankings is harder to game than optimizing absolute scores. The model must genuinely improve, not find reward model exploits.</p><h2 id=experimental-results>Experimental Results<a hidden class=anchor aria-hidden=true href=#experimental-results>#</a></h2><p>We compared GRPO to PPO on alignment benchmarks:</p><table><thead><tr><th>Method</th><th>Helpfulness</th><th>Harmlessness</th><th>Honesty</th><th>Samples Required</th></tr></thead><tbody><tr><td>PPO</td><td>78.2</td><td>82.1</td><td>75.4</td><td>100K</td></tr><tr><td>GRPO</td><td>81.7</td><td>84.3</td><td>79.1</td><td>35K</td></tr></tbody></table><p>GRPO achieves better alignment with 3x fewer samples.</p><p>Training dynamics also improve significantly:</p><ul><li><strong>Stability</strong>: GRPO loss curves show less variance</li><li><strong>Convergence</strong>: Reaches final performance 2x faster</li><li><strong>Robustness</strong>: Less sensitive to learning rate choice</li></ul><h2 id=implementation-details>Implementation Details<a hidden class=anchor aria-hidden=true href=#implementation-details>#</a></h2><p>Key hyperparameters:</p><ul><li><strong>Group size</strong> ($k$): 8-16 works well</li><li><strong>Clipping</strong> ($\epsilon$): 0.1-0.2</li><li><strong>KL penalty</strong>: Lower than PPO (0.01 vs 0.1)</li></ul><p>The larger group size compared to PPO&rsquo;s typical 2-response setup is essential. More comparisons mean better gradient estimates.</p><h2 id=code-release>Code Release<a hidden class=anchor aria-hidden=true href=#code-release>#</a></h2><p>We&rsquo;re releasing our GRPO implementation integrated with the Zen training framework:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>zen.alignment</span> <span class=kn>import</span> <span class=n>GRPOTrainer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>trainer</span> <span class=o>=</span> <span class=n>GRPOTrainer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>=</span><span class=n>model</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>reward_model</span><span class=o>=</span><span class=n>reward_model</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>group_size</span><span class=o>=</span><span class=mi>12</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>clip_epsilon</span><span class=o>=</span><span class=mf>0.15</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>trainer</span><span class=o>.</span><span class=n>train</span><span class=p>(</span><span class=n>prompts</span><span class=p>)</span>
</span></span></code></pre></div><p>Full documentation and examples at github.com/zoo-labs/zen-align.</p><h2 id=whats-next>What&rsquo;s Next<a hidden class=anchor aria-hidden=true href=#whats-next>#</a></h2><p>GRPO opens several research directions:</p><ul><li><strong>Multi-objective GRPO</strong>: Separate groups for different alignment dimensions</li><li><strong>Online preference learning</strong>: Update reward model during training</li><li><strong>Constitutional GRPO</strong>: Use principles instead of learned rewards</li></ul><p>Alignment is the central challenge of AI development. GRPO is one step toward making it more reliable.</p><hr><p><em>Zach Kelling is a co-founder of Zoo Labs Foundation.</em></p></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://zenlm.org/>Zen LM</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>