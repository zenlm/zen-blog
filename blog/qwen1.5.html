<!DOCTYPE html><!--o_J3cFnAZ1mdL_cv2bxKY--><html lang="en" class="dark"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/chunks/f2332aac77592f9d.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/e83606e8fa9cc796.js"/><script src="/_next/static/chunks/36d6595f0156cd7e.js" async=""></script><script src="/_next/static/chunks/040e9cea20a8d9c7.js" async=""></script><script src="/_next/static/chunks/c19fcbf6bf086438.js" async=""></script><script src="/_next/static/chunks/turbopack-7419f7f4f6b062de.js" async=""></script><script src="/_next/static/chunks/59d0ad1b64f8544e.js" async=""></script><script src="/_next/static/chunks/4d80e004cf4896dd.js" async=""></script><script src="/_next/static/chunks/350ee4303b732916.js" async=""></script><script src="/_next/static/chunks/36bfed0236ce2cf2.js" async=""></script><script src="/_next/static/chunks/e62b91212ee7f8ff.js" async=""></script><script src="/_next/static/chunks/2a98816c7d26bf58.js" async=""></script><script src="/_next/static/chunks/cb0a883bafeb6805.js" async=""></script><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#0A0A0A"/><meta name="theme-color" media="(prefers-color-scheme: light)" content="#fff"/><title>Introducing Qwen1.5 — Zen LM Blog | Zen LM</title><meta name="description" content="GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD"/><meta property="og:title" content="Zen LM - Open Foundation Models"/><meta property="og:description" content="Zen AI model family by Hanzo AI. 14 frontier models from 4B to 1T+ parameters for code, reasoning, vision, and multimodal tasks."/><meta property="og:url" content="https://zenlm.org"/><meta property="og:site_name" content="Zen LM"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="@zenlmorg"/><meta name="twitter:creator" content="@zenlmorg"/><meta name="twitter:title" content="Zen LM - Open Foundation Models"/><meta name="twitter:description" content="Zen AI model family by Hanzo AI. 14 frontier models from 4B to 1T+ parameters for code, reasoning, vision, and multimodal tasks."/><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="antialiased"><div hidden=""><!--$--><!--/$--></div><script>((a,b,c,d,e,f,g,h)=>{let i=document.documentElement,j=["light","dark"];function k(b){var c;(Array.isArray(a)?a:[a]).forEach(a=>{let c="class"===a,d=c&&f?e.map(a=>f[a]||a):e;c?(i.classList.remove(...d),i.classList.add(f&&f[b]?f[b]:b)):i.setAttribute(a,b)}),c=b,h&&j.includes(c)&&(i.style.colorScheme=c)}if(d)k(d);else try{let a=localStorage.getItem(b)||c,d=g&&"system"===a?window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":a;k(d)}catch(a){}})("class","theme","system",null,["light","dark"],null,true,true)</script><!--$--><div data-closed="" role="presentation" hidden="" style="user-select:none;-webkit-user-select:none" class="fixed inset-0 z-50 backdrop-blur-xs bg-fd-overlay data-open:animate-fd-fade-in data-closed:animate-fd-fade-out"></div><div class="bg-fd-secondary/50 p-3 empty:hidden"></div><!--/$--><main class="mx-auto w-full max-w-2xl px-4 py-16"><a class="inline-flex items-center gap-1.5 text-sm text-fd-muted-foreground hover:text-fd-foreground mb-10 transition-colors" href="/blog">← Back to Blog</a><div class="mb-8"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">February 3, 2024</time><h1 class="text-3xl font-bold mt-2 mb-3">Introducing Qwen1.5</h1><p class="text-fd-muted-foreground text-lg mb-4">GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD</p><div class="flex items-center gap-3 pt-4 border-t border-fd-border"><span class="text-sm text-fd-muted-foreground">By <!-- -->Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></div><div class="prose dark:prose-invert max-w-none"><p><a href="https://github.com/QwenLM/Qwen1.5" rel="noreferrer noopener" target="_blank">GITHUB</a> <a href="https://huggingface.co/Qwen" rel="noreferrer noopener" target="_blank">HUGGING FACE</a> <a href="https://modelscope.cn/organization/qwen" rel="noreferrer noopener" target="_blank">MODELSCOPE</a> <a href="https://huggingface.co/spaces/Qwen/Qwen1.5-72B-Chat" rel="noreferrer noopener" target="_blank">DEMO</a> <a href="https://discord.gg/yPEP2vHTu4" rel="noreferrer noopener" target="_blank">DISCORD</a></p>
<h1 class="flex scroll-m-28 flex-row items-center gap-2" id="introduction"><a data-card="" href="#introduction" class="peer">Introduction</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h1>
<p>In recent months, our focus has been on developing a “good” model while optimizing the developer experience. As we progress towards <strong>Qwen1.5</strong> , the next iteration in our Qwen series, this update arrives just before the Chinese New Year.</p>
<p>With Qwen1.5, we are open-sourcing base and chat models across six sizes: 0.5B, 1.8B, 4B, 7B, 14B, 32B, 72B, and 110B, and also an MoE model (see <a href="https://qwenlm.github.io/blog/qwen-moe/" rel="noreferrer noopener" target="_blank">blog</a> for more information). In line with tradition, we’re also providing quantized models, including Int4 and Int8 GPTQ models, as well as AWQ and GGUF quantized models. To enhance the developer experience, we’ve merged Qwen1.5’s code into Hugging Face transformers, making it accessible with <code>transformers&gt;=4.37.0</code> without needing <code>trust_remote_code</code>.</p>
<p>We’ve collaborated with frameworks like <a href="https://vllm.readthedocs.io/" rel="noreferrer noopener" target="_blank">vLLM</a>, <a href="https://github.com/sgl-project/sglang" rel="noreferrer noopener" target="_blank">SGLang</a> for deployment, <a href="https://github.com/casper-hansen/AutoAWQ" rel="noreferrer noopener" target="_blank">AutoAWQ</a>, <a href="https://github.com/AutoGPTQ/AutoGPTQ" rel="noreferrer noopener" target="_blank">AutoGPTQ</a> for quantization, <a href="https://github.com/OpenAccess-AI-Collective/axolotl" rel="noreferrer noopener" target="_blank">Axolotl</a>, <a href="https://github.com/hiyouga/LLaMA-Factory" rel="noreferrer noopener" target="_blank">LLaMA-Factory</a> for finetuning, and <a href="https://github.com/ggerganov/llama.cpp" rel="noreferrer noopener" target="_blank">llama.cpp</a> for local LLM inference, all of which now support Qwen1.5. The Qwen1.5 series is available on platforms such as <a href="https://ollama.ai/" rel="noreferrer noopener" target="_blank">Ollama</a> and <a href="https://lmstudio.ai/" rel="noreferrer noopener" target="_blank">LMStudio</a>. Additionally, API services are offered not only on DashScope but also on <a href="https://together.ai/" rel="noreferrer noopener" target="_blank">together.ai</a>, with global accessibility. Visit <a href="https://api.together.ai/" rel="noreferrer noopener" target="_blank">here</a> to get started, and we recommend trying out <a href="https://api.together.xyz/playground/chat/Qwen/Qwen1.5-72B-Chat" rel="noreferrer noopener" target="_blank">Qwen1.5-72B-chat</a>.</p>
<p>This release brings substantial improvements to the alignment of chat models with human preferences and enhanced multilingual capabilities. All models now uniformly support a context length of up to 32768 tokens. There have also been minor improvements in the quality of base language models that may benefit your finetuning endeavors. This step represents a small stride toward our objective of creating a truly “good” model.</p>
<h1 class="flex scroll-m-28 flex-row items-center gap-2" id="performance"><a data-card="" href="#performance" class="peer">Performance</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h1>
<p>To provide a better understanding of the performance of Qwen1.5, we have conducted a comprehensive evaluation of both base and chat models on different capabilities, including basic capabilities such as language understanding, coding, reasoning, multilingual capabilities, human preference, agent, retrieval-augmented generation (RAG), etc.</p>
<h2 class="flex scroll-m-28 flex-row items-center gap-2" id="basic-capabilities"><a data-card="" href="#basic-capabilities" class="peer">Basic Capabilities</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>To assess the basic capabilities of language models, we have conducted evaluations on traditional benchmarks, including MMLU (5-shot), C-Eval, Humaneval, GS8K, BBH, etc.</p>









































































































































<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Model</th><th>MMLU</th><th>C-Eval</th><th>GSM8K</th><th>MATH</th><th>HumanEval</th><th>MBPP</th><th>BBH</th><th>CMMLU</th></tr></thead><tbody><tr><td>GPT-4</td><td>86.4</td><td>69.9</td><td>92.0</td><td>45.8</td><td>67.0</td><td>61.8</td><td>86.7</td><td>71.0</td></tr><tr><td>Llama2-7B</td><td>46.8</td><td>32.5</td><td>16.7</td><td>3.3</td><td>12.8</td><td>20.8</td><td>38.2</td><td>31.8</td></tr><tr><td>Llama2-13B</td><td>55.0</td><td>41.4</td><td>29.6</td><td>5.0</td><td>18.9</td><td>30.3</td><td>45.6</td><td>38.4</td></tr><tr><td>Llama2-34B</td><td>62.6</td><td>-</td><td>42.2</td><td>6.2</td><td>22.6</td><td>33.0</td><td>44.1</td><td>-</td></tr><tr><td>Llama2-70B</td><td>69.8</td><td>50.1</td><td>54.4</td><td>10.6</td><td>23.7</td><td>37.7</td><td>58.4</td><td>53.6</td></tr><tr><td>Mistral-7B</td><td>64.1</td><td>47.4</td><td>47.5</td><td>11.3</td><td>27.4</td><td>38.6</td><td>56.7</td><td>44.7</td></tr><tr><td>Mixtral-8x7B</td><td>70.6</td><td>-</td><td>74.4</td><td>28.4</td><td>40.2</td><td>60.7</td><td>-</td><td>-</td></tr><tr><td>Qwen1.5-7B</td><td>61.0</td><td>74.1</td><td>62.5</td><td>20.3</td><td>36.0</td><td>37.4</td><td>40.2</td><td>73.1</td></tr><tr><td>Qwen1.5-14B</td><td>67.6</td><td>78.7</td><td>70.1</td><td>29.2</td><td>37.8</td><td>44.0</td><td>53.7</td><td>77.6</td></tr><tr><td>Qwen1.5-32B</td><td>73.4</td><td>83.5</td><td>77.4</td><td>36.1</td><td>37.2</td><td>49.4</td><td>66.8</td><td>82.3</td></tr><tr><td>Qwen1.5-72B</td><td>77.5</td><td>84.1</td><td>79.5</td><td>34.1</td><td>41.5</td><td>53.4</td><td>65.5</td><td>83.5</td></tr></tbody></table></div>
<p>At every model size, Qwen1.5 demonstrates strong performance across the diverse evaluation benchmarks. In particular, Qwen1.5-72B outperforms Llama2-70B across all benchmarks, showcasing its exceptional capabilities in language understanding, reasoning, and math.</p>
<p>In light of the recent surge in interest for small language models, we have compared Qwen1.5 with sizes smaller than 7 billion parameters, against the most outstanding small-scale models within the community. The results are shown below:</p>









































































































































<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Model</th><th>Non-Emb Params</th><th>MMLU</th><th>C-Eval</th><th>GSM8K</th><th>MATH</th><th>HumanEval</th><th>MBPP</th><th>BBH</th><th>CMMLU</th></tr></thead><tbody><tr><td>Tinyllama-1.1B</td><td>1.1B</td><td>24.3</td><td>25.0</td><td>2.3</td><td>0.7</td><td>6.7</td><td>19.9</td><td>28.8</td><td>24.0</td></tr><tr><td>Gemini-Nano-3B</td><td>-</td><td>-</td><td>-</td><td>22.8</td><td>-</td><td>-</td><td>27.2</td><td>42.4</td><td>-</td></tr><tr><td>StableLM-Zephyr-3B</td><td>2.7B</td><td>45.9</td><td>30.3</td><td>52.5</td><td>12.5</td><td>35.4</td><td>31.9</td><td>37.7</td><td>30.9</td></tr><tr><td>Phi-2</td><td>2.5B</td><td>52.7</td><td>23.4</td><td>57.2</td><td>3.5</td><td>47.6</td><td>55.0</td><td>43.4</td><td>24.2</td></tr><tr><td>MiniCPM-2B</td><td>2.4B</td><td>53.5</td><td>51.1</td><td>53.8</td><td>10.2</td><td>50.0</td><td>47.3</td><td>36.9</td><td>51.1</td></tr><tr><td>Gemma-2B</td><td>2.0B</td><td>42.3</td><td>-</td><td>17.7</td><td>11.8</td><td>22.0</td><td>29.2</td><td>35.2</td><td>-</td></tr><tr><td>Qwen1.5-0.5B</td><td>0.3B</td><td>39.2</td><td>50.5</td><td>22.0</td><td>3.1</td><td>12.2</td><td>6.8</td><td>18.3</td><td>46.6</td></tr><tr><td>Qwen1.5-1.8B</td><td>1.2B</td><td>46.8</td><td>59.7</td><td>38.4</td><td>10.1</td><td>20.1</td><td>18.0</td><td>24.2</td><td>57.8</td></tr><tr><td>Qwen1.5-4B</td><td>3.1B</td><td>56.1</td><td>67.6</td><td>57.0</td><td>10.0</td><td>25.6</td><td>29.2</td><td>32.5</td><td>66.7</td></tr><tr><td>Qwen1.5-MoE-A2.7B</td><td>2.0B</td><td>62.5</td><td>79.2</td><td>61.5</td><td>21.9</td><td>34.2</td><td>36.6</td><td>39.1</td><td>79.2</td></tr></tbody></table></div>
<p>We can confidently assert that Qwen1.5 base models under 7 billion parameters are highly competitive with the leading small-scale models in the community. In the future, we will continue to improve the quality of small models and exploring methods for effectively transferring the advanced capabilities inherent in larger models into the smaller ones.</p>
<h2 class="flex scroll-m-28 flex-row items-center gap-2" id="aligning-with-human-preference"><a data-card="" href="#aligning-with-human-preference" class="peer">Aligning with Human Preference</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>Alignment aims to enhance instruction-following capabilities of LLMs and help provide responses that are closely aligned with human preferences. Recognizing the significance of integrating human preferences into the learning process, we effectively employed techniques such as Direct Policy Optimization (DPO) and Proximal Policy Optimization (PPO) in aligning the latest Qwen series.</p>
<p>However, assessing the quality of such chat models poses a significant challenge. Admittedly, while comprehensive human evaluation is the optimal approach, it faces significant challenges pertaining to scalability and reproducibility. Therefore, we initially evaluate our models on two widely-used benchmarks, utilizing advanced LLMs as judges: MT-Bench and Alpaca-Eval. The results are presented below:</p>
<p>We notice there are non-negligible variance in the scores on MT-Bench. So we have three runs with different seeds in our results and we report the average score with standard deviation.</p>






























<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Models</th><th>MT-Bench</th><th>AlpacaEval 2.0</th></tr></thead><tbody><tr><td>Avg. Score</td><td>Win Rate</td><td>Length</td></tr><tr><td>Qwen1.5-72B-Chat</td><td>8.610.04 (8.67/8.61/8.56)</td><td>27.181.30</td></tr><tr><td>Qwen1.5-14B-Chat</td><td>7.910.11 (7.99/7.99/7.77)</td><td>19.71.12</td></tr><tr><td>Qwen1.5-7B-Chat</td><td>7.600.05 (7.58/7.55/7.66)</td><td>13.201.43</td></tr></tbody></table></div>
<p>Despite still significantly trailing behind GPT-4-Turbo, the largest open-source Qwen1.5 model, Qwen1.5-72B-Chat, exhibits superior performance, surpassing Claude-2.1, GPT-3.5-Turbo-0613, Mixtral-8x7b-instruct, and TULU 2 DPO 70B, being on par with Mistral Medium, on both MT-Bench and Alpaca-Eval v2.</p>
<p>Furthermore, although the scoring of LLM Judges may seemingly correlate with the lengths of responses, our observations indicate that our models do not generate lengthy responses to manipulate the bias of LLM judges. The average length of Qwen1.5-Chat on AlpacaEval 2.0 is only 1618, which aligns with the length of GPT-4 and is shorter than that of GPT-4-Turbo. Additionally, our experiments with our web service and app also reveal that users prefer the majority of responses from the new chat models.</p>
<h2 class="flex scroll-m-28 flex-row items-center gap-2" id="multilingual-understanding-of-base-models"><a data-card="" href="#multilingual-understanding-of-base-models" class="peer">Multilingual Understanding of Base Models</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>We have carefully selected a diverse set of 12 languages from Europe, East Asia, and Southeast Asia to thoroughly evaluate the multilingual capabilities of our foundational model. In order to accomplish this, we have curated test sets from the community’s open-source repositories, covering four distinct dimensions: Exams, Understanding, Translation, and Math. The table below provides detailed information about each test set, including evaluation settings, metrics, and the languages they encompass:</p>

































































<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Dataset</th><th>Category</th><th>Method/Metric</th><th>Languages</th></tr></thead><tbody><tr><td>MMLU-multi</td><td>Exams</td><td>5-shot/Acc</td><td>ar, es, fr, pt, de, it, ru, ja, ko, id</td></tr><tr><td>M3Exams</td><td>Exams</td><td>5-shot/Acc</td><td>pt, it, vi, th</td></tr><tr><td>BELEBELE</td><td>Understanding</td><td>5-shot/Acc</td><td>ar, es, fr, pt, de, it, ru, ja, ko, vi, th, id</td></tr><tr><td>XWinograd</td><td>Understanding</td><td>5-shot/Acc</td><td>fr, pt, ru, ja</td></tr><tr><td>XCOPA</td><td>Understanding</td><td>5-shot/Acc</td><td>vi, id, th</td></tr><tr><td>PAWS-X</td><td>Understanding</td><td>5-shot/Acc</td><td>es, fr, de, ja, ko</td></tr><tr><td>XStoryCloze</td><td>Understanding</td><td>0-shot/Acc</td><td>ar, es, ru, id</td></tr><tr><td>Flores(zh/en↔xx)</td><td>Translation</td><td>5-shot/BLEU</td><td>ar, es, fr, pt, de, it, ru, ja, ko, vi, th, id</td></tr><tr><td>MGSM</td><td>Math</td><td>8-shot/Acc</td><td>es, fr, ru, de, ja, th</td></tr></tbody></table></div>
<p>The detailed results are demonstrated below:</p>














































































































<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Models</th><th>Exams</th><th>Understanding</th><th>Math</th><th>Translation</th></tr></thead><tbody><tr><td>GPT-3.5</td><td>52.24</td><td>71.84</td><td>32.80</td><td>31.85</td></tr><tr><td>GPT-4</td><td>71.64</td><td>83.82</td><td>80.13</td><td>34.37</td></tr><tr><td>Llama2-7B</td><td>34.03</td><td>50.13</td><td>9.40</td><td>22.19</td></tr><tr><td>Llama2-13B</td><td>39.55</td><td>57.26</td><td>16.80</td><td>25.89</td></tr><tr><td>Llama2-70B</td><td>55.88</td><td>73.19</td><td>40.20</td><td>31.56</td></tr><tr><td>Mistral-7B</td><td>47.12</td><td>63.30</td><td>26.33</td><td>23.33</td></tr><tr><td>Mixtral-8x7B</td><td>56.08</td><td>70.70</td><td>45.00</td><td>29.78</td></tr><tr><td>Qwen1.5-0.5B</td><td>26.98</td><td>44.08</td><td>3.13</td><td>9.17</td></tr><tr><td>Qwen1.5-1.8B</td><td>33.57</td><td>48.37</td><td>6.47</td><td>16.19</td></tr><tr><td>Qwen1.5-4B</td><td>41.43</td><td>59.76</td><td>21.33</td><td>23.34</td></tr><tr><td>Qwen1.5-MoE-A2.7B</td><td>44.54</td><td>61.08</td><td>30.20</td><td>27.35</td></tr><tr><td>Qwen1.5-7B</td><td>47.70</td><td>67.63</td><td>37.27</td><td>28.36</td></tr><tr><td>Qwen1.5-14B</td><td>55.72</td><td>74.10</td><td>49.93</td><td>31.69</td></tr><tr><td>Qwen1.5-72B</td><td>66.35</td><td>78.16</td><td>61.67</td><td>35.57</td></tr></tbody></table></div>
<p>The base models of Qwen1.5 showcase impressive multilingual capabilities, as demonstrated by its performance across a diverse set of 12 languages. In evaluations covering various dimensions such as exams, understanding, translation, and math, Qwen1.5 consistently delivers strong results. From languages like Arabic, Spanish, and French to Japanese, Korean, and Thai, Qwen1.5 demonstrates its ability to comprehend and generate high-quality content across different linguistic contexts. We compared Qwen1.5-72B-Chat with GPT-3.5, and the results are shown below:</p>
<p>These results demonstrate the strong multilingual capabilities of Qwen1.5 chat models, which can serve downstream applications, such as translation, language understanding, and multilingual chat. Also, we believe that the improvements in multilingual capabilities can also level up the general capabilities.</p>
<h2 class="flex scroll-m-28 flex-row items-center gap-2" id="support-of-long-context"><a data-card="" href="#support-of-long-context" class="peer">Support of Long Context</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>With the increasing demand for long-context understanding, we have expanded the capability of all models to support contexts up to 32K tokens. We have evaluated the performance of Qwen1.5 models on the <a href="https://github.com/OpenLMLab/LEval" rel="noreferrer noopener" target="_blank">L-Eval benchmark</a>, which measures the ability of models to generate responses based on long context. The results are shown below:</p>








































































































<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Models</th><th>Coursera</th><th>GSM</th><th>QuALITY</th><th>TOEFL</th><th>SFiction</th><th>Avg.</th></tr></thead><tbody><tr><td>GPT3.5-turbo-16k</td><td>63.51</td><td>84.00</td><td>61.38</td><td>78.43</td><td>64.84</td><td>70.43</td></tr><tr><td>Claude1.3-100k</td><td>60.03</td><td>88.00</td><td>73.76</td><td>83.64</td><td>72.65</td><td>75.62</td></tr><tr><td>GPT4-32k</td><td>75.58</td><td>96.00</td><td>82.17</td><td>84.38</td><td>74.99</td><td>82.62</td></tr><tr><td>Qwen-72B-Chat</td><td>58.13</td><td>76.00</td><td>77.22</td><td>86.24</td><td>69.53</td><td>73.42</td></tr><tr><td>Qwen1.5-0.5B-Chat</td><td>30.81</td><td>6.00</td><td>34.16</td><td>40.52</td><td>49.22</td><td>32.14</td></tr><tr><td>Qwen1.5-1.8B-Chat</td><td>39.24</td><td>37.00</td><td>42.08</td><td>55.76</td><td>44.53</td><td>43.72</td></tr><tr><td>Qwen1.5-4B-Chat</td><td>54.94</td><td>47.00</td><td>57.92</td><td>69.15</td><td>56.25</td><td>57.05</td></tr><tr><td>Qwen1.5-7B-Chat</td><td>59.74</td><td>60.00</td><td>64.36</td><td>79.18</td><td>62.50</td><td>65.16</td></tr><tr><td>Qwen1.5-14B-Chat</td><td>69.04</td><td>79.00</td><td>74.75</td><td>83.64</td><td>75.78</td><td>76.44</td></tr><tr><td>Qwen1.5-72B-Chat</td><td>71.95</td><td>82.00</td><td>77.72</td><td>85.50</td><td>73.44</td><td>78.12</td></tr></tbody></table></div>
<p>In terms of the performance, even a small model like Qwen1.5-7B-Chat demonstrates competitive performance against GPT-3.5 on 4 out of 5 tasks. Our best model, Qwen1.5-72B-Chat, significantly outperforms GPT3.5-turbo-16k and only slightly falls behind GPT4-32k. These results highlight our outstanding performance within 32K tokens, yet they do not imply that our models are limited to supporting only 32K tokens. You can modify <code>max_position_embedding</code> and <code>sliding_window</code> in <code>config.json</code> to a larger value to see if the model performance is still satisfactory for your tasks.</p>
<h2 class="flex scroll-m-28 flex-row items-center gap-2" id="capabilities-to-connect-with-external-systems"><a data-card="" href="#capabilities-to-connect-with-external-systems" class="peer">Capabilities to Connect with External Systems</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>Large language models (LLMs) are popular in part due to their ability to integrate external knowledge and tools. Retrieval-Augmented Generation (RAG) has gained traction as it mitigates common LLM issues like hallucination, real-time data shortage, and private information handling. Additionally, strong LLMs typically excel at using APIs and tools via function calling, making them ideal for serving as AI agents.</p>
<p>We first assess the performance of Qwen1.5-Chat on <a href="https://arxiv.org/abs/2309.01431" rel="noreferrer noopener" target="_blank">RGB</a>, an RAG benchmark for which we have not performed any specific optimization:</p>
<h2 class="flex scroll-m-28 flex-row items-center gap-2" id="rgb-english-benchmark-for-retrieval-augmented-generation"><a data-card="" href="#rgb-english-benchmark-for-retrieval-augmented-generation" class="peer">RGB English Benchmark for Retrieval-Augmented Generation</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>Models| Noise 0.8 (Acc.↑)| Rejection 1.0 (Acc.↑)| Integration 0.4 (Acc.↑)| Counterfactual (Acc.↑)<br/>
GPT4-Turbo| 85.67| 47.33| 60.00| 90.00<br/>
GPT3.5-Turbo| 74.33| 27.67| 47.00| 21.00<br/>
Llama2-70B-Chat| 82.00| 31.00| 56.00| 15.00<br/>
Mistral-7B-Instruct-v0.2| 82.00| 31.00| 56.00| 15.00<br/>
Mixtral-8x7B-Instruct-v0.1| 82.67| 37.00| 67.00| 8.00<br/>
Qwen1.5-7B-Chat| 77.67| 25.00| 52.00| 9.00<br/>
Qwen1.5-14B-Chat| 80.67| 24.00| 60.00| 8.00<br/>
Qwen1.5-72B-Chat| 81.67| 48.67| 61.00| 28.00<br/>
RGB Chinese Benchmark for Retrieval-Augmented Generation<br/>
Models| Noise 0.8 (Acc.↑)| Rejection 1.0 (Acc.↑)| Integration 0.4 (Acc.↑)| Counterfactual (Acc.↑)<br/>
GPT4-Turbo| 75.00| 38.67| 63.00| 90.00<br/>
GPT3.5-Turbo| 69.00| 13.00| 55.00| 25.00<br/>
Llama2-70B-Chat| 28.00| 17.00| 32.00| 8.00<br/>
Mistral-7B-Instruct-v0.2| 54.67| 28.67| 37.00| 4.00<br/>
Mixtral-8x7B-Instruct-v0.1| 27.33| 4.00| 24.00| 4.00<br/>
Qwen1.5-7B-Chat| 71.00| 10.33| 54.00| 20.00<br/>
Qwen1.5-14B-Chat| 75.00| 16.67| 55.00| 22.00<br/>
Qwen1.5-72B-Chat| 76.00| 51.00| 66.00| 44.00</p>
<p>We then assess Qwen’s capacity to function as a general-purpose agent by testing it on the <a href="https://open-compass.github.io/T-Eval/" rel="noreferrer noopener" target="_blank">T-Eval</a> benchmark. None of the Qwen models have undergone any optimization tailored specifically for this benchmark:</p>
<h2 class="flex scroll-m-28 flex-row items-center gap-2" id="agent-performance-on-t-eval-english"><a data-card="" href="#agent-performance-on-t-eval-english" class="peer">Agent Performance on T-Eval English</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>Models| Overall| Instruct| Plan| Reason| Retrieve| Understand| Review<br/>
GPT4-Turbo| 86.4| 96.3| 87.8| 65.3| 88.9| 85.8| 94.5<br/>
Llama-2-70B-Chat| 58.59| 77.80| 63.75| 39.07| 51.35| 50.34| 69.20<br/>
Mistral-7B-Instruct-v0.2| 46.68| 63.57| 60.88| 32.59| 17.58| 38.08| 67.35<br/>
Mixtral-8x7B-Instruct-v0.1| 62.15| 42.39| 46.48| 60.35| 76.69| 73.70| 73.31<br/>
Qwen1.5-7B-Chat| 59.67| 71.12| 62.95| 37.60| 61.17| 53.75| 71.46<br/>
Qwen1.5-14B-Chat| 71.77| 86.16| 73.09| 49.51| 72.07| 66.03| 83.78<br/>
Qwen1.5-72B-Chat| 76.69| 80.96| 83.12| 56.89| 80.17| 76.68| 82.34<br/>
Agent Performance on T-Eval Chinese<br/>
Models| Overall| Instruct| Plan| Reason| Retrieve| Understand| Review<br/>
GPT4-Turbo| 85.9| 97.6| 87.0| 68.4| 89.2| 86.8| 86.0<br/>
Llama-2-70B-Chat| 51.15| 53.78| 56.65| 34.27| 48.24| 50.49| 63.45<br/>
Mistral-7B-Instruct-v0.2| 46.26| 49.64| 61.82| 36.17| 20.26| 47.25| 62.42<br/>
Mixtral-8x7B-Instruct-v0.1| 62.77| 26.38| 60.79| 62.02| 76.60| 77.74| 73.10<br/>
Qwen1.5-7B-Chat| 53.15| 60.56| 62.31| 42.07| 55.28| 55.76| 42.92<br/>
Qwen1.5-14B-Chat| 64.85| 84.25| 64.77| 54.68| 72.35| 68.88| 44.15<br/>
Qwen1.5-72B-Chat| 72.88| 97.50| 80.83| 58.11| 76.14| 71.94| 52.77</p>
<p>To test the capabilities of tool using, also known as function calling, we follow our previous practice and use our opensourced evaluation <a href="https://github.com/QwenLM/Qwen/blob/main/eval/evaluate_plugin.py" rel="noreferrer noopener" target="_blank">benchmark</a> for assessing the models’ ability to appropriately select and use tools:</p>
<h2 class="flex scroll-m-28 flex-row items-center gap-2" id="tool-use-benchmark"><a data-card="" href="#tool-use-benchmark" class="peer">Tool-Use Benchmark</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>Models| Tool Selection (Acc.↑)| Tool Input (Rouge-L↑)| False Positive (Acc.↑)<br/>
GPT-4| 98.0| 95.3| 76.1<br/>
GPT-3.5| 74.5| 80.7| 19.4<br/>
Llama-2-70B-Chat| 88.54| 70.36| 0.37<br/>
Mistral-7B-Instruct-v0.2| 94.79| 82.81| 6.34<br/>
Mixtral-8x7B-Instruct-v0.1| 99.31| 94.46| 31.34<br/>
Qwen1.5-7B-Chat| 95.83| 89.48| 92.54<br/>
Qwen1.5-14B-Chat| 93.06| 88.74| 92.91<br/>
Qwen1.5-72B-Chat| 95.14| 91.14| 98.51</p>
<p>Finally, since the Python code interpreter has emerged as an increasingly powerful tool for advanced LLMs, we also evaluate our models’ capability in utilizing this tool on our previously open-sourced <a href="https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark" rel="noreferrer noopener" target="_blank">benchmark</a>:</p>
<h2 class="flex scroll-m-28 flex-row items-center gap-2" id="code-interpreter-benchmark"><a data-card="" href="#code-interpreter-benchmark" class="peer">Code Interpreter Benchmark</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>Models| Accuracy of Code Execution Results (%)| Executable Rate of Code (%)<br/>
Math↑| Visualization-Hard↑| Visualization-Easy↑| General↑<br/>
GPT-4| 82.8| 66.7| 60.8| 82.8<br/>
GPT-3.5| 47.3| 33.3| 55.7| 74.1<br/>
Mistral-7B-Instruct-v0.2| 25.5| 19.1| 44.3| 62.1<br/>
Mixtral-8x7B-Instruct-v0.1| 47.8| 33.3| 54.4| 60.3<br/>
Qwen1.5-7B-Chat| 54.0| 35.7| 36.7| 65.5<br/>
Qwen1.5-14B-Chat| 62.1| 46.4| 48.1| 70.6<br/>
Qwen1.5-72B-Chat| 73.1| 52.3| 50.6| 87.9</p>
<p>Larger Qwen1.5-Chat models generally outperform smaller ones, nearing GPT-4’s tool-use performance. However, in code interpreter tasks like math problem-solving and visualization, even the largest Qwen1.5-72B-Chat model lags significantly behind GPT-4 due to coding capabilities. We aim in future versions to enhance the coding capabilities of all Qwen models during both pre-training and alignment.</p>
<h1 class="flex scroll-m-28 flex-row items-center gap-2" id="develop-with-qwen15"><a data-card="" href="#develop-with-qwen15" class="peer">Develop with Qwen1.5</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h1>
<p>The biggest difference in Qwen1.5 is the integration of Qwen1.5 to Hugging Face transformers. Since 4.37.0, you can use Qwen1.5 without our custom code, which means that you can load the model like the following:</p>
<figure dir="ltr" class="my-4 bg-fd-card rounded-xl shiki relative border shadow-sm not-prose overflow-hidden text-sm shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e" tabindex="-1"><div class="empty:hidden absolute top-3 right-2 z-2 backdrop-blur-lg rounded-lg text-fd-muted-foreground"><button type="button" class="inline-flex items-center justify-center rounded-md text-sm font-medium transition-colors duration-100 disabled:pointer-events-none disabled:opacity-50 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-fd-ring p-1 [&amp;_svg]:size-4 hover:text-fd-accent-foreground data-checked:text-fd-accent-foreground" aria-label="Copy Text"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-clipboard" aria-hidden="true"><rect width="8" height="4" x="8" y="2" rx="1" ry="1"></rect><path d="M16 4h2a2 2 0 0 1 2 2v14a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V6a2 2 0 0 1 2-2h2"></path></svg></button></div><div role="region" tabindex="0" class="text-[0.8125rem] py-3.5 overflow-auto max-h-[600px] fd-scroll-container focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-inset focus-visible:ring-fd-ring" style="--padding-right:calc(var(--spacing) * 8)"><pre class="min-w-full w-max *:flex *:flex-col"><code><span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">    from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> transformers </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> AutoModelForCausalLM</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">    # This is what we previously used</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> AutoModelForCausalLM.from_pretrained(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">&quot;Qwen/Qwen-7B-Chat&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70">device_map</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">&quot;auto&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70">trust_remote_code</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D">    # This is what you can use now</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> AutoModelForCausalLM.from_pretrained(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">&quot;Qwen/Qwen1.5-7B-Chat&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70">device_map</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">&quot;auto&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    </span></span></code></pre></div></figure>
<p>The usage of Qwen1.5 for chat is different from the previous version. You can use the following code to chat with Qwen1.5:</p>
<figure dir="ltr" class="my-4 bg-fd-card rounded-xl shiki relative border shadow-sm not-prose overflow-hidden text-sm shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e" tabindex="-1"><div class="empty:hidden absolute top-3 right-2 z-2 backdrop-blur-lg rounded-lg text-fd-muted-foreground"><button type="button" class="inline-flex items-center justify-center rounded-md text-sm font-medium transition-colors duration-100 disabled:pointer-events-none disabled:opacity-50 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-fd-ring p-1 [&amp;_svg]:size-4 hover:text-fd-accent-foreground data-checked:text-fd-accent-foreground" aria-label="Copy Text"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-clipboard" aria-hidden="true"><rect width="8" height="4" x="8" y="2" rx="1" ry="1"></rect><path d="M16 4h2a2 2 0 0 1 2 2v14a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V6a2 2 0 0 1 2-2h2"></path></svg></button></div><div role="region" tabindex="0" class="text-[0.8125rem] py-3.5 overflow-auto max-h-[600px] fd-scroll-container focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-inset focus-visible:ring-fd-ring" style="--padding-right:calc(var(--spacing) * 8)"><pre class="min-w-full w-max *:flex *:flex-col"><code><span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">    from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> transformers </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> AutoModelForCausalLM, AutoTokenizer</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    device </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF"> &quot;cuda&quot;</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D"> # the device to load the model onto</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    </span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> AutoModelForCausalLM.from_pretrained(</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">        &quot;Qwen/Qwen1.5-14B-Chat-AWQ&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70">        device_map</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">&quot;auto&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    )</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    tokenizer </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> AutoTokenizer.from_pretrained(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">&quot;Qwen/Qwen1.5-14B-Chat-AWQ&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    </span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    prompt </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF"> &quot;Give me a short introduction to large language model.&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    messages </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> [</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">        {</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">&quot;role&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">&quot;system&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">&quot;content&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">&quot;You are a helpful assistant.&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">},</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">        {</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">&quot;role&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">&quot;user&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">&quot;content&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">: prompt}</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    ]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    text </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> tokenizer.apply_chat_template(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">        messages,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70">        tokenize</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70">        add_generation_prompt</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">True</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    )</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    model_inputs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> tokenizer([text], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70">return_tensors</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">&quot;pt&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">).to(device)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    </span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    generated_ids </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> model.generate(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">        model_inputs.input_ids,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70">        max_new_tokens</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">512</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    )</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    generated_ids </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> [</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">        output_ids[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">len</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(input_ids):] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> input_ids, output_ids </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF"> zip</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">(model_inputs.input_ids, generated_ids)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    ]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    </span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    response </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8"> tokenizer.batch_decode(generated_ids, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70">skip_special_tokens</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">)[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    </span></span></code></pre></div></figure>
<p>For chat models, we no longer use a specific <code>model.chat()</code> method, but instead we use <code>model.generate()</code> with the chat template written in <code>tokenizer_config.json</code> so that we can use <code>tokenizer.apply_chat_template()</code> to generate the input, and we use <code>eos_token</code> to control when to stop the generation.</p>
<p>We also provide AWQ models and GPTQ models (including Int4 and Int8 models) for you to use Qwen1.5 in low-resource or deployment scenarios. As Huggingface transformers supports <a href="https://github.com/casper-hansen/AutoAWQ" rel="noreferrer noopener" target="_blank">AWQ</a> and <a href="https://github.com/AutoGPTQ/AutoGPTQ" rel="noreferrer noopener" target="_blank">GPTQ</a>, you can use them in the same way above only with the corresponding model names.</p>
<p>Furthermore, we have integrated our code to popular inference frameworks so that you can deploy your model easily. Now <code>vLLM&gt;=0.3.0</code> and <code>SGLang&gt;=0.1.11</code> officially support Qwen1.5. Check their official github repos and docs to learn about the detailed usage. Here we demonstrate an example to show how to use vLLM to build an OpenAI-API compatible interface for our model:</p>
<figure dir="ltr" class="my-4 bg-fd-card rounded-xl shiki relative border shadow-sm not-prose overflow-hidden text-sm shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e" tabindex="-1"><div class="empty:hidden absolute top-3 right-2 z-2 backdrop-blur-lg rounded-lg text-fd-muted-foreground"><button type="button" class="inline-flex items-center justify-center rounded-md text-sm font-medium transition-colors duration-100 disabled:pointer-events-none disabled:opacity-50 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-fd-ring p-1 [&amp;_svg]:size-4 hover:text-fd-accent-foreground data-checked:text-fd-accent-foreground" aria-label="Copy Text"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-clipboard" aria-hidden="true"><rect width="8" height="4" x="8" y="2" rx="1" ry="1"></rect><path d="M16 4h2a2 2 0 0 1 2 2v14a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V6a2 2 0 0 1 2-2h2"></path></svg></button></div><div role="region" tabindex="0" class="text-[0.8125rem] py-3.5 overflow-auto max-h-[600px] fd-scroll-container focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-inset focus-visible:ring-fd-ring" style="--padding-right:calc(var(--spacing) * 8)"><pre class="min-w-full w-max *:flex *:flex-col"><code><span class="line"><span></span></span>
<span class="line"><span>    python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen1.5-7B-Chat</span></span>
<span class="line"><span>    </span></span></code></pre></div></figure>
<figure dir="ltr" class="my-4 bg-fd-card rounded-xl shiki relative border shadow-sm not-prose overflow-hidden text-sm shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e" tabindex="-1"><div class="empty:hidden absolute top-3 right-2 z-2 backdrop-blur-lg rounded-lg text-fd-muted-foreground"><button type="button" class="inline-flex items-center justify-center rounded-md text-sm font-medium transition-colors duration-100 disabled:pointer-events-none disabled:opacity-50 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-fd-ring p-1 [&amp;_svg]:size-4 hover:text-fd-accent-foreground data-checked:text-fd-accent-foreground" aria-label="Copy Text"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-clipboard" aria-hidden="true"><rect width="8" height="4" x="8" y="2" rx="1" ry="1"></rect><path d="M16 4h2a2 2 0 0 1 2 2v14a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V6a2 2 0 0 1 2-2h2"></path></svg></button></div><div role="region" tabindex="0" class="text-[0.8125rem] py-3.5 overflow-auto max-h-[600px] fd-scroll-container focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-inset focus-visible:ring-fd-ring" style="--padding-right:calc(var(--spacing) * 8)"><pre class="min-w-full w-max *:flex *:flex-col"><code><span class="line"></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0">    curl</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF"> http://localhost:8000/v1/chat/completions</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF"> \</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">        -H</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF"> &quot;Content-Type: application/json&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF"> \</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF">        -d</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF"> &#x27;{</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">        &quot;model&quot;: &quot;Qwen/Qwen1.5-7B-Chat&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">        &quot;messages&quot;: [</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Tell me something about large language models.&quot;}</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">        ]</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF">        }&#x27;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8">    </span></span></code></pre></div></figure>
<p>For users to run LLM locally, llama.cpp also provides support to Qwen1.5, and we officially provide quantized models in the GGUF format in our HF model hub. You can use the following code to run Qwen1.5 in llama.cpp:</p>
<figure dir="ltr" class="my-4 bg-fd-card rounded-xl shiki relative border shadow-sm not-prose overflow-hidden text-sm shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e" tabindex="-1"><div class="empty:hidden absolute top-3 right-2 z-2 backdrop-blur-lg rounded-lg text-fd-muted-foreground"><button type="button" class="inline-flex items-center justify-center rounded-md text-sm font-medium transition-colors duration-100 disabled:pointer-events-none disabled:opacity-50 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-fd-ring p-1 [&amp;_svg]:size-4 hover:text-fd-accent-foreground data-checked:text-fd-accent-foreground" aria-label="Copy Text"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-clipboard" aria-hidden="true"><rect width="8" height="4" x="8" y="2" rx="1" ry="1"></rect><path d="M16 4h2a2 2 0 0 1 2 2v14a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V6a2 2 0 0 1 2-2h2"></path></svg></button></div><div role="region" tabindex="0" class="text-[0.8125rem] py-3.5 overflow-auto max-h-[600px] fd-scroll-container focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-inset focus-visible:ring-fd-ring" style="--padding-right:calc(var(--spacing) * 8)"><pre class="min-w-full w-max *:flex *:flex-col"><code><span class="line"><span></span></span>
<span class="line"><span>    ./main -m qwen1.5-7b-chat-q2_k.gguf -n 512 --color -i -cml -f prompts/chat-with-qwen.txt</span></span>
<span class="line"><span>    </span></span></code></pre></div></figure>
<p>Besides, you can use the GGUF file with Ollama. Thanks to the support of <a href="https://ollama.ai/" rel="noreferrer noopener" target="_blank">Ollama</a>, you can now directly use one line of command:</p>
<figure dir="ltr" class="my-4 bg-fd-card rounded-xl shiki relative border shadow-sm not-prose overflow-hidden text-sm shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e" tabindex="-1"><div class="empty:hidden absolute top-3 right-2 z-2 backdrop-blur-lg rounded-lg text-fd-muted-foreground"><button type="button" class="inline-flex items-center justify-center rounded-md text-sm font-medium transition-colors duration-100 disabled:pointer-events-none disabled:opacity-50 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-fd-ring p-1 [&amp;_svg]:size-4 hover:text-fd-accent-foreground data-checked:text-fd-accent-foreground" aria-label="Copy Text"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-clipboard" aria-hidden="true"><rect width="8" height="4" x="8" y="2" rx="1" ry="1"></rect><path d="M16 4h2a2 2 0 0 1 2 2v14a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V6a2 2 0 0 1 2-2h2"></path></svg></button></div><div role="region" tabindex="0" class="text-[0.8125rem] py-3.5 overflow-auto max-h-[600px] fd-scroll-container focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-inset focus-visible:ring-fd-ring" style="--padding-right:calc(var(--spacing) * 8)"><pre class="min-w-full w-max *:flex *:flex-col"><code><span class="line"><span></span></span>
<span class="line"><span>    ollama run qwen</span></span>
<span class="line"><span>    </span></span></code></pre></div></figure>
<p>Or you can use the GGUF file to play with <a href="https://github.com/Mozilla-Ocho/llamafile" rel="noreferrer noopener" target="_blank">llamafile</a> to run our models with a single file.</p>
<p>To make a web demo locally, we advise you to use <a href="https://github.com/oobabooga/text-generation-webui" rel="noreferrer noopener" target="_blank">Text generation web UI</a> which is very easy to use.</p>
<p>For advanced developers that hope to train better or more suitable models for themselves, such as post-training, Qwen1.5 is supported by Hugging face <code>trainer</code> and Peft. Also, there are easy-to-use frameworks that support both supervised finetuning (SFT) and alignment (PPO, DPO, etc.). Now, both <a href="https://github.com/hiyouga/LLaMA-Factory" rel="noreferrer noopener" target="_blank">LLaMA-Factory</a> and <a href="https://github.com/OpenAccess-AI-Collective/axolotl" rel="noreferrer noopener" target="_blank">Axolotl</a> have supported the training of Qwen1.5. We advise you to turn to their official github repos and docs for more advanced usages.</p>
<p>If you would like to use Qwen1.5 for downstream applications, such as RAG, tool use, agent, you can now build OpenAI-API compatible API or run local models for famous frameworks, e.g., <a href="https://www.llamaindex.ai/" rel="noreferrer noopener" target="_blank">LlamaIndex</a>, <a href="https://www.langchain.com/" rel="noreferrer noopener" target="_blank">LangChain</a>, <a href="https://www.crewai.io/" rel="noreferrer noopener" target="_blank">CrewAI</a>.</p>
<p>Overall, as we care about your developing experience, we not only have tried our best to provide good models to the community but also have made efforts to make things easier for all of you. We hope that you can enjoy using Qwen1.5 and that it can help you with your tasks of either research or applications.</p>
<h1 class="flex scroll-m-28 flex-row items-center gap-2" id="conclusion"><a data-card="" href="#conclusion" class="peer">Conclusion</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h1>
<p>We are excited to introduce Qwen1.5, the next version of our Qwen series. In this release, we have opensourced both base and chat models of 6 sizes, including 0.5B, 1.8B, 4B, 7B, 14B, and 72B, and we have also provided quantized models. We have merged our code of Qwen1.5 to Hugging face transformers, and you can directly use it with <code>transformers&gt;=4.37.0</code> without <code>trust_remote_code</code>. Additionally, we have had frameworks, e.g., vLLM, SGLang, AutoGPTQ, etc., supported Qwen1.5. We believe from now on, using our models will be much easier. We believe that this release is though a small step towards model quality, but it is a big step towards developer experience. Hope you like it and enjoy using it. Join our <a href="https://discord.gg/yPEP2vHTu4" rel="noreferrer noopener" target="_blank">Discord</a> or <a href="https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png" rel="noreferrer noopener" target="_blank">WeChat</a> to share your experience, comments, or whatever you like with us. We are looking forward to hearing from you.</p>
<h1 class="flex scroll-m-28 flex-row items-center gap-2" id="citation"><a data-card="" href="#citation" class="peer">Citation</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h1>
<figure dir="ltr" class="my-4 bg-fd-card rounded-xl shiki relative border shadow-sm not-prose overflow-hidden text-sm shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e" tabindex="-1"><div class="empty:hidden absolute top-3 right-2 z-2 backdrop-blur-lg rounded-lg text-fd-muted-foreground"><button type="button" class="inline-flex items-center justify-center rounded-md text-sm font-medium transition-colors duration-100 disabled:pointer-events-none disabled:opacity-50 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-fd-ring p-1 [&amp;_svg]:size-4 hover:text-fd-accent-foreground data-checked:text-fd-accent-foreground" aria-label="Copy Text"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-clipboard" aria-hidden="true"><rect width="8" height="4" x="8" y="2" rx="1" ry="1"></rect><path d="M16 4h2a2 2 0 0 1 2 2v14a2 2 0 0 1-2 2H6a2 2 0 0 1-2-2V6a2 2 0 0 1 2-2h2"></path></svg></button></div><div role="region" tabindex="0" class="text-[0.8125rem] py-3.5 overflow-auto max-h-[600px] fd-scroll-container focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-inset focus-visible:ring-fd-ring" style="--padding-right:calc(var(--spacing) * 8)"><pre class="min-w-full w-max *:flex *:flex-col"><code><span class="line"><span></span></span>
<span class="line"><span>    @misc{qwen1.5,</span></span>
<span class="line"><span>        title = {Introducing Qwen1.5},</span></span>
<span class="line"><span>        url = {https://qwenlm.github.io/blog/qwen1.5/},</span></span>
<span class="line"><span>        author = {Qwen Team},</span></span>
<span class="line"><span>        month = {February},</span></span>
<span class="line"><span>        year = {2024}</span></span>
<span class="line"><span>    }</span></span>
<span class="line"><span>    </span></span></code></pre></div></figure></div></main><!--$--><!--/$--><script src="/_next/static/chunks/e83606e8fa9cc796.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[106,[\"/_next/static/chunks/59d0ad1b64f8544e.js\"],\"RootProvider\"]\n3:I[53113,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"default\"]\n4:I[73211,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"default\"]\n5:I[10086,[\"/_next/static/chunks/59d0ad1b64f8544e.js\",\"/_next/static/chunks/36bfed0236ce2cf2.js\",\"/_next/static/chunks/e62b91212ee7f8ff.js\",\"/_next/static/chunks/2a98816c7d26bf58.js\",\"/_next/static/chunks/cb0a883bafeb6805.js\"],\"\"]\n7:I[89923,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"OutletBoundary\"]\n8:\"$Sreact.suspense\"\na:I[89923,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"ViewportBoundary\"]\nc:I[89923,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"MetadataBoundary\"]\ne:I[6998,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"default\"]\n:HL[\"/_next/static/chunks/f2332aac77592f9d.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"o_J3cFnAZ1mdL_cv2bxKY\",\"c\":[\"\",\"blog\",\"qwen1.5\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"qwen1.5\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/f2332aac77592f9d.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/59d0ad1b64f8544e.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"dark\",\"suppressHydrationWarning\":true,\"children\":[\"$\",\"body\",null,{\"className\":\"antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"main\",null,{\"className\":\"flex min-h-screen flex-col items-center justify-center px-4 text-center\",\"children\":[[\"$\",\"div\",null,{\"className\":\"mb-8 opacity-20\",\"children\":[\"$\",\"svg\",null,{\"width\":\"120\",\"height\":\"120\",\"viewBox\":\"0 0 120 120\",\"fill\":\"none\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"circle\",null,{\"cx\":\"60\",\"cy\":\"60\",\"r\":\"50\",\"stroke\":\"currentColor\",\"strokeWidth\":\"3\",\"strokeLinecap\":\"round\",\"strokeDasharray\":\"280 40\"}]}]}],[\"$\",\"p\",null,{\"className\":\"text-xs font-mono uppercase tracking-[0.3em] text-fd-muted-foreground mb-4\",\"children\":\"404\"}],[\"$\",\"h1\",null,{\"className\":\"text-3xl font-semibold mb-3\",\"children\":\"Page not found\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground max-w-sm mb-10\",\"children\":\"This page doesn't exist, or it may have moved. Try the documentation or head home.\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-3 justify-center\",\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/\",\"className\":\"inline-flex items-center gap-2 rounded-lg bg-fd-primary text-fd-primary-foreground px-4 py-2 text-sm font-medium hover:opacity-90 transition\",\"children\":\"Go home\"}],[\"$\",\"$L5\",null,{\"href\":\"/docs\",\"className\":\"inline-flex items-center gap-2 rounded-lg border border-fd-border bg-fd-background px-4 py-2 text-sm font-medium hover:bg-fd-muted transition\",\"children\":\"Documentation\"}],[\"$\",\"$L5\",null,{\"href\":\"/docs/models\",\"className\":\"inline-flex items-center gap-2 rounded-lg border border-fd-border bg-fd-background px-4 py-2 text-sm font-medium hover:bg-fd-muted transition\",\"children\":\"Browse models\"}]]}],[\"$\",\"p\",null,{\"className\":\"mt-16 text-xs font-mono text-fd-muted-foreground opacity-50\",\"children\":\"zenlm.org\"}]]}],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[\"$L6\",[[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/36bfed0236ce2cf2.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-1\",{\"src\":\"/_next/static/chunks/e62b91212ee7f8ff.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-2\",{\"src\":\"/_next/static/chunks/2a98816c7d26bf58.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-3\",{\"src\":\"/_next/static/chunks/cb0a883bafeb6805.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"$L7\",null,{\"children\":[\"$\",\"$8\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@9\"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$La\",null,{\"children\":\"$Lb\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$Lc\",null,{\"children\":[\"$\",\"$8\",null,{\"name\":\"Next.Metadata\",\"children\":\"$Ld\"}]}]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$e\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"f:I[48068,[\"/_next/static/chunks/59d0ad1b64f8544e.js\",\"/_next/static/chunks/36bfed0236ce2cf2.js\",\"/_next/static/chunks/e62b91212ee7f8ff.js\",\"/_next/static/chunks/2a98816c7d26bf58.js\",\"/_next/static/chunks/cb0a883bafeb6805.js\"],\"default\"]\n"])</script><script>self.__next_f.push([1,"6:[\"$\",\"main\",null,{\"className\":\"mx-auto w-full max-w-2xl px-4 py-16\",\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog\",\"className\":\"inline-flex items-center gap-1.5 text-sm text-fd-muted-foreground hover:text-fd-foreground mb-10 transition-colors\",\"children\":\"← Back to Blog\"}],[\"$\",\"div\",null,{\"className\":\"mb-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"February 3, 2024\"}],[\"$\",\"h1\",null,{\"className\":\"text-3xl font-bold mt-2 mb-3\",\"children\":\"Introducing Qwen1.5\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-lg mb-4\",\"children\":\"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3 pt-4 border-t border-fd-border\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-sm text-fd-muted-foreground\",\"children\":[\"By \",\"Zen LM Team\"]}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"prose dark:prose-invert max-w-none\",\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/QwenLM/Qwen1.5\",\"children\":\"GITHUB\"}],\" \",[\"$\",\"$Lf\",null,{\"href\":\"https://huggingface.co/Qwen\",\"children\":\"HUGGING FACE\"}],\" \",[\"$\",\"$Lf\",null,{\"href\":\"https://modelscope.cn/organization/qwen\",\"children\":\"MODELSCOPE\"}],\" \",[\"$\",\"$Lf\",null,{\"href\":\"https://huggingface.co/spaces/Qwen/Qwen1.5-72B-Chat\",\"children\":\"DEMO\"}],\" \",[\"$\",\"$Lf\",null,{\"href\":\"https://discord.gg/yPEP2vHTu4\",\"children\":\"DISCORD\"}]]}],\"\\n\",[\"$\",\"h1\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"introduction\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#introduction\",\"className\":\"peer\",\"children\":\"Introduction\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"In recent months, our focus has been on developing a “good” model while optimizing the developer experience. As we progress towards \",[\"$\",\"strong\",null,{\"children\":\"Qwen1.5\"}],\" , the next iteration in our Qwen series, this update arrives just before the Chinese New Year.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"With Qwen1.5, we are open-sourcing base and chat models across six sizes: 0.5B, 1.8B, 4B, 7B, 14B, 32B, 72B, and 110B, and also an MoE model (see \",[\"$\",\"$Lf\",null,{\"href\":\"https://qwenlm.github.io/blog/qwen-moe/\",\"children\":\"blog\"}],\" for more information). In line with tradition, we’re also providing quantized models, including Int4 and Int8 GPTQ models, as well as AWQ and GGUF quantized models. To enhance the developer experience, we’ve merged Qwen1.5’s code into Hugging Face transformers, making it accessible with \",[\"$\",\"code\",null,{\"children\":\"transformers\u003e=4.37.0\"}],\" without needing \",[\"$\",\"code\",null,{\"children\":\"trust_remote_code\"}],\".\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"We’ve collaborated with frameworks like \",[\"$\",\"$Lf\",null,{\"href\":\"https://vllm.readthedocs.io/\",\"children\":\"vLLM\"}],\", \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/sgl-project/sglang\",\"children\":\"SGLang\"}],\" for deployment, \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/casper-hansen/AutoAWQ\",\"children\":\"AutoAWQ\"}],\", \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/AutoGPTQ/AutoGPTQ\",\"children\":\"AutoGPTQ\"}],\" for quantization, \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/OpenAccess-AI-Collective/axolotl\",\"children\":\"Axolotl\"}],\", \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/hiyouga/LLaMA-Factory\",\"children\":\"LLaMA-Factory\"}],\" for finetuning, and \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/ggerganov/llama.cpp\",\"children\":\"llama.cpp\"}],\" for local LLM inference, all of which now support Qwen1.5. The Qwen1.5 series is available on platforms such as \",\"$L10\",\" and \",\"$L11\",\". Additionally, API services are offered not only on DashScope but also on \",\"$L12\",\", with global accessibility. Visit \",\"$L13\",\" to get started, and we recommend trying out \",\"$L14\",\".\"]}],\"\\n\",\"$L15\",\"\\n\",\"$L16\",\"\\n\",\"$L17\",\"\\n\",\"$L18\",\"\\n\",\"$L19\",\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L1a\",\"\\n\",\"$L1b\",\"\\n\",\"$L1c\",\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L1d\",\"\\n\",\"$L1e\",\"\\n\",\"$L1f\",\"\\n\",\"$L20\",\"\\n\",\"$L21\",\"\\n\",\"$L22\",\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L23\",\"\\n\",\"$L24\",\"\\n\",\"$L25\",\"\\n\",\"$L26\",\"\\n\",\"$L27\",\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L28\",\"\\n\",\"$L29\",\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L2a\",\"\\n\",\"$L2b\",\"\\n\",\"$L2c\",\"\\n\",\"$L2d\",\"\\n\",\"$L2e\",\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L2f\",\"\\n\",\"$L30\",\"\\n\",\"$L31\",\"\\n\",\"$L32\",\"\\n\",\"$L33\",\"\\n\",\"$L34\",\"\\n\",\"$L35\",\"\\n\",\"$L36\",\"\\n\",\"$L37\",\"\\n\",\"$L38\",\"\\n\",\"$L39\",\"\\n\",\"$L3a\",\"\\n\",\"$L3b\",\"\\n\",\"$L3c\",\"\\n\",\"$L3d\",\"\\n\",\"$L3e\",\"\\n\",\"$L3f\",\"\\n\",\"$L40\",\"\\n\",\"$L41\",\"\\n\",\"$L42\",\"\\n\",\"$L43\",\"\\n\",\"$L44\",\"\\n\",\"$L45\",\"\\n\",\"$L46\",\"\\n\",\"$L47\",\"\\n\",\"$L48\",\"\\n\",\"$L49\",\"\\n\",\"$L4a\",\"\\n\",\"$L4b\",\"\\n\",\"$L4c\",\"\\n\",\"$L4d\",\"\\n\",\"$L4e\",\"\\n\",\"$L4f\",\"\\n\",\"$L50\",\"\\n\",\"$L51\",\"\\n\",\"$L52\",\"\\n\",\"$L53\",\"\\n\",\"$L54\",\"\\n\",\"$L55\",\"\\n\",\"$L56\"]}]]}]\n"])</script><script>self.__next_f.push([1,"57:I[51504,[\"/_next/static/chunks/59d0ad1b64f8544e.js\",\"/_next/static/chunks/36bfed0236ce2cf2.js\",\"/_next/static/chunks/e62b91212ee7f8ff.js\",\"/_next/static/chunks/2a98816c7d26bf58.js\",\"/_next/static/chunks/cb0a883bafeb6805.js\"],\"CodeBlock\"]\n59:I[51504,[\"/_next/static/chunks/59d0ad1b64f8544e.js\",\"/_next/static/chunks/36bfed0236ce2cf2.js\",\"/_next/static/chunks/e62b91212ee7f8ff.js\",\"/_next/static/chunks/2a98816c7d26bf58.js\",\"/_next/static/chunks/cb0a883bafeb6805.js\"],\"Pre\"]\n10:[\"$\",\"$Lf\",null,{\"href\":\"https://ollama.ai/\",\"children\":\"Ollama\"}]\n11:[\"$\",\"$Lf\",null,{\"href\":\"https://lmstudio.ai/\",\"children\":\"LMStudio\"}]\n12:[\"$\",\"$Lf\",null,{\"href\":\"https://together.ai/\",\"children\":\"together.ai\"}]\n13:[\"$\",\"$Lf\",null,{\"href\":\"https://api.together.ai/\",\"children\":\"here\"}]\n14:[\"$\",\"$Lf\",null,{\"href\":\"https://api.together.xyz/playground/chat/Qwen/Qwen1.5-72B-Chat\",\"children\":\"Qwen1.5-72B-chat\"}]\n15:[\"$\",\"p\",null,{\"children\":\"This release brings substantial improvements to the alignment of chat models with human preferences and enhanced multilingual capabilities. All models now uniformly support a context length of up to 32768 tokens. There have also been minor improvements in the quality of base language models that may benefit your finetuning endeavors. This step represents a small stride toward our objective of creating a truly “good” model.\"}]\n16:[\"$\",\"h1\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"performance\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#performance\",\"className\":\"peer\",\"children\":\"Performance\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n17:[\"$\",\"p\",null,{\"children\":\"To provide a better understanding of the performance of Qwen1.5, we have conducted a comprehensive evaluation of both base and chat models on different capabilities, including basic capabilities such as language understanding, coding, reasoning, multilingual capabilities, human preference, agent, retrieval-augmented generation (RAG), etc.\"}]\n18:[\"$\",\"h2\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"basic-capabilities\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#basic-capabilities\",\"className\":\"peer\",\"children\":\"Basic Capabilities\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n19:[\"$\",\"p\",null,{\"children\":\"To assess the basic capabilities of language models, we have conducted evaluations on traditional benchmarks, including MMLU (5-shot), C-Eval, Humaneval, GS8K, BBH, etc.\"}]\n"])</script><script>self.__next_f.push([1,"1a:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Model\"}],[\"$\",\"th\",null,{\"children\":\"MMLU\"}],[\"$\",\"th\",null,{\"children\":\"C-Eval\"}],[\"$\",\"th\",null,{\"children\":\"GSM8K\"}],[\"$\",\"th\",null,{\"children\":\"MATH\"}],[\"$\",\"th\",null,{\"children\":\"HumanEval\"}],[\"$\",\"th\",null,{\"children\":\"MBPP\"}],[\"$\",\"th\",null,{\"children\":\"BBH\"}],[\"$\",\"th\",null,{\"children\":\"CMMLU\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GPT-4\"}],[\"$\",\"td\",null,{\"children\":\"86.4\"}],[\"$\",\"td\",null,{\"children\":\"69.9\"}],[\"$\",\"td\",null,{\"children\":\"92.0\"}],[\"$\",\"td\",null,{\"children\":\"45.8\"}],[\"$\",\"td\",null,{\"children\":\"67.0\"}],[\"$\",\"td\",null,{\"children\":\"61.8\"}],[\"$\",\"td\",null,{\"children\":\"86.7\"}],[\"$\",\"td\",null,{\"children\":\"71.0\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Llama2-7B\"}],[\"$\",\"td\",null,{\"children\":\"46.8\"}],[\"$\",\"td\",null,{\"children\":\"32.5\"}],[\"$\",\"td\",null,{\"children\":\"16.7\"}],[\"$\",\"td\",null,{\"children\":\"3.3\"}],[\"$\",\"td\",null,{\"children\":\"12.8\"}],[\"$\",\"td\",null,{\"children\":\"20.8\"}],[\"$\",\"td\",null,{\"children\":\"38.2\"}],[\"$\",\"td\",null,{\"children\":\"31.8\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Llama2-13B\"}],[\"$\",\"td\",null,{\"children\":\"55.0\"}],[\"$\",\"td\",null,{\"children\":\"41.4\"}],[\"$\",\"td\",null,{\"children\":\"29.6\"}],[\"$\",\"td\",null,{\"children\":\"5.0\"}],[\"$\",\"td\",null,{\"children\":\"18.9\"}],[\"$\",\"td\",null,{\"children\":\"30.3\"}],[\"$\",\"td\",null,{\"children\":\"45.6\"}],[\"$\",\"td\",null,{\"children\":\"38.4\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Llama2-34B\"}],[\"$\",\"td\",null,{\"children\":\"62.6\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"42.2\"}],[\"$\",\"td\",null,{\"children\":\"6.2\"}],[\"$\",\"td\",null,{\"children\":\"22.6\"}],[\"$\",\"td\",null,{\"children\":\"33.0\"}],[\"$\",\"td\",null,{\"children\":\"44.1\"}],[\"$\",\"td\",null,{\"children\":\"-\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Llama2-70B\"}],[\"$\",\"td\",null,{\"children\":\"69.8\"}],[\"$\",\"td\",null,{\"children\":\"50.1\"}],[\"$\",\"td\",null,{\"children\":\"54.4\"}],[\"$\",\"td\",null,{\"children\":\"10.6\"}],[\"$\",\"td\",null,{\"children\":\"23.7\"}],[\"$\",\"td\",null,{\"children\":\"37.7\"}],[\"$\",\"td\",null,{\"children\":\"58.4\"}],[\"$\",\"td\",null,{\"children\":\"53.6\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Mistral-7B\"}],[\"$\",\"td\",null,{\"children\":\"64.1\"}],[\"$\",\"td\",null,{\"children\":\"47.4\"}],[\"$\",\"td\",null,{\"children\":\"47.5\"}],[\"$\",\"td\",null,{\"children\":\"11.3\"}],[\"$\",\"td\",null,{\"children\":\"27.4\"}],[\"$\",\"td\",null,{\"children\":\"38.6\"}],[\"$\",\"td\",null,{\"children\":\"56.7\"}],[\"$\",\"td\",null,{\"children\":\"44.7\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Mixtral-8x7B\"}],[\"$\",\"td\",null,{\"children\":\"70.6\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"74.4\"}],[\"$\",\"td\",null,{\"children\":\"28.4\"}],[\"$\",\"td\",null,{\"children\":\"40.2\"}],[\"$\",\"td\",null,{\"children\":\"60.7\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"-\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Qwen1.5-7B\"}],[\"$\",\"td\",null,{\"children\":\"61.0\"}],[\"$\",\"td\",null,{\"children\":\"74.1\"}],[\"$\",\"td\",null,{\"children\":\"62.5\"}],[\"$\",\"td\",null,{\"children\":\"20.3\"}],[\"$\",\"td\",null,{\"children\":\"36.0\"}],[\"$\",\"td\",null,{\"children\":\"37.4\"}],[\"$\",\"td\",null,{\"children\":\"40.2\"}],[\"$\",\"td\",null,{\"children\":\"73.1\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Qwen1.5-14B\"}],[\"$\",\"td\",null,{\"children\":\"67.6\"}],[\"$\",\"td\",null,{\"children\":\"78.7\"}],[\"$\",\"td\",null,{\"children\":\"70.1\"}],[\"$\",\"td\",null,{\"children\":\"29.2\"}],[\"$\",\"td\",null,{\"children\":\"37.8\"}],[\"$\",\"td\",null,{\"children\":\"44.0\"}],[\"$\",\"td\",null,{\"children\":\"53.7\"}],[\"$\",\"td\",null,{\"children\":\"77.6\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Qwen1.5-32B\"}],[\"$\",\"td\",null,{\"children\":\"73.4\"}],[\"$\",\"td\",null,{\"children\":\"83.5\"}],[\"$\",\"td\",null,{\"children\":\"77.4\"}],[\"$\",\"td\",null,{\"children\":\"36.1\"}],[\"$\",\"td\",null,{\"children\":\"37.2\"}],[\"$\",\"td\",null,{\"children\":\"49.4\"}],[\"$\",\"td\",null,{\"children\":\"66.8\"}],[\"$\",\"td\",null,{\"children\":\"82.3\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Qwen1.5-72B\"}],[\"$\",\"td\",null,{\"children\":\"77.5\"}],[\"$\",\"td\",null,{\"children\":\"84.1\"}],[\"$\",\"td\",null,{\"children\":\"79.5\"}],[\"$\",\"td\",null,{\"children\":\"34.1\"}],[\"$\",\"td\",null,{\"children\":\"41.5\"}],[\"$\",\"td\",null,{\"children\":\"53.4\"}],[\"$\",\"td\",null,{\"children\":\"65.5\"}],[\"$\",\"td\",null,{\"children\":\"83.5\"}]]}]]}]]}]}]\n"])</script><script>self.__next_f.push([1,"1b:[\"$\",\"p\",null,{\"children\":\"At every model size, Qwen1.5 demonstrates strong performance across the diverse evaluation benchmarks. In particular, Qwen1.5-72B outperforms Llama2-70B across all benchmarks, showcasing its exceptional capabilities in language understanding, reasoning, and math.\"}]\n1c:[\"$\",\"p\",null,{\"children\":\"In light of the recent surge in interest for small language models, we have compared Qwen1.5 with sizes smaller than 7 billion parameters, against the most outstanding small-scale models within the community. The results are shown below:\"}]\n"])</script><script>self.__next_f.push([1,"1d:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Model\"}],[\"$\",\"th\",null,{\"children\":\"Non-Emb Params\"}],[\"$\",\"th\",null,{\"children\":\"MMLU\"}],[\"$\",\"th\",null,{\"children\":\"C-Eval\"}],[\"$\",\"th\",null,{\"children\":\"GSM8K\"}],[\"$\",\"th\",null,{\"children\":\"MATH\"}],[\"$\",\"th\",null,{\"children\":\"HumanEval\"}],[\"$\",\"th\",null,{\"children\":\"MBPP\"}],[\"$\",\"th\",null,{\"children\":\"BBH\"}],[\"$\",\"th\",null,{\"children\":\"CMMLU\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Tinyllama-1.1B\"}],[\"$\",\"td\",null,{\"children\":\"1.1B\"}],[\"$\",\"td\",null,{\"children\":\"24.3\"}],[\"$\",\"td\",null,{\"children\":\"25.0\"}],[\"$\",\"td\",null,{\"children\":\"2.3\"}],[\"$\",\"td\",null,{\"children\":\"0.7\"}],[\"$\",\"td\",null,{\"children\":\"6.7\"}],[\"$\",\"td\",null,{\"children\":\"19.9\"}],[\"$\",\"td\",null,{\"children\":\"28.8\"}],[\"$\",\"td\",null,{\"children\":\"24.0\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Gemini-Nano-3B\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"22.8\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"27.2\"}],[\"$\",\"td\",null,{\"children\":\"42.4\"}],[\"$\",\"td\",null,{\"children\":\"-\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"StableLM-Zephyr-3B\"}],[\"$\",\"td\",null,{\"children\":\"2.7B\"}],[\"$\",\"td\",null,{\"children\":\"45.9\"}],[\"$\",\"td\",null,{\"children\":\"30.3\"}],[\"$\",\"td\",null,{\"children\":\"52.5\"}],[\"$\",\"td\",null,{\"children\":\"12.5\"}],[\"$\",\"td\",null,{\"children\":\"35.4\"}],[\"$\",\"td\",null,{\"children\":\"31.9\"}],[\"$\",\"td\",null,{\"children\":\"37.7\"}],[\"$\",\"td\",null,{\"children\":\"30.9\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Phi-2\"}],[\"$\",\"td\",null,{\"children\":\"2.5B\"}],[\"$\",\"td\",null,{\"children\":\"52.7\"}],[\"$\",\"td\",null,{\"children\":\"23.4\"}],[\"$\",\"td\",null,{\"children\":\"57.2\"}],[\"$\",\"td\",null,{\"children\":\"3.5\"}],[\"$\",\"td\",null,{\"children\":\"47.6\"}],[\"$\",\"td\",null,{\"children\":\"55.0\"}],[\"$\",\"td\",null,{\"children\":\"43.4\"}],[\"$\",\"td\",null,{\"children\":\"24.2\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MiniCPM-2B\"}],[\"$\",\"td\",null,{\"children\":\"2.4B\"}],[\"$\",\"td\",null,{\"children\":\"53.5\"}],[\"$\",\"td\",null,{\"children\":\"51.1\"}],[\"$\",\"td\",null,{\"children\":\"53.8\"}],[\"$\",\"td\",null,{\"children\":\"10.2\"}],[\"$\",\"td\",null,{\"children\":\"50.0\"}],[\"$\",\"td\",null,{\"children\":\"47.3\"}],[\"$\",\"td\",null,{\"children\":\"36.9\"}],[\"$\",\"td\",null,{\"children\":\"51.1\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Gemma-2B\"}],[\"$\",\"td\",null,{\"children\":\"2.0B\"}],[\"$\",\"td\",null,{\"children\":\"42.3\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"17.7\"}],[\"$\",\"td\",null,{\"children\":\"11.8\"}],[\"$\",\"td\",null,{\"children\":\"22.0\"}],[\"$\",\"td\",null,{\"children\":\"29.2\"}],[\"$\",\"td\",null,{\"children\":\"35.2\"}],[\"$\",\"td\",null,{\"children\":\"-\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Qwen1.5-0.5B\"}],[\"$\",\"td\",null,{\"children\":\"0.3B\"}],[\"$\",\"td\",null,{\"children\":\"39.2\"}],[\"$\",\"td\",null,{\"children\":\"50.5\"}],[\"$\",\"td\",null,{\"children\":\"22.0\"}],[\"$\",\"td\",null,{\"children\":\"3.1\"}],[\"$\",\"td\",null,{\"children\":\"12.2\"}],[\"$\",\"td\",null,{\"children\":\"6.8\"}],[\"$\",\"td\",null,{\"children\":\"18.3\"}],[\"$\",\"td\",null,{\"children\":\"46.6\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Qwen1.5-1.8B\"}],[\"$\",\"td\",null,{\"children\":\"1.2B\"}],[\"$\",\"td\",null,{\"children\":\"46.8\"}],[\"$\",\"td\",null,{\"children\":\"59.7\"}],[\"$\",\"td\",null,{\"children\":\"38.4\"}],[\"$\",\"td\",null,{\"children\":\"10.1\"}],[\"$\",\"td\",null,{\"children\":\"20.1\"}],[\"$\",\"td\",null,{\"children\":\"18.0\"}],[\"$\",\"td\",null,{\"children\":\"24.2\"}],[\"$\",\"td\",null,{\"children\":\"57.8\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Qwen1.5-4B\"}],[\"$\",\"td\",null,{\"children\":\"3.1B\"}],[\"$\",\"td\",null,{\"children\":\"56.1\"}],[\"$\",\"td\",null,{\"children\":\"67.6\"}],[\"$\",\"td\",null,{\"children\":\"57.0\"}],[\"$\",\"td\",null,{\"children\":\"10.0\"}],[\"$\",\"td\",null,{\"children\":\"25.6\"}],[\"$\",\"td\",null,{\"children\":\"29.2\"}],[\"$\",\"td\",null,{\"children\":\"32.5\"}],[\"$\",\"td\",null,{\"children\":\"66.7\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Qwen1.5-MoE-A2.7B\"}],[\"$\",\"td\",null,{\"children\":\"2.0B\"}],[\"$\",\"td\",null,{\"children\":\"62.5\"}],[\"$\",\"td\",null,{\"children\":\"79.2\"}],[\"$\",\"td\",null,{\"children\":\"61.5\"}],[\"$\",\"td\",null,{\"children\":\"21.9\"}],[\"$\",\"td\",null,{\"children\":\"34.2\"}],[\"$\",\"td\",null,{\"children\":\"36.6\"}],[\"$\",\"td\",null,{\"children\":\"39.1\"}],[\"$\",\"td\",null,{\"children\":\"79.2\"}]]}]]}]]}]}]\n"])</script><script>self.__next_f.push([1,"1e:[\"$\",\"p\",null,{\"children\":\"We can confidently assert that Qwen1.5 base models under 7 billion parameters are highly competitive with the leading small-scale models in the community. In the future, we will continue to improve the quality of small models and exploring methods for effectively transferring the advanced capabilities inherent in larger models into the smaller ones.\"}]\n1f:[\"$\",\"h2\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"aligning-with-human-preference\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#aligning-with-human-preference\",\"className\":\"peer\",\"children\":\"Aligning with Human Preference\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n20:[\"$\",\"p\",null,{\"children\":\"Alignment aims to enhance instruction-following capabilities of LLMs and help provide responses that are closely aligned with human preferences. Recognizing the significance of integrating human preferences into the learning process, we effectively employed techniques such as Direct Policy Optimization (DPO) and Proximal Policy Optimization (PPO) in aligning the latest Qwen series.\"}]\n21:[\"$\",\"p\",null,{\"children\":\"However, assessing the quality of such chat models poses a significant challenge. Admittedly, while comprehensive human evaluation is the optimal approach, it faces significant challenges pertaining to scalability and reproducibility. Therefore, we initially evaluate our models on two widely-used benchmarks, utilizing advanced LLMs as judges: MT-Bench and Alpaca-Eval. The results are presented below:\"}]\n22:[\"$\",\"p\",null,{\"children\":\"We notice there are non-negligible variance in the scores on MT-Bench. So we have three runs with different seeds in our results and we report the average score with standard deviation.\"}]\n23:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Models\"}],[\"$\",\"th\",null,{\"children\":\"MT-Bench\"}],[\"$\",\"th\",null,{\"children\":\"AlpacaEval 2.0\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Avg. Score\"}],[\"$\",\"td\",null,{\"children\":\"Win Rate\"}],[\"$\",\"td\",null,{\"children\":\"Length\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Qwen1.5-72B-Chat\"}],[\"$\",\"td\",null,{\"children\":\"8.610.04 (8.67/8.61/8.56)\"}],[\"$\",\"td\",null,{\"children\":\"27.181.30\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Qwen1.5-14B-Chat\"}],[\"$\",\"td\",null,{\"children\":\"7.910.11 (7.99/7.99/7.77)\"}],[\"$\",\"td\",null,{\"children\":\"19.71.12\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Qwen1.5-7B-Chat\"}],[\"$\",\"td\",null,{\"children\":\"7.600.05 (7.58/7.55/7.66)\"}],[\"$\",\"td\",null,{\"children\":\"13.201.43\"}]]}]]}]]}]}]\n24:[\"$\",\"p\",null,{\"children\":\"Despite still significantly trailing behind GPT-4-Turbo, the largest open-source Qwen1.5 model, Qwen1.5-72B-Chat, exhibits superior performance, surpassing Claude-2.1, GPT-3.5-Turbo-0613, Mixtral-8x7b-instruct, and TULU 2 DPO 70B, being on par with Mistral Medium, on both MT-Bench and Alpaca-Eval v2.\"}]\n25:[\"$\",\"p\",null,{\"children\":\"Furthermore, although the scoring of LLM Judges may seemingly correlate with the lengths of responses, our observations indicate that our models do not generate lengthy responses to manipulate the bias of LLM judges. The average length of Qwen1.5-Chat on AlpacaEval 2.0 is only 1618, which aligns with the length of GPT-4 and is shorter than that of GPT-4-Turbo. Additionally, our experiments with our "])</script><script>self.__next_f.push([1,"web service and app also reveal that users prefer the majority of responses from the new chat models.\"}]\n26:[\"$\",\"h2\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"multilingual-understanding-of-base-models\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#multilingual-understanding-of-base-models\",\"className\":\"peer\",\"children\":\"Multilingual Understanding of Base Models\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n27:[\"$\",\"p\",null,{\"children\":\"We have carefully selected a diverse set of 12 languages from Europe, East Asia, and Southeast Asia to thoroughly evaluate the multilingual capabilities of our foundational model. In order to accomplish this, we have curated test sets from the community’s open-source repositories, covering four distinct dimensions: Exams, Understanding, Translation, and Math. The table below provides detailed information about each test set, including evaluation settings, metrics, and the languages they encompass:\"}]\n"])</script><script>self.__next_f.push([1,"28:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Dataset\"}],[\"$\",\"th\",null,{\"children\":\"Category\"}],[\"$\",\"th\",null,{\"children\":\"Method/Metric\"}],[\"$\",\"th\",null,{\"children\":\"Languages\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU-multi\"}],[\"$\",\"td\",null,{\"children\":\"Exams\"}],[\"$\",\"td\",null,{\"children\":\"5-shot/Acc\"}],[\"$\",\"td\",null,{\"children\":\"ar, es, fr, pt, de, it, ru, ja, ko, id\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"M3Exams\"}],[\"$\",\"td\",null,{\"children\":\"Exams\"}],[\"$\",\"td\",null,{\"children\":\"5-shot/Acc\"}],[\"$\",\"td\",null,{\"children\":\"pt, it, vi, th\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"BELEBELE\"}],[\"$\",\"td\",null,{\"children\":\"Understanding\"}],[\"$\",\"td\",null,{\"children\":\"5-shot/Acc\"}],[\"$\",\"td\",null,{\"children\":\"ar, es, fr, pt, de, it, ru, ja, ko, vi, th, id\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"XWinograd\"}],[\"$\",\"td\",null,{\"children\":\"Understanding\"}],[\"$\",\"td\",null,{\"children\":\"5-shot/Acc\"}],[\"$\",\"td\",null,{\"children\":\"fr, pt, ru, ja\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"XCOPA\"}],[\"$\",\"td\",null,{\"children\":\"Understanding\"}],[\"$\",\"td\",null,{\"children\":\"5-shot/Acc\"}],[\"$\",\"td\",null,{\"children\":\"vi, id, th\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"PAWS-X\"}],[\"$\",\"td\",null,{\"children\":\"Understanding\"}],[\"$\",\"td\",null,{\"children\":\"5-shot/Acc\"}],[\"$\",\"td\",null,{\"children\":\"es, fr, de, ja, ko\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"XStoryCloze\"}],[\"$\",\"td\",null,{\"children\":\"Understanding\"}],[\"$\",\"td\",null,{\"children\":\"0-shot/Acc\"}],[\"$\",\"td\",null,{\"children\":\"ar, es, ru, id\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Flores(zh/en↔xx)\"}],[\"$\",\"td\",null,{\"children\":\"Translation\"}],[\"$\",\"td\",null,{\"children\":\"5-shot/BLEU\"}],[\"$\",\"td\",null,{\"children\":\"ar, es, fr, pt, de, it, ru, ja, ko, vi, th, id\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MGSM\"}],[\"$\",\"td\",null,{\"children\":\"Math\"}],[\"$\",\"td\",null,{\"children\":\"8-shot/Acc\"}],[\"$\",\"td\",null,{\"children\":\"es, fr, ru, de, ja, th\"}]]}]]}]]}]}]\n"])</script><script>self.__next_f.push([1,"29:[\"$\",\"p\",null,{\"children\":\"The detailed results are demonstrated below:\"}]\n"])</script><script>self.__next_f.push([1,"2a:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Models\"}],[\"$\",\"th\",null,{\"children\":\"Exams\"}],[\"$\",\"th\",null,{\"children\":\"Understanding\"}],[\"$\",\"th\",null,{\"children\":\"Math\"}],[\"$\",\"th\",null,{\"children\":\"Translation\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GPT-3.5\"}],[\"$\",\"td\",null,{\"children\":\"52.24\"}],[\"$\",\"td\",null,{\"children\":\"71.84\"}],[\"$\",\"td\",null,{\"children\":\"32.80\"}],[\"$\",\"td\",null,{\"children\":\"31.85\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GPT-4\"}],[\"$\",\"td\",null,{\"children\":\"71.64\"}],[\"$\",\"td\",null,{\"children\":\"83.82\"}],[\"$\",\"td\",null,{\"children\":\"80.13\"}],[\"$\",\"td\",null,{\"children\":\"34.37\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Llama2-7B\"}],[\"$\",\"td\",null,{\"children\":\"34.03\"}],[\"$\",\"td\",null,{\"children\":\"50.13\"}],[\"$\",\"td\",null,{\"children\":\"9.40\"}],[\"$\",\"td\",null,{\"children\":\"22.19\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Llama2-13B\"}],[\"$\",\"td\",null,{\"children\":\"39.55\"}],[\"$\",\"td\",null,{\"children\":\"57.26\"}],[\"$\",\"td\",null,{\"children\":\"16.80\"}],[\"$\",\"td\",null,{\"children\":\"25.89\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Llama2-70B\"}],[\"$\",\"td\",null,{\"children\":\"55.88\"}],[\"$\",\"td\",null,{\"children\":\"73.19\"}],[\"$\",\"td\",null,{\"children\":\"40.20\"}],[\"$\",\"td\",null,{\"children\":\"31.56\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Mistral-7B\"}],[\"$\",\"td\",null,{\"children\":\"47.12\"}],[\"$\",\"td\",null,{\"children\":\"63.30\"}],[\"$\",\"td\",null,{\"children\":\"26.33\"}],[\"$\",\"td\",null,{\"children\":\"23.33\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Mixtral-8x7B\"}],[\"$\",\"td\",null,{\"children\":\"56.08\"}],[\"$\",\"td\",null,{\"children\":\"70.70\"}],[\"$\",\"td\",null,{\"children\":\"45.00\"}],[\"$\",\"td\",null,{\"children\":\"29.78\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Qwen1.5-0.5B\"}],[\"$\",\"td\",null,{\"children\":\"26.98\"}],[\"$\",\"td\",null,{\"children\":\"44.08\"}],[\"$\",\"td\",null,{\"children\":\"3.13\"}],[\"$\",\"td\",null,{\"children\":\"9.17\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Qwen1.5-1.8B\"}],[\"$\",\"td\",null,{\"children\":\"33.57\"}],[\"$\",\"td\",null,{\"children\":\"48.37\"}],[\"$\",\"td\",null,{\"children\":\"6.47\"}],[\"$\",\"td\",null,{\"children\":\"16.19\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Qwen1.5-4B\"}],[\"$\",\"td\",null,{\"children\":\"41.43\"}],[\"$\",\"td\",null,{\"children\":\"59.76\"}],[\"$\",\"td\",null,{\"children\":\"21.33\"}],[\"$\",\"td\",null,{\"children\":\"23.34\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Qwen1.5-MoE-A2.7B\"}],[\"$\",\"td\",null,{\"children\":\"44.54\"}],[\"$\",\"td\",null,{\"children\":\"61.08\"}],[\"$\",\"td\",null,{\"children\":\"30.20\"}],[\"$\",\"td\",null,{\"children\":\"27.35\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Qwen1.5-7B\"}],[\"$\",\"td\",null,{\"children\":\"47.70\"}],[\"$\",\"td\",null,{\"children\":\"67.63\"}],[\"$\",\"td\",null,{\"children\":\"37.27\"}],[\"$\",\"td\",null,{\"children\":\"28.36\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Qwen1.5-14B\"}],[\"$\",\"td\",null,{\"children\":\"55.72\"}],[\"$\",\"td\",null,{\"children\":\"74.10\"}],[\"$\",\"td\",null,{\"children\":\"49.93\"}],[\"$\",\"td\",null,{\"children\":\"31.69\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Qwen1.5-72B\"}],[\"$\",\"td\",null,{\"children\":\"66.35\"}],[\"$\",\"td\",null,{\"children\":\"78.16\"}],[\"$\",\"td\",null,{\"children\":\"61.67\"}],[\"$\",\"td\",null,{\"children\":\"35.57\"}]]}]]}]]}]}]\n"])</script><script>self.__next_f.push([1,"2b:[\"$\",\"p\",null,{\"children\":\"The base models of Qwen1.5 showcase impressive multilingual capabilities, as demonstrated by its performance across a diverse set of 12 languages. In evaluations covering various dimensions such as exams, understanding, translation, and math, Qwen1.5 consistently delivers strong results. From languages like Arabic, Spanish, and French to Japanese, Korean, and Thai, Qwen1.5 demonstrates its ability to comprehend and generate high-quality content across different linguistic contexts. We compared Qwen1.5-72B-Chat with GPT-3.5, and the results are shown below:\"}]\n2c:[\"$\",\"p\",null,{\"children\":\"These results demonstrate the strong multilingual capabilities of Qwen1.5 chat models, which can serve downstream applications, such as translation, language understanding, and multilingual chat. Also, we believe that the improvements in multilingual capabilities can also level up the general capabilities.\"}]\n2d:[\"$\",\"h2\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"support-of-long-context\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#support-of-long-context\",\"className\":\"peer\",\"children\":\"Support of Long Context\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n2e:[\"$\",\"p\",null,{\"children\":[\"With the increasing demand for long-context understanding, we have expanded the capability of all models to support contexts up to 32K tokens. We have evaluated the performance of Qwen1.5 models on the \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/OpenLMLab/LEval\",\"children\":\"L-Eval benchmark\"}],\", which measures the ability of models to generate responses based on long context. The results are shown below:\"]}]\n"])</script><script>self.__next_f.push([1,"2f:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Models\"}],[\"$\",\"th\",null,{\"children\":\"Coursera\"}],[\"$\",\"th\",null,{\"children\":\"GSM\"}],[\"$\",\"th\",null,{\"children\":\"QuALITY\"}],[\"$\",\"th\",null,{\"children\":\"TOEFL\"}],[\"$\",\"th\",null,{\"children\":\"SFiction\"}],[\"$\",\"th\",null,{\"children\":\"Avg.\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GPT3.5-turbo-16k\"}],[\"$\",\"td\",null,{\"children\":\"63.51\"}],[\"$\",\"td\",null,{\"children\":\"84.00\"}],[\"$\",\"td\",null,{\"children\":\"61.38\"}],[\"$\",\"td\",null,{\"children\":\"78.43\"}],[\"$\",\"td\",null,{\"children\":\"64.84\"}],[\"$\",\"td\",null,{\"children\":\"70.43\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Claude1.3-100k\"}],[\"$\",\"td\",null,{\"children\":\"60.03\"}],[\"$\",\"td\",null,{\"children\":\"88.00\"}],[\"$\",\"td\",null,{\"children\":\"73.76\"}],[\"$\",\"td\",null,{\"children\":\"83.64\"}],[\"$\",\"td\",null,{\"children\":\"72.65\"}],[\"$\",\"td\",null,{\"children\":\"75.62\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GPT4-32k\"}],[\"$\",\"td\",null,{\"children\":\"75.58\"}],[\"$\",\"td\",null,{\"children\":\"96.00\"}],[\"$\",\"td\",null,{\"children\":\"82.17\"}],[\"$\",\"td\",null,{\"children\":\"84.38\"}],[\"$\",\"td\",null,{\"children\":\"74.99\"}],[\"$\",\"td\",null,{\"children\":\"82.62\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Qwen-72B-Chat\"}],[\"$\",\"td\",null,{\"children\":\"58.13\"}],[\"$\",\"td\",null,{\"children\":\"76.00\"}],[\"$\",\"td\",null,{\"children\":\"77.22\"}],[\"$\",\"td\",null,{\"children\":\"86.24\"}],[\"$\",\"td\",null,{\"children\":\"69.53\"}],[\"$\",\"td\",null,{\"children\":\"73.42\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Qwen1.5-0.5B-Chat\"}],[\"$\",\"td\",null,{\"children\":\"30.81\"}],[\"$\",\"td\",null,{\"children\":\"6.00\"}],[\"$\",\"td\",null,{\"children\":\"34.16\"}],[\"$\",\"td\",null,{\"children\":\"40.52\"}],[\"$\",\"td\",null,{\"children\":\"49.22\"}],[\"$\",\"td\",null,{\"children\":\"32.14\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Qwen1.5-1.8B-Chat\"}],[\"$\",\"td\",null,{\"children\":\"39.24\"}],[\"$\",\"td\",null,{\"children\":\"37.00\"}],[\"$\",\"td\",null,{\"children\":\"42.08\"}],[\"$\",\"td\",null,{\"children\":\"55.76\"}],[\"$\",\"td\",null,{\"children\":\"44.53\"}],[\"$\",\"td\",null,{\"children\":\"43.72\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Qwen1.5-4B-Chat\"}],[\"$\",\"td\",null,{\"children\":\"54.94\"}],[\"$\",\"td\",null,{\"children\":\"47.00\"}],[\"$\",\"td\",null,{\"children\":\"57.92\"}],[\"$\",\"td\",null,{\"children\":\"69.15\"}],[\"$\",\"td\",null,{\"children\":\"56.25\"}],[\"$\",\"td\",null,{\"children\":\"57.05\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Qwen1.5-7B-Chat\"}],[\"$\",\"td\",null,{\"children\":\"59.74\"}],[\"$\",\"td\",null,{\"children\":\"60.00\"}],[\"$\",\"td\",null,{\"children\":\"64.36\"}],[\"$\",\"td\",null,{\"children\":\"79.18\"}],[\"$\",\"td\",null,{\"children\":\"62.50\"}],[\"$\",\"td\",null,{\"children\":\"65.16\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Qwen1.5-14B-Chat\"}],[\"$\",\"td\",null,{\"children\":\"69.04\"}],[\"$\",\"td\",null,{\"children\":\"79.00\"}],[\"$\",\"td\",null,{\"children\":\"74.75\"}],[\"$\",\"td\",null,{\"children\":\"83.64\"}],[\"$\",\"td\",null,{\"children\":\"75.78\"}],[\"$\",\"td\",null,{\"children\":\"76.44\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Qwen1.5-72B-Chat\"}],[\"$\",\"td\",null,{\"children\":\"71.95\"}],[\"$\",\"td\",null,{\"children\":\"82.00\"}],[\"$\",\"td\",null,{\"children\":\"77.72\"}],[\"$\",\"td\",null,{\"children\":\"85.50\"}],[\"$\",\"td\",null,{\"children\":\"73.44\"}],[\"$\",\"td\",null,{\"children\":\"78.12\"}]]}]]}]]}]}]\n"])</script><script>self.__next_f.push([1,"30:[\"$\",\"p\",null,{\"children\":[\"In terms of the performance, even a small model like Qwen1.5-7B-Chat demonstrates competitive performance against GPT-3.5 on 4 out of 5 tasks. Our best model, Qwen1.5-72B-Chat, significantly outperforms GPT3.5-turbo-16k and only slightly falls behind GPT4-32k. These results highlight our outstanding performance within 32K tokens, yet they do not imply that our models are limited to supporting only 32K tokens. You can modify \",[\"$\",\"code\",null,{\"children\":\"max_position_embedding\"}],\" and \",[\"$\",\"code\",null,{\"children\":\"sliding_window\"}],\" in \",[\"$\",\"code\",null,{\"children\":\"config.json\"}],\" to a larger value to see if the model performance is still satisfactory for your tasks.\"]}]\n31:[\"$\",\"h2\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"capabilities-to-connect-with-external-systems\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#capabilities-to-connect-with-external-systems\",\"className\":\"peer\",\"children\":\"Capabilities to Connect with External Systems\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n32:[\"$\",\"p\",null,{\"children\":\"Large language models (LLMs) are popular in part due to their ability to integrate external knowledge and tools. Retrieval-Augmented Generation (RAG) has gained traction as it mitigates common LLM issues like hallucination, real-time data shortage, and private information handling. Additionally, strong LLMs typically excel at using APIs and tools via function calling, making them ideal for serving as AI agents.\"}]\n33:[\"$\",\"p\",null,{\"children\":[\"We first assess the performance of Qwen1.5-Chat on \",[\"$\",\"$Lf\",null,{\"href\":\"https://arxiv.org/abs/2309.01431\",\"children\":\"RGB\"}],\", an RAG benchmark for which we have not performed any specific optimization:\"]}]\n34:[\"$\",\"h2\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"rgb-english-benchmark-for-retrieval-augmented-generation\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#rgb-english-benchmark-for-retrieval-augmented-generation\",\"className\":\"peer\",\"children\":\"RGB English Benchmark for Retrieval-Augmented Generation\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"35:[\"$\",\"p\",null,{\"children\":[\"Models| Noise 0.8 (Acc.↑)| Rejection 1.0 (Acc.↑)| Integration 0.4 (Acc.↑)| Counterfactual (Acc.↑)\",[\"$\",\"br\",null,{}],\"\\nGPT4-Turbo| 85.67| 47.33| 60.00| 90.00\",[\"$\",\"br\",null,{}],\"\\nGPT3.5-Turbo| 74.33| 27.67| 47.00| 21.00\",[\"$\",\"br\",null,{}],\"\\nLlama2-70B-Chat| 82.00| 31.00| 56.00| 15.00\",[\"$\",\"br\",null,{}],\"\\nMistral-7B-Instruct-v0.2| 82.00| 31.00| 56.00| 15.00\",[\"$\",\"br\",null,{}],\"\\nMixtral-8x7B-Instruct-v0.1| 82.67| 37.00| 67.00| 8.00\",[\"$\",\"br\",null,{}],\"\\nQwen1.5-7B-Chat| 77.67| 25.00| 52.00| 9.00\",[\"$\",\"br\",null,{}],\"\\nQwen1.5-14B-Chat| 80.67| 24.00| 60.00| 8.00\",[\"$\",\"br\",null,{}],\"\\nQwen1.5-72B-Chat| 81.67| 48.67| 61.00| 28.00\",[\"$\",\"br\",null,{}],\"\\nRGB Chinese Benchmark for Retrieval-Augmented Generation\",[\"$\",\"br\",null,{}],\"\\nModels| Noise 0.8 (Acc.↑)| Rejection 1.0 (Acc.↑)| Integration 0.4 (Acc.↑)| Counterfactual (Acc.↑)\",[\"$\",\"br\",null,{}],\"\\nGPT4-Turbo| 75.00| 38.67| 63.00| 90.00\",[\"$\",\"br\",null,{}],\"\\nGPT3.5-Turbo| 69.00| 13.00| 55.00| 25.00\",[\"$\",\"br\",null,{}],\"\\nLlama2-70B-Chat| 28.00| 17.00| 32.00| 8.00\",[\"$\",\"br\",null,{}],\"\\nMistral-7B-Instruct-v0.2| 54.67| 28.67| 37.00| 4.00\",[\"$\",\"br\",null,{}],\"\\nMixtral-8x7B-Instruct-v0.1| 27.33| 4.00| 24.00| 4.00\",[\"$\",\"br\",null,{}],\"\\nQwen1.5-7B-Chat| 71.00| 10.33| 54.00| 20.00\",[\"$\",\"br\",null,{}],\"\\nQwen1.5-14B-Chat| 75.00| 16.67| 55.00| 22.00\",[\"$\",\"br\",null,{}],\"\\nQwen1.5-72B-Chat| 76.00| 51.00| 66.00| 44.00\"]}]\n"])</script><script>self.__next_f.push([1,"36:[\"$\",\"p\",null,{\"children\":[\"We then assess Qwen’s capacity to function as a general-purpose agent by testing it on the \",[\"$\",\"$Lf\",null,{\"href\":\"https://open-compass.github.io/T-Eval/\",\"children\":\"T-Eval\"}],\" benchmark. None of the Qwen models have undergone any optimization tailored specifically for this benchmark:\"]}]\n37:[\"$\",\"h2\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"agent-performance-on-t-eval-english\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#agent-performance-on-t-eval-english\",\"className\":\"peer\",\"children\":\"Agent Performance on T-Eval English\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"38:[\"$\",\"p\",null,{\"children\":[\"Models| Overall| Instruct| Plan| Reason| Retrieve| Understand| Review\",[\"$\",\"br\",null,{}],\"\\nGPT4-Turbo| 86.4| 96.3| 87.8| 65.3| 88.9| 85.8| 94.5\",[\"$\",\"br\",null,{}],\"\\nLlama-2-70B-Chat| 58.59| 77.80| 63.75| 39.07| 51.35| 50.34| 69.20\",[\"$\",\"br\",null,{}],\"\\nMistral-7B-Instruct-v0.2| 46.68| 63.57| 60.88| 32.59| 17.58| 38.08| 67.35\",[\"$\",\"br\",null,{}],\"\\nMixtral-8x7B-Instruct-v0.1| 62.15| 42.39| 46.48| 60.35| 76.69| 73.70| 73.31\",[\"$\",\"br\",null,{}],\"\\nQwen1.5-7B-Chat| 59.67| 71.12| 62.95| 37.60| 61.17| 53.75| 71.46\",[\"$\",\"br\",null,{}],\"\\nQwen1.5-14B-Chat| 71.77| 86.16| 73.09| 49.51| 72.07| 66.03| 83.78\",[\"$\",\"br\",null,{}],\"\\nQwen1.5-72B-Chat| 76.69| 80.96| 83.12| 56.89| 80.17| 76.68| 82.34\",[\"$\",\"br\",null,{}],\"\\nAgent Performance on T-Eval Chinese\",[\"$\",\"br\",null,{}],\"\\nModels| Overall| Instruct| Plan| Reason| Retrieve| Understand| Review\",[\"$\",\"br\",null,{}],\"\\nGPT4-Turbo| 85.9| 97.6| 87.0| 68.4| 89.2| 86.8| 86.0\",[\"$\",\"br\",null,{}],\"\\nLlama-2-70B-Chat| 51.15| 53.78| 56.65| 34.27| 48.24| 50.49| 63.45\",[\"$\",\"br\",null,{}],\"\\nMistral-7B-Instruct-v0.2| 46.26| 49.64| 61.82| 36.17| 20.26| 47.25| 62.42\",[\"$\",\"br\",null,{}],\"\\nMixtral-8x7B-Instruct-v0.1| 62.77| 26.38| 60.79| 62.02| 76.60| 77.74| 73.10\",[\"$\",\"br\",null,{}],\"\\nQwen1.5-7B-Chat| 53.15| 60.56| 62.31| 42.07| 55.28| 55.76| 42.92\",[\"$\",\"br\",null,{}],\"\\nQwen1.5-14B-Chat| 64.85| 84.25| 64.77| 54.68| 72.35| 68.88| 44.15\",[\"$\",\"br\",null,{}],\"\\nQwen1.5-72B-Chat| 72.88| 97.50| 80.83| 58.11| 76.14| 71.94| 52.77\"]}]\n"])</script><script>self.__next_f.push([1,"39:[\"$\",\"p\",null,{\"children\":[\"To test the capabilities of tool using, also known as function calling, we follow our previous practice and use our opensourced evaluation \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/QwenLM/Qwen/blob/main/eval/evaluate_plugin.py\",\"children\":\"benchmark\"}],\" for assessing the models’ ability to appropriately select and use tools:\"]}]\n3a:[\"$\",\"h2\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"tool-use-benchmark\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#tool-use-benchmark\",\"className\":\"peer\",\"children\":\"Tool-Use Benchmark\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n3b:[\"$\",\"p\",null,{\"children\":[\"Models| Tool Selection (Acc.↑)| Tool Input (Rouge-L↑)| False Positive (Acc.↑)\",[\"$\",\"br\",null,{}],\"\\nGPT-4| 98.0| 95.3| 76.1\",[\"$\",\"br\",null,{}],\"\\nGPT-3.5| 74.5| 80.7| 19.4\",[\"$\",\"br\",null,{}],\"\\nLlama-2-70B-Chat| 88.54| 70.36| 0.37\",[\"$\",\"br\",null,{}],\"\\nMistral-7B-Instruct-v0.2| 94.79| 82.81| 6.34\",[\"$\",\"br\",null,{}],\"\\nMixtral-8x7B-Instruct-v0.1| 99.31| 94.46| 31.34\",[\"$\",\"br\",null,{}],\"\\nQwen1.5-7B-Chat| 95.83| 89.48| 92.54\",[\"$\",\"br\",null,{}],\"\\nQwen1.5-14B-Chat| 93.06| 88.74| 92.91\",[\"$\",\"br\",null,{}],\"\\nQwen1.5-72B-Chat| 95.14| 91.14| 98.51\"]}]\n3c:[\"$\",\"p\",null,{\"children\":[\"Finally, since the Python code interpreter has emerged as an increasingly powerful tool for advanced LLMs, we also evaluate our models’ capability in utilizing this tool on our previously open-sourced \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark\",\"children\":\"benchmark\"}],\":\"]}]\n3d:[\"$\",\"h2\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"code-interpreter-benchmark\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#code-interpreter-benchmark\",\"className\":\"peer\",\"children\":\"Code Interpreter Benchmark\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n3e:[\"$\",\"p\",null,{\"children\":[\"Models| Accuracy of Code Execution Results (%)| Executable Rate of Code (%)\",[\"$\",\"br\",null,{}],\"\\nMath↑| Visualization-Hard↑| Visualization-Easy↑| General↑\",[\"$\",\"br\",null,{}],\"\\nGPT-4| 82.8| 66.7| 60.8| 82.8\",[\"$\",\"br\",null,{}],\"\\nGPT-3.5| 47.3| 33.3| 55.7| 74.1\",[\"$\",\"br\",null,{}],\"\\nMistral-7B-Instruct-v0.2| 25.5| 19.1| 44.3| 62.1\",[\"$\",\"br\",null,{}],\"\\nMixtral-8x7B-Instruct-v0.1| 47.8| 33.3| 54.4| 60.3\",[\"$\",\"br\",null,{}],\"\\nQwen1.5-7B-Chat| 54.0| 35.7| 36.7| 65.5\",[\"$\",\"br\",null,{}],\"\\nQwen1.5-14B-Chat| 62.1| 46.4| 48.1| 70.6\",[\"$\",\"br\",null,{}],\"\\nQwen1.5-72B-Chat| 73.1| 52.3| 50.6| 87.9\"]}]\n3f:[\"$\",\"p\",null,{\"children\":\"Larger Qwen1.5-Chat models generally outperform smaller ones, nearing GPT-4’s tool-use performance. However, in code interpreter tasks like math problem-solving and visualization, even the largest Qwen1.5-72B-Chat model lags significantly behind GPT-4 due to coding capabilities. We aim in future versions to enhance the coding capabilities of all Qwen models during both pre-training and alignment.\"}]\n40:[\"$\",\"h1\",null,{\"className\":\"flex scroll-m-28 flex-row it"])</script><script>self.__next_f.push([1,"ems-center gap-2\",\"id\":\"develop-with-qwen15\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#develop-with-qwen15\",\"className\":\"peer\",\"children\":\"Develop with Qwen1.5\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n41:[\"$\",\"p\",null,{\"children\":\"The biggest difference in Qwen1.5 is the integration of Qwen1.5 to Hugging Face transformers. Since 4.37.0, you can use Qwen1.5 without our custom code, which means that you can load the model like the following:\"}]\n58:T5c0,"])</script><script>self.__next_f.push([1,"\u003csvg viewBox=\"0 0 24 24\"\u003e\u003cpath d=\"M14.25.18l.9.2.73.26.59.3.45.32.34.34.25.34.16.33.1.3.04.26.02.2-.01.13V8.5l-.05.63-.13.55-.21.46-.26.38-.3.31-.33.25-.35.19-.35.14-.33.1-.3.07-.26.04-.21.02H8.77l-.69.05-.59.14-.5.22-.41.27-.33.32-.27.35-.2.36-.15.37-.1.35-.07.32-.04.27-.02.21v3.06H3.17l-.21-.03-.28-.07-.32-.12-.35-.18-.36-.26-.36-.36-.35-.46-.32-.59-.28-.73-.21-.88-.14-1.05-.05-1.23.06-1.22.16-1.04.24-.87.32-.71.36-.57.4-.44.42-.33.42-.24.4-.16.36-.1.32-.05.24-.01h.16l.06.01h8.16v-.83H6.18l-.01-2.75-.02-.37.05-.34.11-.31.17-.28.25-.26.31-.23.38-.2.44-.18.51-.15.58-.12.64-.1.71-.06.77-.04.84-.02 1.27.05zm-6.3 1.98l-.23.33-.08.41.08.41.23.34.33.22.41.09.41-.09.33-.22.23-.34.08-.41-.08-.41-.23-.33-.33-.22-.41-.09-.41.09zm13.09 3.95l.28.06.32.12.35.18.36.27.36.35.35.47.32.59.28.73.21.88.14 1.04.05 1.23-.06 1.23-.16 1.04-.24.86-.32.71-.36.57-.4.45-.42.33-.42.24-.4.16-.36.09-.32.05-.24.02-.16-.01h-8.22v.82h5.84l.01 2.76.02.36-.05.34-.11.31-.17.29-.25.25-.31.24-.38.2-.44.17-.51.15-.58.13-.64.09-.71.07-.77.04-.84.01-1.27-.04-1.07-.14-.9-.2-.73-.25-.59-.3-.45-.33-.34-.34-.25-.34-.16-.33-.1-.3-.04-.25-.02-.2.01-.13v-5.34l.05-.64.13-.54.21-.46.26-.38.3-.32.33-.24.35-.2.35-.14.33-.1.3-.06.26-.04.21-.02.13-.01h5.84l.69-.05.59-.14.5-.21.41-.28.33-.32.27-.35.2-.36.15-.36.1-.35.07-.32.04-.28.02-.21V6.07h2.09l.14.01zm-6.47 14.25l-.23.33-.08.41.08.41.23.33.33.23.41.08.41-.08.33-.23.23-.33.08-.41-.08-.41-.23-.33-.33-.23-.41-.08-.41.08z\" fill=\"currentColor\" /\u003e\u003c/svg\u003e"])</script><script>self.__next_f.push([1,"42:[\"$\",\"$L57\",null,{\"className\":\"shiki shiki-themes github-light github-dark\",\"style\":{\"--shiki-light\":\"#24292e\",\"--shiki-dark\":\"#e1e4e8\",\"--shiki-light-bg\":\"#fff\",\"--shiki-dark-bg\":\"#24292e\"},\"tabIndex\":\"0\",\"icon\":\"$58\",\"children\":[\"$\",\"$L59\",null,{\"children\":[\"$\",\"code\",null,{\"children\":[[\"$\",\"span\",null,{\"className\":\"line\"}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"    from\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\" transformers \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"import\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\" AutoModelForCausalLM\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#6A737D\",\"--shiki-dark\":\"#6A737D\"},\"children\":\"    # This is what we previously used\"}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    model \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\" AutoModelForCausalLM.from_pretrained(\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\"\\\"Qwen/Qwen-7B-Chat\\\"\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\", \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#E36209\",\"--shiki-dark\":\"#FFAB70\"},\"children\":\"device_map\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\"\\\"auto\\\"\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\", \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#E36209\",\"--shiki-dark\":\"#FFAB70\"},\"children\":\"trust_remote_code\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#005CC5\",\"--shiki-dark\":\"#79B8FF\"},\"children\":\"True\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\")\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#6A737D\",\"--shiki-dark\":\"#6A737D\"},\"children\":\"    # This is what you can use now\"}]}],\"\\n\",\"$L5a\",\"\\n\",\"$L5b\"]}]}]}]\n"])</script><script>self.__next_f.push([1,"43:[\"$\",\"p\",null,{\"children\":\"The usage of Qwen1.5 for chat is different from the previous version. You can use the following code to chat with Qwen1.5:\"}]\n5c:T5c0,"])</script><script>self.__next_f.push([1,"\u003csvg viewBox=\"0 0 24 24\"\u003e\u003cpath d=\"M14.25.18l.9.2.73.26.59.3.45.32.34.34.25.34.16.33.1.3.04.26.02.2-.01.13V8.5l-.05.63-.13.55-.21.46-.26.38-.3.31-.33.25-.35.19-.35.14-.33.1-.3.07-.26.04-.21.02H8.77l-.69.05-.59.14-.5.22-.41.27-.33.32-.27.35-.2.36-.15.37-.1.35-.07.32-.04.27-.02.21v3.06H3.17l-.21-.03-.28-.07-.32-.12-.35-.18-.36-.26-.36-.36-.35-.46-.32-.59-.28-.73-.21-.88-.14-1.05-.05-1.23.06-1.22.16-1.04.24-.87.32-.71.36-.57.4-.44.42-.33.42-.24.4-.16.36-.1.32-.05.24-.01h.16l.06.01h8.16v-.83H6.18l-.01-2.75-.02-.37.05-.34.11-.31.17-.28.25-.26.31-.23.38-.2.44-.18.51-.15.58-.12.64-.1.71-.06.77-.04.84-.02 1.27.05zm-6.3 1.98l-.23.33-.08.41.08.41.23.34.33.22.41.09.41-.09.33-.22.23-.34.08-.41-.08-.41-.23-.33-.33-.22-.41-.09-.41.09zm13.09 3.95l.28.06.32.12.35.18.36.27.36.35.35.47.32.59.28.73.21.88.14 1.04.05 1.23-.06 1.23-.16 1.04-.24.86-.32.71-.36.57-.4.45-.42.33-.42.24-.4.16-.36.09-.32.05-.24.02-.16-.01h-8.22v.82h5.84l.01 2.76.02.36-.05.34-.11.31-.17.29-.25.25-.31.24-.38.2-.44.17-.51.15-.58.13-.64.09-.71.07-.77.04-.84.01-1.27-.04-1.07-.14-.9-.2-.73-.25-.59-.3-.45-.33-.34-.34-.25-.34-.16-.33-.1-.3-.04-.25-.02-.2.01-.13v-5.34l.05-.64.13-.54.21-.46.26-.38.3-.32.33-.24.35-.2.35-.14.33-.1.3-.06.26-.04.21-.02.13-.01h5.84l.69-.05.59-.14.5-.21.41-.28.33-.32.27-.35.2-.36.15-.36.1-.35.07-.32.04-.28.02-.21V6.07h2.09l.14.01zm-6.47 14.25l-.23.33-.08.41.08.41.23.33.33.23.41.08.41-.08.33-.23.23-.33.08-.41-.08-.41-.23-.33-.33-.23-.41-.08-.41.08z\" fill=\"currentColor\" /\u003e\u003c/svg\u003e"])</script><script>self.__next_f.push([1,"44:[\"$\",\"$L57\",null,{\"className\":\"shiki shiki-themes github-light github-dark\",\"style\":{\"--shiki-light\":\"#24292e\",\"--shiki-dark\":\"#e1e4e8\",\"--shiki-light-bg\":\"#fff\",\"--shiki-dark-bg\":\"#24292e\"},\"tabIndex\":\"0\",\"icon\":\"$5c\",\"children\":[\"$\",\"$L59\",null,{\"children\":[\"$\",\"code\",null,{\"children\":[[\"$\",\"span\",null,{\"className\":\"line\"}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"    from\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\" transformers \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"import\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\" AutoModelForCausalLM, AutoTokenizer\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    device \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\" \\\"cuda\\\"\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#6A737D\",\"--shiki-dark\":\"#6A737D\"},\"children\":\" # the device to load the model onto\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    \"}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    model \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\" AutoModelForCausalLM.from_pretrained(\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\"        \\\"Qwen/Qwen1.5-14B-Chat-AWQ\\\"\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\",\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#E36209\",\"--shiki-dark\":\"#FFAB70\"},\"children\":\"        device_map\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\"\\\"auto\\\"\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    )\"}]}],\"\\n\",\"$L5d\",\"\\n\",\"$L5e\",\"\\n\",\"$L5f\",\"\\n\",\"$L60\",\"\\n\",\"$L61\",\"\\n\",\"$L62\",\"\\n\",\"$L63\",\"\\n\",\"$L64\",\"\\n\",\"$L65\",\"\\n\",\"$L66\",\"\\n\",\"$L67\",\"\\n\",\"$L68\",\"\\n\",\"$L69\",\"\\n\",\"$L6a\",\"\\n\",\"$L6b\",\"\\n\",\"$L6c\",\"\\n\",\"$L6d\",\"\\n\",\"$L6e\",\"\\n\",\"$L6f\",\"\\n\",\"$L70\",\"\\n\",\"$L71\",\"\\n\",\"$L72\",\"\\n\",\"$L73\",\"\\n\",\"$L74\"]}]}]}]\n"])</script><script>self.__next_f.push([1,"45:[\"$\",\"p\",null,{\"children\":[\"For chat models, we no longer use a specific \",[\"$\",\"code\",null,{\"children\":\"model.chat()\"}],\" method, but instead we use \",[\"$\",\"code\",null,{\"children\":\"model.generate()\"}],\" with the chat template written in \",[\"$\",\"code\",null,{\"children\":\"tokenizer_config.json\"}],\" so that we can use \",[\"$\",\"code\",null,{\"children\":\"tokenizer.apply_chat_template()\"}],\" to generate the input, and we use \",[\"$\",\"code\",null,{\"children\":\"eos_token\"}],\" to control when to stop the generation.\"]}]\n46:[\"$\",\"p\",null,{\"children\":[\"We also provide AWQ models and GPTQ models (including Int4 and Int8 models) for you to use Qwen1.5 in low-resource or deployment scenarios. As Huggingface transformers supports \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/casper-hansen/AutoAWQ\",\"children\":\"AWQ\"}],\" and \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/AutoGPTQ/AutoGPTQ\",\"children\":\"GPTQ\"}],\", you can use them in the same way above only with the corresponding model names.\"]}]\n47:[\"$\",\"p\",null,{\"children\":[\"Furthermore, we have integrated our code to popular inference frameworks so that you can deploy your model easily. Now \",[\"$\",\"code\",null,{\"children\":\"vLLM\u003e=0.3.0\"}],\" and \",[\"$\",\"code\",null,{\"children\":\"SGLang\u003e=0.1.11\"}],\" officially support Qwen1.5. Check their official github repos and docs to learn about the detailed usage. Here we demonstrate an example to show how to use vLLM to build an OpenAI-API compatible interface for our model:\"]}]\n48:[\"$\",\"$L57\",null,{\"className\":\"shiki shiki-themes github-light github-dark\",\"style\":{\"--shiki-light\":\"#24292e\",\"--shiki-dark\":\"#e1e4e8\",\"--shiki-light-bg\":\"#fff\",\"--shiki-dark-bg\":\"#24292e\"},\"tabIndex\":\"0\",\"icon\":\"\u003csvg viewBox=\\\"0 0 24 24\\\"\u003e\u003cpath d=\\\"M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z\\\" fill=\\\"currentColor\\\" /\u003e\u003c/svg\u003e\",\"children\":[\"$\",\"$L59\",null,{\"children\":[\"$\",\"code\",null,{\"children\":[[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"children\":\"    python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen1.5-7B-Chat\"}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"children\":\"    \"}]}]]}]}]}]\n"])</script><script>self.__next_f.push([1,"49:[\"$\",\"$L57\",null,{\"className\":\"shiki shiki-themes github-light github-dark\",\"style\":{\"--shiki-light\":\"#24292e\",\"--shiki-dark\":\"#e1e4e8\",\"--shiki-light-bg\":\"#fff\",\"--shiki-dark-bg\":\"#24292e\"},\"tabIndex\":\"0\",\"icon\":\"\u003csvg viewBox=\\\"0 0 24 24\\\"\u003e\u003cpath d=\\\"m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z\\\" fill=\\\"currentColor\\\" /\u003e\u003c/svg\u003e\",\"children\":[\"$\",\"$L59\",null,{\"children\":[\"$\",\"code\",null,{\"children\":[[\"$\",\"span\",null,{\"className\":\"line\"}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#6F42C1\",\"--shiki-dark\":\"#B392F0\"},\"children\":\"    curl\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\" http://localhost:8000/v1/chat/completions\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#005CC5\",\"--shiki-dark\":\"#79B8FF\"},\"children\":\" \\\\\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#005CC5\",\"--shiki-dark\":\"#79B8FF\"},\"children\":\"        -H\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\" \\\"Content-Type: application/json\\\"\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#005CC5\",\"--shiki-dark\":\"#79B8FF\"},\"children\":\" \\\\\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#005CC5\",\"--shiki-dark\":\"#79B8FF\"},\"children\":\"        -d\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\" '{\"}]]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\"        \\\"model\\\": \\\"Qwen/Qwen1.5-7B-Chat\\\",\"}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\"        \\\"messages\\\": [\"}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\"        {\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are a helpful assistant.\\\"},\"}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\"        {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Tell me something about large language models.\\\"}\"}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\"        ]\"}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\"        }'\"}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    \"}]}]]}]}]}]\n"])</script><script>self.__next_f.push([1,"4a:[\"$\",\"p\",null,{\"children\":\"For users to run LLM locally, llama.cpp also provides support to Qwen1.5, and we officially provide quantized models in the GGUF format in our HF model hub. You can use the following code to run Qwen1.5 in llama.cpp:\"}]\n4b:[\"$\",\"$L57\",null,{\"className\":\"shiki shiki-themes github-light github-dark\",\"style\":{\"--shiki-light\":\"#24292e\",\"--shiki-dark\":\"#e1e4e8\",\"--shiki-light-bg\":\"#fff\",\"--shiki-dark-bg\":\"#24292e\"},\"tabIndex\":\"0\",\"icon\":\"\u003csvg viewBox=\\\"0 0 24 24\\\"\u003e\u003cpath d=\\\"M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z\\\" fill=\\\"currentColor\\\" /\u003e\u003c/svg\u003e\",\"children\":[\"$\",\"$L59\",null,{\"children\":[\"$\",\"code\",null,{\"children\":[[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"children\":\"    ./main -m qwen1.5-7b-chat-q2_k.gguf -n 512 --color -i -cml -f prompts/chat-with-qwen.txt\"}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"children\":\"    \"}]}]]}]}]}]\n4c:[\"$\",\"p\",null,{\"children\":[\"Besides, you can use the GGUF file with Ollama. Thanks to the support of \",[\"$\",\"$Lf\",null,{\"href\":\"https://ollama.ai/\",\"children\":\"Ollama\"}],\", you can now directly use one line of command:\"]}]\n4d:[\"$\",\"$L57\",null,{\"className\":\"shiki shiki-themes github-light github-dark\",\"style\":{\"--shiki-light\":\"#24292e\",\"--shiki-dark\":\"#e1e4e8\",\"--shiki-light-bg\":\"#fff\",\"--shiki-dark-bg\":\"#24292e\"},\"tabIndex\":\"0\",\"icon\":\"\u003csvg viewBox=\\\"0 0 24 24\\\"\u003e\u003cpath d=\\\"M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z\\\" fill=\\\"currentColor\\\" /\u003e\u003c/svg\u003e\",\"children\":[\"$\",\"$L59\",null,{\"children\":[\"$\",\"code\",null,{\"children\":[[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"children\":\"    ollama run qwen\"}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"children\":\"    \"}]}]]}]}]}]\n4e:[\"$\",\"p\",null,{\"children\":[\"Or you can use the GGUF file to play with \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/Mozilla-Ocho/llamafile\",\"children\":\"llamafile\"}],\" to run our models with a single file.\"]}]\n4f:[\"$\",\"p\",null,{\"children\":[\"To make a web demo locally, we advise you to use \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/oobabooga/text-generation-webui\",\"children\":\"Text generation web UI\"}],\" which is very easy to use.\"]}]\n50:[\"$\",\"p\",null,{\"children\":[\"For advanced developers that hope to train better or more suitable models for themselves, such as post-training, Qwen1.5 is supported by Hugging face \",[\"$\",\"code\",null,{\"children\":\"trainer\"}],\" and Peft. Also, there are easy-to-use frameworks that support both supervised finetuning (SFT) and alignment (PPO, DPO, etc.). Now, both \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/hiyouga/LLaMA-Factory\",\"children\":\"LLaMA-Factory\"}],\" and \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/OpenAccess-AI-Collective/axolotl\",\"children\":\"Axolotl\"}],\" have supported the training of Qwen1.5. We advise you to turn to their official github repos and docs for more advanced usages.\"]}]\n51:[\"$\",\"p\",null,{\"children\":[\"If you would like to use Qwen1.5 for downstream applications, such as RAG, tool use, agent, you can now build OpenAI-API compatible API or run local models for famous frameworks, e.g., \",[\"$\",\"$Lf\",null,{\"href\":\"https://www.llamaindex.ai/\",\"children\":\"LlamaIndex\"}],\", \",[\"$"])</script><script>self.__next_f.push([1,"\",\"$Lf\",null,{\"href\":\"https://www.langchain.com/\",\"children\":\"LangChain\"}],\", \",[\"$\",\"$Lf\",null,{\"href\":\"https://www.crewai.io/\",\"children\":\"CrewAI\"}],\".\"]}]\n52:[\"$\",\"p\",null,{\"children\":\"Overall, as we care about your developing experience, we not only have tried our best to provide good models to the community but also have made efforts to make things easier for all of you. We hope that you can enjoy using Qwen1.5 and that it can help you with your tasks of either research or applications.\"}]\n53:[\"$\",\"h1\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"conclusion\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#conclusion\",\"className\":\"peer\",\"children\":\"Conclusion\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n54:[\"$\",\"p\",null,{\"children\":[\"We are excited to introduce Qwen1.5, the next version of our Qwen series. In this release, we have opensourced both base and chat models of 6 sizes, including 0.5B, 1.8B, 4B, 7B, 14B, and 72B, and we have also provided quantized models. We have merged our code of Qwen1.5 to Hugging face transformers, and you can directly use it with \",[\"$\",\"code\",null,{\"children\":\"transformers\u003e=4.37.0\"}],\" without \",[\"$\",\"code\",null,{\"children\":\"trust_remote_code\"}],\". Additionally, we have had frameworks, e.g., vLLM, SGLang, AutoGPTQ, etc., supported Qwen1.5. We believe from now on, using our models will be much easier. We believe that this release is though a small step towards model quality, but it is a big step towards developer experience. Hope you like it and enjoy using it. Join our \",[\"$\",\"$Lf\",null,{\"href\":\"https://discord.gg/yPEP2vHTu4\",\"children\":\"Discord\"}],\" or \",[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\",\"children\":\"WeChat\"}],\" to share your experience, comments, or whatever you like with us. We are looking forward to hearing from you.\"]}]\n55:[\"$\",\"h1\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"citation\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#citation\",\"className\":\"peer\",\"children\":\"Citation\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"56:[\"$\",\"$L57\",null,{\"className\":\"shiki shiki-themes github-light github-dark\",\"style\":{\"--shiki-light\":\"#24292e\",\"--shiki-dark\":\"#e1e4e8\",\"--shiki-light-bg\":\"#fff\",\"--shiki-dark-bg\":\"#24292e\"},\"tabIndex\":\"0\",\"icon\":\"\u003csvg viewBox=\\\"0 0 24 24\\\"\u003e\u003cpath d=\\\"M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z\\\" fill=\\\"currentColor\\\" /\u003e\u003c/svg\u003e\",\"children\":[\"$\",\"$L59\",null,{\"children\":[\"$\",\"code\",null,{\"children\":[[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"children\":\"    @misc{qwen1.5,\"}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"children\":\"        title = {Introducing Qwen1.5},\"}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"children\":\"        url = {https://qwenlm.github.io/blog/qwen1.5/},\"}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"children\":\"        author = {Qwen Team},\"}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"children\":\"        month = {February},\"}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"children\":\"        year = {2024}\"}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"children\":\"    }\"}]}],\"\\n\",[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"children\":\"    \"}]}]]}]}]}]\n"])</script><script>self.__next_f.push([1,"5a:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    model \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\" AutoModelForCausalLM.from_pretrained(\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\"\\\"Qwen/Qwen1.5-7B-Chat\\\"\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\", \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#E36209\",\"--shiki-dark\":\"#FFAB70\"},\"children\":\"device_map\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\"\\\"auto\\\"\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\")\"}]]}]\n5b:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    \"}]}]\n5d:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    tokenizer \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\" AutoTokenizer.from_pretrained(\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\"\\\"Qwen/Qwen1.5-14B-Chat-AWQ\\\"\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\")\"}]]}]\n5e:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    \"}]}]\n5f:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    prompt \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\" \\\"Give me a short introduction to large language model.\\\"\"}]]}]\n60:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    messages \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\" [\"}]]}]\n61:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"        {\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\"\\\"role\\\"\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\": \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\"\\\"system\\\"\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\", \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\"\\\"content\\\"\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\": \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\"\\\"You are a helpful assistant.\\\"\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"},\"}]]}]\n62:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"        {\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\"\\\"role\\\"\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\": \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9"])</script><script>self.__next_f.push([1,"ECBFF\"},\"children\":\"\\\"user\\\"\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\", \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\"\\\"content\\\"\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\": prompt}\"}]]}]\n63:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    ]\"}]}]\n64:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    text \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\" tokenizer.apply_chat_template(\"}]]}]\n65:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"        messages,\"}]}]\n66:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#E36209\",\"--shiki-dark\":\"#FFAB70\"},\"children\":\"        tokenize\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#005CC5\",\"--shiki-dark\":\"#79B8FF\"},\"children\":\"False\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\",\"}]]}]\n67:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#E36209\",\"--shiki-dark\":\"#FFAB70\"},\"children\":\"        add_generation_prompt\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#005CC5\",\"--shiki-dark\":\"#79B8FF\"},\"children\":\"True\"}]]}]\n68:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    )\"}]}]\n69:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    model_inputs \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\" tokenizer([text], \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#E36209\",\"--shiki-dark\":\"#FFAB70\"},\"children\":\"return_tensors\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#032F62\",\"--shiki-dark\":\"#9ECBFF\"},\"children\":\"\\\"pt\\\"\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\").to(device)\"}]]}]\n6a:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    \"}]}]\n6b:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    generated_ids \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\" model.generate(\"}]]}]\n6c:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"        model_inputs.input_ids,\"}]}]\n6d:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#E36209\",\"--shiki-dark\":\"#FFAB70\"},\"children\":\"        max_new_tokens\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#005CC5\",\"--shiki-dark\":\"#79B8FF\"},\"children\":\"512\"}]]}]\n6e:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    )\"}]}]\n6f:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$"])</script><script>self.__next_f.push([1,"\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    generated_ids \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\" [\"}]]}]\n70:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"        output_ids[\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#005CC5\",\"--shiki-dark\":\"#79B8FF\"},\"children\":\"len\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"(input_ids):] \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"for\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\" input_ids, output_ids \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"in\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#005CC5\",\"--shiki-dark\":\"#79B8FF\"},\"children\":\" zip\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"(model_inputs.input_ids, generated_ids)\"}]]}]\n71:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    ]\"}]}]\n72:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    \"}]}]\n73:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    response \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\" tokenizer.batch_decode(generated_ids, \"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#E36209\",\"--shiki-dark\":\"#FFAB70\"},\"children\":\"skip_special_tokens\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#D73A49\",\"--shiki-dark\":\"#F97583\"},\"children\":\"=\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#005CC5\",\"--shiki-dark\":\"#79B8FF\"},\"children\":\"True\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\")[\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#005CC5\",\"--shiki-dark\":\"#79B8FF\"},\"children\":\"0\"}],[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"]\"}]]}]\n74:[\"$\",\"span\",null,{\"className\":\"line\",\"children\":[\"$\",\"span\",null,{\"style\":{\"--shiki-light\":\"#24292E\",\"--shiki-dark\":\"#E1E4E8\"},\"children\":\"    \"}]}]\n"])</script><script>self.__next_f.push([1,"b:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"2\",{\"name\":\"theme-color\",\"media\":\"(prefers-color-scheme: dark)\",\"content\":\"#0A0A0A\"}],[\"$\",\"meta\",\"3\",{\"name\":\"theme-color\",\"media\":\"(prefers-color-scheme: light)\",\"content\":\"#fff\"}]]\n"])</script><script>self.__next_f.push([1,"9:null\nd:[[\"$\",\"title\",\"0\",{\"children\":\"Introducing Qwen1.5 — Zen LM Blog | Zen LM\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\"}],[\"$\",\"meta\",\"2\",{\"property\":\"og:title\",\"content\":\"Zen LM - Open Foundation Models\"}],[\"$\",\"meta\",\"3\",{\"property\":\"og:description\",\"content\":\"Zen AI model family by Hanzo AI. 14 frontier models from 4B to 1T+ parameters for code, reasoning, vision, and multimodal tasks.\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:url\",\"content\":\"https://zenlm.org\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:site_name\",\"content\":\"Zen LM\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"7\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"8\",{\"name\":\"twitter:site\",\"content\":\"@zenlmorg\"}],[\"$\",\"meta\",\"9\",{\"name\":\"twitter:creator\",\"content\":\"@zenlmorg\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:title\",\"content\":\"Zen LM - Open Foundation Models\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:description\",\"content\":\"Zen AI model family by Hanzo AI. 14 frontier models from 4B to 1T+ parameters for code, reasoning, vision, and multimodal tasks.\"}]]\n"])</script></body></html>