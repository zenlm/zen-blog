1:"$Sreact.fragment"
2:I[10086,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/8de849ca74fc071f.js","/_next/static/chunks/e62b91212ee7f8ff.js","/_next/static/chunks/f19fe44237e54646.js","/_next/static/chunks/cb0a883bafeb6805.js"],""]
3:I[48068,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/8de849ca74fc071f.js","/_next/static/chunks/e62b91212ee7f8ff.js","/_next/static/chunks/f19fe44237e54646.js","/_next/static/chunks/cb0a883bafeb6805.js"],"default"]
41:I[51504,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/8de849ca74fc071f.js","/_next/static/chunks/e62b91212ee7f8ff.js","/_next/static/chunks/f19fe44237e54646.js","/_next/static/chunks/cb0a883bafeb6805.js"],"CodeBlock"]
42:I[51504,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/8de849ca74fc071f.js","/_next/static/chunks/e62b91212ee7f8ff.js","/_next/static/chunks/f19fe44237e54646.js","/_next/static/chunks/cb0a883bafeb6805.js"],"Pre"]
59:I[89923,["/_next/static/chunks/4d80e004cf4896dd.js","/_next/static/chunks/350ee4303b732916.js"],"OutletBoundary"]
5a:"$Sreact.suspense"
0:{"buildId":"i-dnJM_MIpJSOCQWNJVMq","rsc":["$","$1","c",{"children":[["$","main",null,{"className":"mx-auto w-full max-w-2xl px-4 py-16","children":[["$","$L2",null,{"href":"/blog","className":"inline-flex items-center gap-1.5 text-sm text-fd-muted-foreground hover:text-fd-foreground mb-10 transition-colors","children":"← Back to Blog"}],["$","div",null,{"className":"mb-8","children":[["$","time",null,{"className":"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider","children":"January 26, 2025"}],["$","h1",null,{"className":"text-3xl font-bold mt-2 mb-3","children":"zen-1M: Deploy Your Own Qwen with Context Length up to 1M Tokens"}],["$","p",null,{"className":"text-fd-muted-foreground text-lg mb-4","children":"Tech Report HuggingFace ModelScope Qwen Chat HuggingFace Demo ModelScope Demo DISCORD"}],["$","div",null,{"className":"flex items-center gap-3 pt-4 border-t border-fd-border","children":[["$","span",null,{"className":"text-sm text-fd-muted-foreground","children":["By ","Zen LM Team"]}],["$","div",null,{"className":"flex gap-1.5 ml-auto","children":[]}]]}]]}],["$","div",null,{"className":"prose dark:prose-invert max-w-none","children":[["$","p",null,{"children":[["$","$L3",null,{"href":"https://qianwen-res.oss-cn-beijing.aliyuncs.com/zen-1M/zen_1M_Technical_Report.pdf","children":"Tech Report"}]," ",["$","$L3",null,{"href":"https://huggingface.co/Qwen","children":"HuggingFace"}]," ",["$","$L3",null,{"href":"https://modelscope.cn/organization/qwen","children":"ModelScope"}]," ",["$","$L3",null,{"href":"https://chat.qwenlm.ai/","children":"Qwen Chat"}]," ",["$","$L3",null,{"href":"https://huggingface.co/spaces/Qwen/zen-1M-Demo","children":"HuggingFace Demo"}]," ",["$","$L3",null,{"href":"https://www.modelscope.cn/studios/Qwen/zen-1M-Demo","children":"ModelScope Demo"}]," ",["$","$L3",null,{"href":"https://discord.gg/yPEP2vHTu4","children":"DISCORD"}]]}],"\n",["$","h1",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"introduction","children":[["$","a",null,{"data-card":"","href":"#introduction","className":"peer","children":"Introduction"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}],"\n",["$","p",null,{"children":["Two months after upgrading ",["$","$L3",null,{"href":"../qwen3-turbo","children":"zen-Turbo"}]," to support context length up to one million tokens, we are back with the open-source zen-1M models and the corresponding inference framework support. Here’s what you can expect from this release:"]}],"\n",["$","ol",null,{"children":["\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Opensource Models:"}]," We’re releasing two new checkpoints, ",["$","strong",null,{"children":"zen-7B-Instruct-1M"}]," and ",["$","strong",null,{"children":"zen-14B-Instruct-1M"}]," , marking the first time we’ve upgraded our opensource Qwen models to handle 1M-token contexts."]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Inference Framework:"}]," To help developers deploy the zen-1M series models more efficiently, we’ve fully open-sourced our inference framework based on ",["$","$L3",null,{"href":"https://github.com/vllm-project/vllm","children":"vLLM"}],". With integration with sparse attention methods, our framework can process 1M-token inputs ",["$","strong",null,{"children":"3x to 7x"}]," faster."]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":[["$","$L3",null,{"href":"https://qianwen-res.oss-cn-beijing.aliyuncs.com/zen-1M/zen_1M_Technical_Report.pdf","children":"Technical Report"}],":"]}]," We’re also sharing the technical details behind the zen-1M series, including design insights for training and inference frameworks, as well as ablation experiments."]}],"\n"]}],"\n"]}],"\n","$L4","\n","$L5","\n","$L6","\n","$L7","\n","$L8","\n","$L9","\n","$La","\n","$Lb","\n","$Lc","\n","$Ld","\n","$Le","\n","$Lf","\n","$L10","\n","$L11","\n","$L12","\n","$L13","\n","$L14","\n","$L15","\n","$L16","\n","$L17","\n","$L18","\n","$L19","\n","$L1a","\n","$L1b","\n","$L1c","\n","$L1d","\n","$L1e","\n","$L1f","\n","$L20","\n","$L21","\n","$L22","\n","$L23","\n","$L24","\n","$L25","\n","$L26","\n","$L27","\n","$L28","\n","$L29","\n","$L2a","\n","$L2b","\n","$L2c","\n","$L2d","\n","$L2e","\n","$L2f","\n","$L30","\n","$L31","\n","$L32","\n","$L33","\n","$L34","\n","$L35","\n","$L36","\n","$L37","\n","$L38","\n","$L39","\n","$L3a","\n","$L3b"]}]]}],["$L3c","$L3d","$L3e","$L3f"],"$L40"]}],"loading":null,"isPartial":false}
4:["$","p",null,{"children":["You can experience zen-1M models online by visiting our demo on ",["$","$L3",null,{"href":"https://huggingface.co/spaces/Qwen/zen-1M-Demo","children":"Huggingface"}]," and ",["$","$L3",null,{"href":"https://www.modelscope.cn/studios/Qwen/zen-1M-Demo","children":"Modelscope"}],"."]}]
5:["$","p",null,{"children":["Additionally, we recently introduced ",["$","strong",null,{"children":["$","$L3",null,{"href":"https://chat.qwenlm.ai/","children":"Qwen Chat"}]}]," , an advanced AI assistant from the Qwen series. With Qwen Chat, you can engage in conversations, write code, perform searches, generate images and videos, and utilize various tools. Notably, Qwen Chat also features the zen-Turbo model, which supports long-context processing with a context length of up to 1M tokens."]}]
6:["$","h1",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"model-performance","children":[["$","a",null,{"data-card":"","href":"#model-performance","className":"peer","children":"Model Performance"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
7:["$","p",null,{"children":"Let’s start by diving into the performance of the zen-1M series models, covering both long-context and short text tasks."}]
8:["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"long-context-tasks","children":[["$","a",null,{"data-card":"","href":"#long-context-tasks","className":"peer","children":"Long-Context Tasks"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
9:["$","p",null,{"children":"First off, we evaluate the zen-1M models on the Passkey Retrieval task with a context length of 1 million tokens. The results show that these models can accurately retrieve hidden information from documents containing up to 1M tokens, with only minor errors observed in the 7B model."}]
a:["$","p",null,{"children":["For more complex long-context understanding tasks, we select ",["$","$L3",null,{"href":"https://github.com/hsiehjackson/RULER","children":"RULER"}],", ",["$","$L3",null,{"href":"https://github.com/infinigence/LVEval","children":"LV-Eval"}],", ",["$","$L3",null,{"href":"https://github.com/THUDM/LongAlign","children":"LongbenchChat"}]," used in ",["$","$L3",null,{"href":"../qwen3-turbo/#more-complex-long-text-tasks","children":"this blog"}],"."]}]
b:["$","p",null,{"children":"From these results, we can draw a few key conclusions:"}]
c:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"Significantly Superior to the 128k Version:"}]," The zen-1M series models significantly outperform their 128K counterparts in most long-context tasks, especially for sequences exceeding 64K in length."]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"Notable Performance Advantage:"}]," The zen-14B-Instruct-1M model not only beats zen-Turbo but also consistently outperforms GPT-4o-mini across multiple datasets, offering a robust open-source alternative for long-context tasks."]}],"\n"]}]
d:["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"short-context-tasks","children":[["$","a",null,{"data-card":"","href":"#short-context-tasks","className":"peer","children":"Short-Context Tasks"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
e:["$","p",null,{"children":"Besides performance on long sequences, we’re equally interested in how these models handle short sequences. So, we compare the zen-1M models and their 128K versions on widely used academic benchmarks, throwing in GPT-4o-mini for comparison."}]
f:["$","p",null,{"children":"Here’s what we find:"}]
10:["$","ul",null,{"children":["\n",["$","li",null,{"children":"Both zen-7B-Instruct-1M and zen-14B-Instruct-1M maintain performance on short text tasks that is similar to their 128K versions, ensuring the fundamental capabilities haven’t been compromised by the addition of long-sequence processing abilities."}],"\n",["$","li",null,{"children":"Compared to GPT-4o-mini, both zen-14B-Instruct-1M and zen-Turbo achieve similar performance on short text tasks while supporting a context length that’s eight times longer."}],"\n"]}]
11:["$","h1",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"key-techniques","children":[["$","a",null,{"data-card":"","href":"#key-techniques","className":"peer","children":"Key Techniques"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
12:["$","p",null,{"children":["Here, we’ll briefly introduce the key techniques behind building zen-1M. For more details, please check out our ",["$","$L3",null,{"href":"https://qianwen-res.oss-cn-beijing.aliyuncs.com/zen-1M/zen_1M_Technical_Report.pdf","children":"technical report"}],"."]}]
13:["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"long-context-training","children":[["$","a",null,{"data-card":"","href":"#long-context-training","className":"peer","children":"Long-Context Training"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
14:["$","p",null,{"children":"Training with long sequences demands substantial computational resources, so we adopt a progressive approach to expand the context length for zen-1M through multiple stages:"}]
15:["$","ul",null,{"children":["\n",["$","li",null,{"children":"We begin with an intermediate checkpoint of pre-trained zen, which had a 4K token context length."}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"In Pretraining"}]," , we gradually increase the context length from 4K to 256K tokens while using ",["$","$L3",null,{"href":"https://arxiv.org/abs/2309.16039","children":"Adjusted Base Frequency"}],", raising the RoPE base from 10,000 to 10,000,000."]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"In Supervised Fine-tuning"}]," , we split this into two stages to preserve performance on shorter sequences:\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"Stage 1:"}]," Fine-tuned only on short instructions (up to 32K tokens) using the same data and steps as the 128K versions of zen."]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"Stage 2:"}]," Mixed short (up to 32K) and long (up to 256K) instructions to enhance long-context task performance while maintaining short-task quality."]}],"\n"]}],"\n"]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"In Reinforcement Learning"}]," , we train models on short texts up to 8K tokens, which sufficiently improves alignment with human preferences and generalizes well to long-context tasks."]}],"\n"]}]
16:["$","p",null,{"children":"The final instruction-tuned models are capable of handling sequences up to 256K tokens."}]
17:["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"length-extrapolation","children":[["$","a",null,{"data-card":"","href":"#length-extrapolation","className":"peer","children":"Length Extrapolation"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
18:["$","p",null,{"children":"During training, we develop an instruction-tuned model with a context length of 256K tokens. To extend this to 1M tokens, we employ length extrapolation techniques."}]
19:["$","p",null,{"children":["The degradation of LLMs based on RoPE in long-context tasks is mainly due to unseen, large relative positional distances between queries and keys in computing attention weight. We employ ",["$","$L3",null,{"href":"https://arxiv.org/abs/2402.17463","children":["$","strong",null,{"children":"Dual Chunk Attention"}]}]," (DCA), which addresses this issue by remapping relative positions to smaller values, avoiding the large distances not seen during training."]}]
1a:["$","p",null,{"children":"We evaluat the zen-1M models and their 128K counterparts with and without the length extrapolation method. We can find:"}]
1b:["$","p",null,{"children":"Even models trained on just 32K tokens, such as the zen-7B-Instruct, achieve nearly perfect accuracy in passkey retrieval tasks with 1M-token contexts. This underscores the remarkable ability of DCA to extend supported context lengths, without any training required."}]
1c:["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"sparse-attention","children":[["$","a",null,{"data-card":"","href":"#sparse-attention","className":"peer","children":"Sparse Attention"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
1d:["$","p",null,{"children":["For long-context language models, inference speed is crucial for user experience. We introduce a sparse attention mechanism based on ",["$","$L3",null,{"href":"https://arxiv.org/abs/2407.02490","children":["$","strong",null,{"children":"MInference"}]}]," to accelerate the prefill phase. Furthermore, we propose several improvements:"]}]
1e:["$","ul",null,{"children":["\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Integrating with Chunked Prefill:"}]," Directly processing sequences of 1M tokens results in substantial memory overhead to store the activations in MLP layers, consuming 71GB of VRAM in zen-7B. By integrating with chunk prefill with a chunk length of 32,768 tokens, activation VRAM usage is reduced by 96.7%, leading to a significant decrease in memory consumption."]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Integrating with Length Extrapolation:"}]," We integrate DCA with MInference in long-context processing, thereby enhancing inference efficiency and achieving greater accuracy."]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"Sparsity Refinement on Long Sequences:"}]," MInference requires an offline search to determine the optimal sparsification configuration for each attention head. Due to the computational demand of full attention weights, this search is typically conducted on short sequences, which may not generalize well to longer sequences. We developed a method to refine the sparsification configuration specifically for sequences up to 1M tokens, which significantly reduces the accuracy loss brought by sparse attention."]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":[["$","strong",null,{"children":"More Optimizations:"}]," We introduce additional optimizations, such as enhanced kernel efficiency and dynamic chunked pipeline parallelism, to fully unlock the potential of the entire framework."]}],"\n"]}],"\n"]}]
1f:["$","p",null,{"children":"With these enhancements, our inference framework results in a 3.2x to 6.7x acceleration in the prefill speed across different model sizes and GPU devices for sequences of 1M token length."}]
20:["$","h1",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"deploy-zen-1m-models-locally","children":[["$","a",null,{"data-card":"","href":"#deploy-zen-1m-models-locally","className":"peer","children":"Deploy zen-1M Models Locally"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
21:["$","p",null,{"children":"Here we provide step-by-step instructions for deploying the zen-1M models on your local devices."}]
22:["$","h3",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"1-system-preparation","children":[["$","a",null,{"data-card":"","href":"#1-system-preparation","className":"peer","children":"1. System Preparation"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
23:["$","p",null,{"children":"To achieve the best performance, we recommend using GPUs with Ampere or Hopper architecture, which support optimized kernels."}]
24:["$","p",null,{"children":"Ensure your system meets the following requirements:"}]
25:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"CUDA Version"}]," : 12.1 or 12.3"]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"Python Version"}]," : >=3.9 and <=3.12"]}],"\n"]}]
26:["$","p",null,{"children":"VRAM Requirement for processing 1 million-token sequences:"}]
27:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","strong",null,{"children":"zen-7B-Instruct-1M"}]," : At least 120GB VRAM (total across GPUs)."]}],"\n",["$","li",null,{"children":[["$","strong",null,{"children":"zen-14B-Instruct-1M"}]," : At least 320GB VRAM (total across GPUs)."]}],"\n"]}]
28:["$","p",null,{"children":"If your GPUs do not have sufficient VRAM, you can still use zen-1M models for shorter tasks."}]
29:["$","h3",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"2-install-dependencies","children":[["$","a",null,{"data-card":"","href":"#2-install-dependencies","className":"peer","children":"2. Install Dependencies"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
2a:["$","p",null,{"children":"For now, you need to clone the vLLM repository from our custom branch and install it manually. We are working on getting our branch merged into the main vLLM project."}]
2b:["$","$L41",null,{"className":"shiki shiki-themes github-light github-dark","style":{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},"tabIndex":"0","icon":"<svg viewBox=\"0 0 24 24\"><path d=\"m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z\" fill=\"currentColor\" /></svg>","children":["$","$L42",null,{"children":["$","code",null,{"children":[["$","span",null,{"className":"line"}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},"children":"    git"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":" clone"}],["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":" -b"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":" dev/dual-chunk-attn"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":" git@github.com:QwenLM/vllm.git"}]]}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":"    cd"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":" vllm"}]]}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},"children":"    pip"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":" install"}],["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":" -e"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":" ."}],["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":" -v"}]]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"    "}]}]]}]}]}]
2c:["$","h3",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"3-launch-openai-compatible-api-service","children":[["$","a",null,{"data-card":"","href":"#3-launch-openai-compatible-api-service","className":"peer","children":"3. Launch OpenAI-Compatible API Service"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
2d:["$","p",null,{"children":"Use the following command to start the service, configuring it based on your hardware setup:"}]
2e:["$","$L41",null,{"className":"shiki shiki-themes github-light github-dark","style":{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},"tabIndex":"0","icon":"<svg viewBox=\"0 0 24 24\"><path d=\"M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z\" fill=\"currentColor\" /></svg>","children":["$","$L42",null,{"children":["$","code",null,{"children":[["$","span",null,{"className":"line","children":["$","span",null,{}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"    vllm serve Qwen/zen-7B-Instruct-1M \\"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"      --tensor-parallel-size 4 \\"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"      --max-model-len 1010000 \\"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"      --enable-chunked-prefill --max-num-batched-tokens 131072 \\"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"      --enforce-eager \\"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"      --max-num-seqs 1"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"    "}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"    # --quantization fp8 # Enabling FP8 quantization for model weights can reduce memory usage."}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"    "}]}]]}]}]}]
2f:["$","p",null,{"children":["If you encounter any issues, please refer to the ",["$","$L3",null,{"href":"https://huggingface.co/Qwen/zen-7B-Instruct-1M#troubleshooting","children":"Troubleshooting"}]," section for more information."]}]
30:["$","p",null,{"children":["$","strong",null,{"children":"Parameter Explanations:"}]}]
31:["$","ul",null,{"children":["\n",["$","li",null,{"children":["\n",["$","p",null,{"children":["$","strong",null,{"children":["$","code",null,{"children":"--tensor-parallel-size"}]}]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"Set to the number of GPUs you are using. Max 4 GPUs for the 7B model, and 8 GPUs for the 14B model."}],"\n"]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":["$","strong",null,{"children":["$","code",null,{"children":"--max-model-len"}]}]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"Defines the maximum input sequence length. Reduce this value if you encounter Out of Memory issues."}],"\n"]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":["$","strong",null,{"children":["$","code",null,{"children":"--max-num-batched-tokens"}]}]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"Sets the chunk size in Chunked Prefill. A smaller value reduces activation memory usage but may slow down inference."}],"\n",["$","li",null,{"children":"Recommend 131072 for optimal performance."}],"\n"]}],"\n"]}],"\n",["$","li",null,{"children":["\n",["$","p",null,{"children":["$","strong",null,{"children":["$","code",null,{"children":"--max-num-seqs"}]}]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"Limits concurrent sequences processed."}],"\n"]}],"\n"]}],"\n"]}]
32:["$","h3",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"4-interact-with-the-model","children":[["$","a",null,{"data-card":"","href":"#4-interact-with-the-model","className":"peer","children":"4. Interact with the Model"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
33:["$","p",null,{"children":"You can interact with the deployed model using one of the following methods:"}]
34:["$","p",null,{"children":["$","strong",null,{"children":"Option 1. Using Curl"}]}]
35:["$","$L41",null,{"className":"shiki shiki-themes github-light github-dark","style":{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},"tabIndex":"0","icon":"<svg viewBox=\"0 0 24 24\"><path d=\"m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z\" fill=\"currentColor\" /></svg>","children":["$","$L42",null,{"children":["$","code",null,{"children":[["$","span",null,{"className":"line"}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},"children":"     curl"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":" http://localhost:8000/v1/chat/completions"}],["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":" \\"}]]}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":"      -H"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":" \"Content-Type: application/json\""}],["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":" \\"}]]}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":"      -d"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":" '{"}]]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"        \"model\": \"Qwen/zen-7B-Instruct-1M\","}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"        \"messages\": ["}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"          {\"role\": \"user\", \"content\": \"Tell me something about large language models.\"}"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"        ],"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"        \"temperature\": 0.7,"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"        \"top_p\": 0.8,"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"        \"repetition_penalty\": 1.05,"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"        \"max_tokens\": 512"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"      }'"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"    "}]}]]}]}]}]
36:["$","p",null,{"children":["$","strong",null,{"children":"Option 2. Using Python"}]}]
43:T5c0,<svg viewBox="0 0 24 24"><path d="M14.25.18l.9.2.73.26.59.3.45.32.34.34.25.34.16.33.1.3.04.26.02.2-.01.13V8.5l-.05.63-.13.55-.21.46-.26.38-.3.31-.33.25-.35.19-.35.14-.33.1-.3.07-.26.04-.21.02H8.77l-.69.05-.59.14-.5.22-.41.27-.33.32-.27.35-.2.36-.15.37-.1.35-.07.32-.04.27-.02.21v3.06H3.17l-.21-.03-.28-.07-.32-.12-.35-.18-.36-.26-.36-.36-.35-.46-.32-.59-.28-.73-.21-.88-.14-1.05-.05-1.23.06-1.22.16-1.04.24-.87.32-.71.36-.57.4-.44.42-.33.42-.24.4-.16.36-.1.32-.05.24-.01h.16l.06.01h8.16v-.83H6.18l-.01-2.75-.02-.37.05-.34.11-.31.17-.28.25-.26.31-.23.38-.2.44-.18.51-.15.58-.12.64-.1.71-.06.77-.04.84-.02 1.27.05zm-6.3 1.98l-.23.33-.08.41.08.41.23.34.33.22.41.09.41-.09.33-.22.23-.34.08-.41-.08-.41-.23-.33-.33-.22-.41-.09-.41.09zm13.09 3.95l.28.06.32.12.35.18.36.27.36.35.35.47.32.59.28.73.21.88.14 1.04.05 1.23-.06 1.23-.16 1.04-.24.86-.32.71-.36.57-.4.45-.42.33-.42.24-.4.16-.36.09-.32.05-.24.02-.16-.01h-8.22v.82h5.84l.01 2.76.02.36-.05.34-.11.31-.17.29-.25.25-.31.24-.38.2-.44.17-.51.15-.58.13-.64.09-.71.07-.77.04-.84.01-1.27-.04-1.07-.14-.9-.2-.73-.25-.59-.3-.45-.33-.34-.34-.25-.34-.16-.33-.1-.3-.04-.25-.02-.2.01-.13v-5.34l.05-.64.13-.54.21-.46.26-.38.3-.32.33-.24.35-.2.35-.14.33-.1.3-.06.26-.04.21-.02.13-.01h5.84l.69-.05.59-.14.5-.21.41-.28.33-.32.27-.35.2-.36.15-.36.1-.35.07-.32.04-.28.02-.21V6.07h2.09l.14.01zm-6.47 14.25l-.23.33-.08.41.08.41.23.33.33.23.41.08.41-.08.33-.23.23-.33.08-.41-.08-.41-.23-.33-.33-.23-.41-.08-.41.08z" fill="currentColor" /></svg>37:["$","$L41",null,{"className":"shiki shiki-themes github-light github-dark","style":{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},"tabIndex":"0","icon":"$43","children":["$","$L42",null,{"children":["$","code",null,{"children":[["$","span",null,{"className":"line"}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"     from"}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":" openai "}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"import"}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":" OpenAI"}]]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"    "}]}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"    openai_api_key "}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"="}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":" \"EMPTY\""}]]}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"    openai_api_base "}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"="}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":" \"http://localhost:8000/v1\""}]]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"    "}]}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"    client "}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"="}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":" OpenAI("}]]}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#E36209","--shiki-dark":"#FFAB70"},"children":"        api_key"}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"="}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"openai_api_key,"}]]}],"\n",["$","span",null,{"className":"line","children":["$L44","$L45","$L46"]}],"\n","$L47","\n","$L48","\n","$L49","\n","$L4a","\n","$L4b","\n","$L4c","\n","$L4d","\n","$L4e","\n","$L4f","\n","$L50","\n","$L51","\n","$L52","\n","$L53","\n","$L54","\n","$L55","\n","$L56","\n","$L57","\n","$L58"]}]}]}]
38:["$","p",null,{"children":["$","strong",null,{"children":"Other Options"}]}]
39:["$","p",null,{"children":["For more advanced use cases, consider exploring frameworks like ",["$","$L3",null,{"href":"https://github.com/QwenLM/Qwen-Agent/tree/main","children":"Qwen-Agent"}],", which enable the model to read PDF files and perform other specialized tasks."]}]
3a:["$","h1",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"whats-next","children":[["$","a",null,{"data-card":"","href":"#whats-next","className":"peer","children":"What’s Next?"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
3b:["$","p",null,{"children":"We recognize that long-context models still have a lot of room for improvement. Our goal is to build models that excel in both short and long-context tasks, making sure they bring real value to practical, long-context scenarios. We’re diving deep into more efficient training methods, model architectures, and inference methods to make them deployable effectively and perform exceptionally well even in environments with limited resources. We’re confident that all these efforts will open up a whole new world of possibilities for long-context models, expanding their use across a much broader range of applications. Stay tuned as we keep pushing the boundaries!"}]
3c:["$","script","script-0",{"src":"/_next/static/chunks/8de849ca74fc071f.js","async":true}]
3d:["$","script","script-1",{"src":"/_next/static/chunks/e62b91212ee7f8ff.js","async":true}]
3e:["$","script","script-2",{"src":"/_next/static/chunks/f19fe44237e54646.js","async":true}]
3f:["$","script","script-3",{"src":"/_next/static/chunks/cb0a883bafeb6805.js","async":true}]
40:["$","$L59",null,{"children":["$","$5a",null,{"name":"Next.MetadataOutlet","children":"$@5b"}]}]
44:["$","span",null,{"style":{"--shiki-light":"#E36209","--shiki-dark":"#FFAB70"},"children":"        base_url"}]
45:["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"="}]
46:["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"openai_api_base,"}]
47:["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"    )"}]}]
48:["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"    "}]}]
49:["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"    prompt "}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"="}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":" ("}]]}]
4a:["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"        \"There is an important info hidden inside a lot of irrelevant text. \""}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":" +"}]]}]
4b:["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"        \"Find it and memorize it. I will quiz you about the important information there."}],["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":"\\n\\n"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\""}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":" +"}]]}]
4c:["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"        \"The pass key is 28884. Remember it. 28884 is the pass key."}],["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":"\\n"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\""}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":" +"}]]}]
4d:["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"        \"The grass is green. The sky is blue. The sun is yellow. Here we go. There and back again. \""}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":" *"}],["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":" 800"}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":" +"}]]}]
4e:["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"        \""}],["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":"\\n"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"What is the pass key?\""}]]}]
4f:["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#6A737D","--shiki-dark":"#6A737D"},"children":"        # The prompt is approximately 20k tokens long. You can try longer prompts by increasing the multiplier."}]}]
50:["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"    )"}]}]
51:["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"    "}]}]
52:["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"    chat_response "}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"="}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":" client.chat.completions.create("}]]}]
53:["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#E36209","--shiki-dark":"#FFAB70"},"children":"        model"}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"="}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\"Qwen/zen-7B-Instruct-1M\""}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":","}]]}]
54:["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#E36209","--shiki-dark":"#FFAB70"},"children":"        messages"}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"="}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"[{"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\"role\""}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":": "}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\"user\""}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":", "}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\"content\""}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":": prompt}],"}]]}]
55:["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#E36209","--shiki-dark":"#FFAB70"},"children":"        temperature"}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"="}],["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":"0"}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":","}]]}]
56:["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"    )"}]}]
57:["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":"    print"}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"("}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\"Chat response:\""}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":", chat_response.choices["}],["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":"0"}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"].message.content)"}]]}]
58:["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"    "}]}]
5b:null
