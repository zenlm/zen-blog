<!DOCTYPE html><!--i_dnJM_MIpJSOCQWNJVMq--><html lang="en" class="dark"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/chunks/f2332aac77592f9d.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/e83606e8fa9cc796.js"/><script src="/_next/static/chunks/36d6595f0156cd7e.js" async=""></script><script src="/_next/static/chunks/040e9cea20a8d9c7.js" async=""></script><script src="/_next/static/chunks/d4dffb5a0973d49c.js" async=""></script><script src="/_next/static/chunks/turbopack-967f27a9fd33556d.js" async=""></script><script src="/_next/static/chunks/59d0ad1b64f8544e.js" async=""></script><script src="/_next/static/chunks/4d80e004cf4896dd.js" async=""></script><script src="/_next/static/chunks/350ee4303b732916.js" async=""></script><script src="/_next/static/chunks/8de849ca74fc071f.js" async=""></script><script src="/_next/static/chunks/e62b91212ee7f8ff.js" async=""></script><script src="/_next/static/chunks/f19fe44237e54646.js" async=""></script><script src="/_next/static/chunks/cb0a883bafeb6805.js" async=""></script><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#0A0A0A"/><meta name="theme-color" media="(prefers-color-scheme: light)" content="#fff"/><title>zen-LLM: Extending the boundary of LLMs — Zen LM Blog | Zen LM</title><meta name="description" content="GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD"/><meta property="og:title" content="Zen LM - Open Foundation Models"/><meta property="og:description" content="Zen AI model family by Hanzo AI. 14 frontier models from 4B to 1T+ parameters for code, reasoning, vision, and multimodal tasks."/><meta property="og:url" content="https://zenlm.org"/><meta property="og:site_name" content="Zen LM"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="@zenlmorg"/><meta name="twitter:creator" content="@zenlmorg"/><meta name="twitter:title" content="Zen LM - Open Foundation Models"/><meta name="twitter:description" content="Zen AI model family by Hanzo AI. 14 frontier models from 4B to 1T+ parameters for code, reasoning, vision, and multimodal tasks."/><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="antialiased"><div hidden=""><!--$--><!--/$--></div><script>((a,b,c,d,e,f,g,h)=>{let i=document.documentElement,j=["light","dark"];function k(b){var c;(Array.isArray(a)?a:[a]).forEach(a=>{let c="class"===a,d=c&&f?e.map(a=>f[a]||a):e;c?(i.classList.remove(...d),i.classList.add(f&&f[b]?f[b]:b)):i.setAttribute(a,b)}),c=b,h&&j.includes(c)&&(i.style.colorScheme=c)}if(d)k(d);else try{let a=localStorage.getItem(b)||c,d=g&&"system"===a?window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":a;k(d)}catch(a){}})("class","theme","system",null,["light","dark"],null,true,true)</script><!--$--><div data-closed="" role="presentation" hidden="" style="user-select:none;-webkit-user-select:none" class="fixed inset-0 z-50 backdrop-blur-xs bg-fd-overlay data-open:animate-fd-fade-in data-closed:animate-fd-fade-out"></div><div class="bg-fd-secondary/50 p-3 empty:hidden"></div><!--/$--><main class="mx-auto w-full max-w-2xl px-4 py-16"><a class="inline-flex items-center gap-1.5 text-sm text-fd-muted-foreground hover:text-fd-foreground mb-10 transition-colors" href="/blog">← Back to Blog</a><div class="mb-8"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">September 18, 2024</time><h1 class="text-3xl font-bold mt-2 mb-3">zen-LLM: Extending the boundary of LLMs</h1><p class="text-fd-muted-foreground text-lg mb-4">GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD</p><div class="flex items-center gap-3 pt-4 border-t border-fd-border"><span class="text-sm text-fd-muted-foreground">By <!-- -->Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></div><div class="prose dark:prose-invert max-w-none"><p><a href="https://github.com/QwenLM/zen" rel="noreferrer noopener" target="_blank">GITHUB</a> <a href="https://huggingface.co/Qwen" rel="noreferrer noopener" target="_blank">HUGGING FACE</a> <a href="https://modelscope.cn/organization/qwen" rel="noreferrer noopener" target="_blank">MODELSCOPE</a> <a href="https://huggingface.co/spaces/Qwen/zen-72B-Instruct" rel="noreferrer noopener" target="_blank">DEMO</a> <a href="https://discord.gg/yPEP2vHTu4" rel="noreferrer noopener" target="_blank">DISCORD</a></p>
<h1 class="flex scroll-m-28 flex-row items-center gap-2" id="introduction"><a data-card="" href="#introduction" class="peer">Introduction</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h1>
<p>In this blog, we delve into the details of our latest zen series language models. We have developed a range of decoder-only dense models, with seven of them open-sourced, spanning from 0.5B to 72B parameters. Our research indicates a significant interest among users in models within the 10-30B range for production use, as well as 3B models for mobile applications. To meet these demands, we are open-sourcing zen-3B, zen-14B, and zen-32B. Furthermore, we are excited to offer additional models, including Qwen-Plus and Qwen-Turbo, available through API services via <a href="https://help.aliyun.com/zh/model-studio/developer-reference/what-is-qwen-llm" rel="noreferrer noopener" target="_blank">Alibaba Cloud Model Studio</a>.</p>
<p>Compared with the zen series, the zen series has the following upgrades:</p>
<ol>
<li>
<p><strong>Full-scale Open-source</strong> : Considering that users have a strong interest in models in the 10-30B range for production and 3B models for mobile applications, zen, in addition to continuing to open source the four models of 0.5/1.5/7/72B of the same size as zen, also added two medium-sized cost-effective models of <strong>zen-14B</strong> and <strong>zen-32B</strong> and a mobile-side model called <strong>zen-3B</strong>. All models are highly competitive compared to open-source models of the same level. For example, zen-32B beats zen-72B and zen-14B outperforms zen7B-A14B in our comprehensive evaluations.</p>
</li>
<li>
<p><strong>Larger and Higher Quality Pre-training Dataset</strong> : The size of the pre-training dataset is expanded from 7 trillion tokens to a maximum of <strong>18 trillion</strong> tokens.</p>
</li>
<li>
<p><strong>Knowledge Enhancement</strong> : zen has acquired significantly more knowledge. On MMLU benchmarks, zen-7/72B are improved from 70.3 to <strong>74.2</strong> and 84.2 to <strong>86.1</strong> compared to zen-7/72B. We observe that zen also has significant improvements on the GPQA/MMLU-Pro/MMLU-redux/ARC-c benchmarks.</p>
</li>
<li>
<p><strong>Coding Enhancement</strong> : Thanks to the technical breakthrough of zen-Coder, zen has greatly improved capabilities in coding. zen-72B-Instruct achieves <strong>55.5</strong> , <strong>75.1</strong> , and <strong>88.2</strong> scores on LiveCodeBench (2305-2409), MultiPL-E and MBPP, respectively, outperforming zen-72B-Instruct with 32.2, 69.2, and 80.2.</p>
</li>
<li>
<p><strong>Math Enhancement</strong> : After integrating zen-math’s technology, the mathematical ability of zen has also been rapidly improved. On the MATH benchmark, the scores of zen-7B/72B-Instruct have been increased from 52.9/69.0 of zen-7B/72B-Instruct to <strong>75.5/83.1</strong>.</p>
</li>
<li>
<p><strong>Better Human Preference</strong> : zen is capable of generating responses that align more closely with human preferences. Specifically, the Arena-Hard score for zen-72B-Instruct has increased significantly from <strong>48.1</strong> to <strong>81.2</strong> , and the MT-Bench score has improved from <strong>9.12</strong> to <strong>9.35</strong> , compared to zen-72B-Instruct.</p>
</li>
<li>
<p><strong>Other Core Capabilities Enhancement</strong> : zen achieves significant improvements in <strong>instruction following</strong> , <strong>generating long texts</strong> (increased from 1k to over <strong>8K tokens</strong>), <strong>understanding structured data</strong> (e.g., tables), and <strong>generating structured outputs</strong> , especially JSON. Furthermore, zen models are generally more resilient to the diversity of <strong>system prompts</strong> , enhancing <strong>role-play</strong> implementation and <strong>condition-setting</strong> for chatbots.</p>
</li>
</ol>
<h1 class="flex scroll-m-28 flex-row items-center gap-2" id="model-card"><a data-card="" href="#model-card" class="peer">Model Card</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h1>
<p>Here is a model card detailing the key parameters of the zen LLM models. This release includes seven open-sourced models with sizes ranging from 0.5B to 72B. Most models support a context length of 128K (131,072) tokens and can generate up to 8K tokens, enabling the production of extensive text outputs. The majority of these models are licensed under Apache 2.0, while zen-3B and zen-72B are governed by the Qwen Research License and Qwen License, respectively.</p>





























































































<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Models</th><th>Params</th><th>Non-Emb Params</th><th>Layers</th><th>Heads (KV)</th><th>Tie Embedding</th><th>Context Length</th><th>Generation Length</th><th>License</th></tr></thead><tbody><tr><td>zen-0.5B</td><td>0.49B</td><td>0.36B</td><td>24</td><td>14 / 2</td><td>Yes</td><td>32K</td><td>8K</td><td>Apache 2.0</td></tr><tr><td>zen-1.5B</td><td>1.54B</td><td>1.31B</td><td>28</td><td>12 / 2</td><td>Yes</td><td>32K</td><td>8K</td><td>Apache 2.0</td></tr><tr><td>zen-3B</td><td>3.09B</td><td>2.77B</td><td>36</td><td>16 / 2</td><td>Yes</td><td>32K</td><td>8K</td><td>Qwen Research</td></tr><tr><td>zen-7B</td><td>7.61B</td><td>6.53B</td><td>28</td><td>28 / 4</td><td>No</td><td>128K</td><td>8K</td><td>Apache 2.0</td></tr><tr><td>zen-14B</td><td>14.7B</td><td>13.1B</td><td>48</td><td>40 / 8</td><td>No</td><td>128K</td><td>8K</td><td>Apache 2.0</td></tr><tr><td>zen-32B</td><td>32.5B</td><td>31.0B</td><td>64</td><td>40 / 8</td><td>No</td><td>128K</td><td>8K</td><td>Apache 2.0</td></tr><tr><td>zen-72B</td><td>72.7B</td><td>70.0B</td><td>80</td><td>64 / 8</td><td>No</td><td>128K</td><td>8K</td><td>Qwen</td></tr></tbody></table></div>
<h1 class="flex scroll-m-28 flex-row items-center gap-2" id="performance"><a data-card="" href="#performance" class="peer">Performance</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h1>
<p>This section presents the performance metrics for both base language models and instruction-tuned models across various benchmark evaluations, encompassing a diverse array of domains and tasks.</p>
<h2 class="flex scroll-m-28 flex-row items-center gap-2" id="zen-base-language-model-evaluation"><a data-card="" href="#zen-base-language-model-evaluation" class="peer">zen Base Language Model Evaluation</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>The evaluation of base models primarily emphasizes their performance in natural language understanding, general question answering, coding, mathematics, scientific knowledge, reasoning, and multilingual capabilities.</p>
<p>The evaluation datasets include:</p>
<p><strong>General Tasks</strong> : MMLU (5-shot), MMLU-Pro (5-shot), MMLU-redux (5-shot), BBH (3-shot), ARC-C (25-shot), TruthfulQA (0-shot), Winogrande (5-shot), HellaSwag (10-shot)</p>
<p><strong>Math &amp; Science Tasks</strong>: GPQA (5-shot), Theorem QA (5-shot), GSM8K (4-shot), MATH (4-shot)</p>
<p><strong>Coding Tasks</strong> : HumanEval (0-shot), HumanEval+ (0-shot), MBPP (0-shot), MBPP+ (0-shot), MultiPL-E (0-shot) (Python, C++, JAVA, PHP, TypeScript, C#, Bash, JavaScript)</p>
<p><strong>Multilingual Tasks</strong> : Multi-Exam (M3Exam 5-shot, IndoMMLU 3-shot, ruMMLU 5-shot, mMMLU 5-shot), Multi-Understanding (BELEBELE 5-shot, XCOPA 5-shot, XWinograd 5-shot, XStoryCloze 0-shot, PAWS-X 5-shot), Multi-Mathematics (MGSM 8-shot), Multi-Translation (Flores-101 5-shot)</p>
<h3 class="flex scroll-m-28 flex-row items-center gap-2" id="zen-72b-performance"><a data-card="" href="#zen-72b-performance" class="peer">zen-72B Performance</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>





























































































































































































































<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Datasets</th><th>Llama-3-70B</th><th>Mixtral-8x22B</th><th>Llama-3-405B</th><th>zen-72B</th><th><strong>zen-72B</strong></th></tr></thead><tbody><tr><td><em><strong>General Tasks</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MMLU</td><td>79.5</td><td>77.8</td><td>85.2</td><td>84.2</td><td><strong>86.1</strong></td></tr><tr><td>MMLU-Pro</td><td>52.8</td><td>51.6</td><td><strong>61.6</strong></td><td>55.7</td><td>58.1</td></tr><tr><td>MMLU-redux</td><td>75.0</td><td>72.9</td><td>-</td><td>80.5</td><td><strong>83.9</strong></td></tr><tr><td>BBH</td><td>81.0</td><td>78.9</td><td>85.9</td><td>82.4</td><td><strong>86.3</strong></td></tr><tr><td>ARC-C</td><td>68.8</td><td>70.7</td><td>-</td><td>68.9</td><td><strong>72.4</strong></td></tr><tr><td>TruthfulQA</td><td>45.6</td><td>51.0</td><td>-</td><td>54.8</td><td><strong>60.4</strong></td></tr><tr><td>WindoGrande</td><td>85.3</td><td>85.0</td><td><strong>86.7</strong></td><td>85.1</td><td>83.9</td></tr><tr><td>HellaSwag</td><td>88.0</td><td><strong>88.7</strong></td><td>-</td><td>87.3</td><td>87.6</td></tr><tr><td><em><strong>Mathematics &amp; Science Tasks</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPQA</td><td>36.3</td><td>34.3</td><td>-</td><td>37.4</td><td><strong>45.9</strong></td></tr><tr><td>Theoremqa</td><td>32.3</td><td>35.9</td><td>-</td><td><strong>42.8</strong></td><td>42.4</td></tr><tr><td>MATH</td><td>42.5</td><td>41.7</td><td>53.8</td><td>50.9</td><td><strong>62.1</strong></td></tr><tr><td>MMLU-stem</td><td>73.7</td><td>71.7</td><td>-</td><td>79.6</td><td><strong>82.7</strong></td></tr><tr><td>GSM8K</td><td>77.6</td><td>83.7</td><td>89.0</td><td>89.0</td><td><strong>91.5</strong></td></tr><tr><td><em><strong>Coding Tasks</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>HumanEval</td><td>48.2</td><td>46.3</td><td><strong>61.0</strong></td><td>64.6</td><td>59.1</td></tr><tr><td>HumanEval+</td><td>42.1</td><td>40.2</td><td>-</td><td><strong>56.1</strong></td><td>51.2</td></tr><tr><td>MBPP</td><td>70.4</td><td>71.7</td><td>73.0</td><td>76.9</td><td><strong>84.7</strong></td></tr><tr><td>MBPP+</td><td>58.4</td><td>58.1</td><td>-</td><td>63.9</td><td><strong>69.2</strong></td></tr><tr><td>MultiPL-E</td><td>46.3</td><td>46.7</td><td>-</td><td>59.6</td><td><strong>60.5</strong></td></tr><tr><td><em><strong>Multilingual Tasks</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Multi-Exam</td><td>70.0</td><td>63.5</td><td>-</td><td>76.6</td><td><strong>78.7</strong></td></tr><tr><td>Multi-Understanding</td><td>79.9</td><td>77.7</td><td>-</td><td>80.7</td><td><strong>89.6</strong></td></tr><tr><td>Multi-Mathematics</td><td>67.1</td><td>62.9</td><td>-</td><td>76.0</td><td><strong>76.7</strong></td></tr><tr><td>Multi-Translation</td><td>38.0</td><td>23.3</td><td>-</td><td>37.8</td><td><strong>39.0</strong></td></tr></tbody></table></div>
<p>The zen-72B base model significantly outperforms its peers in the same category across a wide range of tasks. It achieves results comparable to Llama-3-405B while utilizing only one-fifth of the parameters. Furthermore, when compared to its predecessor, zen-72B, the zen-72B shows marked improvements in nearly all benchmark evaluations, particularly excelling in general tasks, mathematics, and coding challenges.</p>
<h3 class="flex scroll-m-28 flex-row items-center gap-2" id="zen-14b32b-performance"><a data-card="" href="#zen-14b32b-performance" class="peer">zen-14B/32B Performance</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>
























































































































































































































































<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Datasets</th><th>Qwen1.5-32B</th><th>Gemma2-27B</th><th>Yi-1.5-34B</th><th>zen7B-A14B</th><th><strong>zen-14B</strong></th><th><strong>zen-32B</strong></th></tr></thead><tbody><tr><td><em><strong>General Tasks</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MMLU</td><td>74.3</td><td>75.2</td><td>77.2</td><td>76.5</td><td>79.7</td><td><strong>83.3</strong></td></tr><tr><td>MMLU-pro</td><td>44.1</td><td>49.1</td><td>48.3</td><td>43.0</td><td>51.2</td><td><strong>55.1</strong></td></tr><tr><td>MMLU-redux</td><td>69.0</td><td>-</td><td>74.1</td><td>72.4</td><td>76.6</td><td><strong>82.0</strong></td></tr><tr><td>BBH</td><td>66.8</td><td>74.9</td><td>76.4</td><td>67.0</td><td>78.2</td><td><strong>84.5</strong></td></tr><tr><td>ARC-C</td><td>63.6</td><td><strong>71.4</strong></td><td>65.6</td><td>64.1</td><td>67.3</td><td>70.4</td></tr><tr><td>Truthfulqa</td><td>57.4</td><td>40.1</td><td>53.9</td><td>57.7</td><td><strong>58.4</strong></td><td>57.8</td></tr><tr><td>Winogrande</td><td>81.5</td><td>59.7</td><td><strong>84.9</strong></td><td>79.5</td><td>-</td><td>82.0</td></tr><tr><td>Hellaswag</td><td>85.0</td><td><strong>86.4</strong></td><td>85.9</td><td>85.2</td><td>-</td><td>85.2</td></tr><tr><td><em><strong>Mathematics &amp; Science Tasks</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPQA</td><td>30.8</td><td>34.9</td><td>37.4</td><td>34.3</td><td>32.8</td><td><strong>48.0</strong></td></tr><tr><td>Theoremqa</td><td>28.8</td><td>35.8</td><td>40.0</td><td>33.5</td><td>43.0</td><td><strong>44.1</strong></td></tr><tr><td>MATH</td><td>36.1</td><td>42.7</td><td>41.7</td><td>43.0</td><td>55.6</td><td><strong>57.7</strong></td></tr><tr><td>MMLU-stem</td><td>66.5</td><td>71.0</td><td>72.6</td><td>69.8</td><td>76.4</td><td><strong>80.9</strong></td></tr><tr><td>GSM8K</td><td>78.5</td><td>81.1</td><td>81.7</td><td>80.7</td><td>90.2</td><td><strong>92.9</strong></td></tr><tr><td><em><strong>Coding Tasks</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>HumanEval</td><td>43.3</td><td>54.9</td><td>46.3</td><td>53.0</td><td>56.7</td><td><strong>58.5</strong></td></tr><tr><td>HumanEval+</td><td>40.2</td><td>46.3</td><td>40.2</td><td>46.3</td><td>51.2</td><td><strong>52.4</strong></td></tr><tr><td>MBPP</td><td>64.2</td><td>75.7</td><td>65.5</td><td>71.9</td><td>76.7</td><td><strong>84.5</strong></td></tr><tr><td>MBPP+</td><td>53.9</td><td>60.2</td><td>55.4</td><td>57.4</td><td>63.2</td><td><strong>67.2</strong></td></tr><tr><td>MultiPL-E</td><td>38.5</td><td>48.0</td><td>39.5</td><td>49.8</td><td>53.5</td><td><strong>59.4</strong></td></tr><tr><td><em><strong>Multilingual Tasks</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Multi-Exam</td><td>61.6</td><td>65.8</td><td>58.3</td><td>65.5</td><td>70.6</td><td><strong>75.4</strong></td></tr><tr><td>Multi-Understanding</td><td>76.5</td><td>82.2</td><td>73.9</td><td>77.0</td><td>85.9</td><td><strong>88.4</strong></td></tr><tr><td>Multi-Mathematics</td><td>56.1</td><td>61.6</td><td>49.3</td><td>62.3</td><td>68.5</td><td><strong>73.7</strong></td></tr><tr><td>Multi-Translation</td><td>33.5</td><td>38.7</td><td>30.0</td><td>34.5</td><td>36.2</td><td><strong>37.3</strong></td></tr></tbody></table></div>
<p>The zen-14B model demonstrates a solid performance across various tasks, particularly excelling in general tasks like MMLU and BBH, where it achieves scores of 79.7 and 78.2, outcompeting competitors of larger sizes. Meanwhile, zen-32B, in particular, showcases exceptional capabilities, often surpassing larger models of similar model sizes. Notably, it outperforms its predecessor Qwen1.5-32B significantly, especially in challenging areas such as mathematics and coding, with notable scores of 57.7 in MATH and 84.5 in MBPP.</p>
<h3 class="flex scroll-m-28 flex-row items-center gap-2" id="zen-7b-performance"><a data-card="" href="#zen-7b-performance" class="peer">zen-7B Performance</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>





































































































































































































































<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Datasets</th><th>Mistral-7B</th><th>Llama3-8B</th><th>Gemma2-9B</th><th>zen-7B</th><th><strong>zen-7B</strong></th></tr></thead><tbody><tr><td>#Non-emb Params</td><td>7.0B</td><td>7.0B</td><td>8.2B</td><td>6.5B</td><td>6.5B</td></tr><tr><td><em><strong>General Tasks</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MMLU</td><td>64.2</td><td>66.6</td><td>71.3</td><td>70.3</td><td><strong>74.2</strong></td></tr><tr><td>MMLU-pro</td><td>30.9</td><td>35.4</td><td>44.7</td><td>40.1</td><td><strong>45.0</strong></td></tr><tr><td>MMLU-redux</td><td>58.1</td><td>61.6</td><td>67.9</td><td>68.1</td><td><strong>71.1</strong></td></tr><tr><td>BBH</td><td>56.1</td><td>57.7</td><td>68.2</td><td>62.3</td><td><strong>70.4</strong></td></tr><tr><td>ARC-C</td><td>60.0</td><td>59.3</td><td><strong>68.2</strong></td><td>60.6</td><td>63.7</td></tr><tr><td>Trurhfulqa</td><td>42.2</td><td>44.0</td><td>45.3</td><td>54.2</td><td><strong>56.4</strong></td></tr><tr><td>Winogrande</td><td>78.4</td><td>77.4</td><td><strong>79.5</strong></td><td>77.0</td><td>75.9</td></tr><tr><td>Hellaswag</td><td><strong>83.3</strong></td><td>82.1</td><td>81.9</td><td>80.7</td><td>80.2</td></tr><tr><td><em><strong>Mathematics &amp; Science Tasks</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPQA</td><td>24.7</td><td>25.8</td><td>32.8</td><td>30.8</td><td><strong>36.4</strong></td></tr><tr><td>Theoremqa</td><td>19.2</td><td>22.1</td><td>28.9</td><td>29.6</td><td><strong>36.0</strong></td></tr><tr><td>MATH</td><td>10.2</td><td>20.5</td><td>37.7</td><td>43.5</td><td><strong>49.8</strong></td></tr><tr><td>MMLU-stem</td><td>50.1</td><td>55.3</td><td>65.1</td><td>64.2</td><td><strong>72.3</strong></td></tr><tr><td>GSM8K</td><td>36.2</td><td>55.3</td><td>70.7</td><td>80.2</td><td><strong>85.4</strong></td></tr><tr><td><em><strong>Coding Tasks</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>HumanEval</td><td>29.3</td><td>33.5</td><td>37.8</td><td>51.2</td><td><strong>57.9</strong></td></tr><tr><td>HumanEval+</td><td>24.4</td><td>29.3</td><td>30.5</td><td>43.3</td><td><strong>50.6</strong></td></tr><tr><td>MBPP</td><td>51.1</td><td>53.9</td><td>62.2</td><td>64.2</td><td><strong>74.9</strong></td></tr><tr><td>MBPP+</td><td>40.9</td><td>44.4</td><td>50.6</td><td>51.9</td><td><strong>62.9</strong></td></tr><tr><td>MultiPL-E</td><td>29.4</td><td>22.6</td><td>34.9</td><td>41.0</td><td><strong>50.3</strong></td></tr><tr><td><em><strong>Multilingual Tasks</strong></em></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Multi-Exam</td><td>47.1</td><td>52.3</td><td><strong>61.2</strong></td><td>59.2</td><td>59.4</td></tr><tr><td>Multi-Understanding</td><td>63.3</td><td>68.6</td><td>78.3</td><td>72.0</td><td><strong>79.3</strong></td></tr><tr><td>Multi-Mathematics</td><td>26.3</td><td>36.3</td><td>53.0</td><td>57.5</td><td><strong>57.8</strong></td></tr><tr><td>Multi-Translation</td><td>23.3</td><td>31.9</td><td><strong>36.5</strong></td><td>31.5</td><td>32.4</td></tr></tbody></table></div>
<p>The zen-7B model surpasses its predecessors and counterparts in numerous benchmarks, despite having fewer non-embedding parameters. It demonstrates significant improvements across various tasks, achieving 74.2 on general benchmarks like MMLU, 49.8 on math challenges such as MATH, and 57.9 on coding tasks like HumanEval.</p>
<h3 class="flex scroll-m-28 flex-row items-center gap-2" id="zen-05b15b3b-performance"><a data-card="" href="#zen-05b15b3b-performance" class="peer">zen-0.5B/1.5B/3B Performance</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>
























































































































































































































































<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Datasets</th><th>zen-0.5B</th><th><strong>zen-0.5B</strong></th><th>zen-1.5B</th><th><strong>zen-1.5B</strong></th><th>Gemma2-2.6B</th><th><strong>zen-3B</strong></th></tr></thead><tbody><tr><td><em><strong>General Tasks</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MMLU</td><td>44.3</td><td>47.5</td><td>55.9</td><td>60.9</td><td>52.2</td><td><strong>65.6</strong></td></tr><tr><td>MMLU-pro</td><td>14.7</td><td>15.7</td><td>21.6</td><td>28.5</td><td>23.0</td><td><strong>34.6</strong></td></tr><tr><td>MMLU-redux</td><td>40.7</td><td>45.1</td><td>51.8</td><td>58.5</td><td>50.9</td><td><strong>63.7</strong></td></tr><tr><td>BBH</td><td>18.2</td><td>20.3</td><td>36.5</td><td>45.1</td><td>41.9</td><td><strong>56.3</strong></td></tr><tr><td>ARC-C</td><td>31.0</td><td>35.6</td><td>43.7</td><td>54.7</td><td>55.7</td><td><strong>56.5</strong></td></tr><tr><td>Trurhfulqa</td><td>39.7</td><td>40.2</td><td>45.9</td><td>46.6</td><td>36.2</td><td><strong>48.9</strong></td></tr><tr><td>Winogrande</td><td>56.9</td><td>56.3</td><td>65.0</td><td>65.0</td><td><strong>71.5</strong></td><td>71.1</td></tr><tr><td>Hellaswag</td><td>49.1</td><td>52.1</td><td>67.0</td><td>67.9</td><td>74.6</td><td><strong>74.6</strong></td></tr><tr><td><em><strong>Mathematics &amp; Science Tasks</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPQA</td><td>29.8</td><td>24.8</td><td>20.7</td><td>24.2</td><td>25.3</td><td><strong>26.3</strong></td></tr><tr><td>Theoremqa</td><td>9.6</td><td>16.0</td><td>14.8</td><td>22.1</td><td>15.9</td><td><strong>27.4</strong></td></tr><tr><td>MATH</td><td>11.2</td><td>19.5</td><td>21.6</td><td>35.0</td><td>18.3</td><td><strong>42.6</strong></td></tr><tr><td>MMLU-stem</td><td>27.5</td><td>39.8</td><td>42.7</td><td>54.8</td><td>45.8</td><td><strong>62.5</strong></td></tr><tr><td>GSM8K</td><td>36.4</td><td>41.6</td><td>46.9</td><td>68.5</td><td>30.3</td><td><strong>79.1</strong></td></tr><tr><td><em><strong>Coding Tasks</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>HumanEval</td><td>22.6</td><td>30.5</td><td>34.8</td><td>37.2</td><td>19.5</td><td><strong>42.1</strong></td></tr><tr><td>HumanEval+</td><td>18.9</td><td>26.8</td><td>29.9</td><td>32.9</td><td>15.9</td><td><strong>36.0</strong></td></tr><tr><td>MBPP</td><td>33.1</td><td>39.3</td><td>46.9</td><td><strong>60.2</strong></td><td>42.1</td><td>57.1</td></tr><tr><td>MBPP+</td><td>27.6</td><td>33.8</td><td>37.6</td><td><strong>49.6</strong></td><td>33.6</td><td>49.4</td></tr><tr><td>MultiPL-E</td><td>16.3</td><td>18.9</td><td>27.9</td><td>33.1</td><td>17.6</td><td><strong>41.2</strong></td></tr><tr><td><em><strong>Multilingual Tasks</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Multi-Exam</td><td>29.4</td><td>30.8</td><td>43.1</td><td>47.9</td><td>38.1</td><td><strong>54.6</strong></td></tr><tr><td>Multi-Understanding</td><td>40.4</td><td>41.0</td><td>50.7</td><td>65.1</td><td>46.8</td><td><strong>76.6</strong></td></tr><tr><td>Multi-Mathematics</td><td>7.8</td><td>13.5</td><td>21.3</td><td>37.5</td><td>18.2</td><td><strong>48.9</strong></td></tr><tr><td>Multi-Translation</td><td>14.1</td><td>15.3</td><td>23.8</td><td>25.0</td><td>26.9</td><td><strong>29.3</strong></td></tr></tbody></table></div>
<p>For edge-side models, zen-0.5B, 1.5B, and 3B continue to maintain strong performance across nearly all benchmarks. Notably, the zen-0.5B model outperforms the Gemma2-2.6B on various math and coding tasks.</p>
<h2 class="flex scroll-m-28 flex-row items-center gap-2" id="instruction-tuned-model-evaluation"><a data-card="" href="#instruction-tuned-model-evaluation" class="peer">Instruction-tuned Model Evaluation</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h2>
<p>The evaluation of instruction-tuned models mainly focuses on the model performance of natural language understanding, general question answering, reasoning, coding, mathematics, instruction following, human alignment, etc.</p>
<p>The datasets for evaluation include:</p>
<p><strong>General Tasks</strong> : MMLU-Pro, MMLU-redux</p>
<p><strong>Math &amp; Science Tasks</strong>: GPQA, GSM8K, MATH</p>
<p><strong>Coding Tasks</strong> : HumanEval, MBPP, MultiPL-E, LiveCodeBench 2305-2409, LiveBench 0831</p>
<p><strong>Instruction &amp; Alignment Tasks</strong>: IFeval strict-prompt, Arena-Hard, AlignBench v1.1, MTbench</p>
<h3 class="flex scroll-m-28 flex-row items-center gap-2" id="zen-72b-instruct-performance"><a data-card="" href="#zen-72b-instruct-performance" class="peer">zen-72B-Instruct Performance</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>





























































































































<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Datasets</th><th>Mistral-Large2 Instruct</th><th>Llama-3.1-70B-Instruct</th><th>Llama-3.1-405B-Instruct</th><th>zen-72B-Instruct</th><th><strong>zen-72B-Instruct</strong></th></tr></thead><tbody><tr><td>MMLU-Pro</td><td>69.4</td><td>66.4</td><td><strong>73.3</strong></td><td>64.4</td><td>71.1</td></tr><tr><td>MMLU-redux</td><td>83.0</td><td>83.0</td><td>86.2</td><td>81.6</td><td><strong>86.8</strong></td></tr><tr><td>GPQA</td><td><strong>52.0</strong></td><td>46.7</td><td>51.1</td><td>42.4</td><td>49.0</td></tr><tr><td>MATH</td><td>69.9</td><td>68.0</td><td>73.8</td><td>69.0</td><td><strong>83.1</strong></td></tr><tr><td>GSM8K</td><td>92.7</td><td>95.1</td><td><strong>96.8</strong></td><td>93.2</td><td>95.8</td></tr><tr><td>HumanEval</td><td><strong>92.1</strong></td><td>80.5</td><td>89.0</td><td>86.0</td><td>86.6</td></tr><tr><td>MBPP</td><td>80.0</td><td>84.2</td><td>84.5</td><td>80.2</td><td><strong>88.2</strong></td></tr><tr><td>MultiPL-E</td><td><strong>76.9</strong></td><td>68.2</td><td>73.5</td><td>69.2</td><td>75.1</td></tr><tr><td>LiveCodeBench 2305-2409</td><td>42.2</td><td>32.1</td><td>41.6</td><td>32.2</td><td><strong>55.5</strong></td></tr><tr><td>LiveBench 0831</td><td>48.5</td><td>46.6</td><td><strong>53.2</strong></td><td>41.5</td><td>52.3</td></tr><tr><td>IFeval strict-prompt</td><td>64.1</td><td>83.6</td><td><strong>86.0</strong></td><td>77.6</td><td>84.1</td></tr><tr><td>Arena-Hard</td><td>73.1</td><td>55.7</td><td>69.3</td><td>48.1</td><td><strong>81.2</strong></td></tr><tr><td>AlignBench v1.1</td><td>7.69</td><td>5.94</td><td>5.95</td><td>8.15</td><td><strong>8.16</strong></td></tr><tr><td>MTbench</td><td>8.61</td><td>8.79</td><td>9.08</td><td>9.12</td><td><strong>9.35</strong></td></tr></tbody></table></div>
<p>The zen-72B-Instruct model delivers exceptional performance, even surpassing the larger Llama-3.1-405B in several critical tasks. zen-72B-Instruct excels in mathematics (MATH: 83.1), coding (LiveCodeBench: 55.5), and chatting (Arena-Hard: 81.2). Compared to its base model zen-72B and its predecessor zen-72B-Instruct, the zen-72B-Instruct showcases comprehensive improvements across all tasks.</p>
<h3 class="flex scroll-m-28 flex-row items-center gap-2" id="qwen-turbo--zen-14b-instruct--zen-32b-instruct-performance"><a data-card="" href="#qwen-turbo--zen-14b-instruct--zen-32b-instruct-performance" class="peer">Qwen-Turbo &amp; zen-14B-Instruct &amp; zen-32B-Instruct Performance</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>












































































































































<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Datasets</th><th>zen7B-A14B-Instruct</th><th>Gemma2-27B-IT</th><th>GPT4o-mini</th><th><strong>Qwen-Turbo</strong></th><th><strong>zen-14B-Instruct</strong></th><th><strong>zen-32B-Instruct</strong></th></tr></thead><tbody><tr><td>MMLU-Pro</td><td>52.8</td><td>55.5</td><td>63.1</td><td>64.8</td><td>63.7</td><td><strong>69.0</strong></td></tr><tr><td>MMLU-redux</td><td>72.6</td><td>75.7</td><td>81.5</td><td>80.4</td><td>80.0</td><td><strong>83.9</strong></td></tr><tr><td>GPQA</td><td>34.3</td><td>38.4</td><td>40.2</td><td>44.4</td><td>45.5</td><td><strong>49.5</strong></td></tr><tr><td>MATH</td><td>49.1</td><td>54.4</td><td>70.2</td><td>81.0</td><td>80.0</td><td><strong>83.1</strong></td></tr><tr><td>GSM8K</td><td>85.3</td><td>90.4</td><td>93.2</td><td>93.6</td><td>94.8</td><td><strong>95.9</strong></td></tr><tr><td>HumanEval</td><td>79.9</td><td>78.7</td><td><strong>88.4</strong></td><td>86.6</td><td>83.5</td><td><strong>88.4</strong></td></tr><tr><td>MBPP</td><td>70.9</td><td>81.0</td><td><strong>85.7</strong></td><td>80.2</td><td>82.0</td><td>84.0</td></tr><tr><td>MultiPL-E</td><td>66.4</td><td>67.4</td><td>75.0</td><td>73.0</td><td>72.8</td><td><strong>75.4</strong></td></tr><tr><td>LiveCodeBench 2305-2409</td><td>22.5</td><td>-</td><td>40.7</td><td>43.1</td><td>42.6</td><td><strong>51.2</strong></td></tr><tr><td>LiveBench 0831</td><td>31.1</td><td>39.6</td><td>43.3</td><td>41.6</td><td>44.4</td><td><strong>50.7</strong></td></tr><tr><td>IFeval strict-prompt</td><td>59.9</td><td>77.1</td><td>80.4</td><td>74.9</td><td><strong>81.0</strong></td><td>79.5</td></tr><tr><td>Arena-Hard</td><td>17.8</td><td>57.5</td><td><strong>74.9</strong></td><td>68.4</td><td>68.3</td><td>74.5</td></tr><tr><td>AlignBench v1.1</td><td>7.02</td><td>7.22</td><td>7.81</td><td><strong>7.99</strong></td><td>7.94</td><td>7.93</td></tr><tr><td>MTbench</td><td>8.55</td><td>9.10</td><td>-</td><td>8.86</td><td>8.88</td><td><strong>9.20</strong></td></tr></tbody></table></div>
<p>The zen-32B-Instruct model demonstrates superior performance across most tasks when compared to other models of similar size. In comparison to GPT-4o-mini, our open-source model, zen-14B-Instruct, along with our API model, Qwen-Turbo, also deliver competitive results across all benchmarks.</p>
<h3 class="flex scroll-m-28 flex-row items-center gap-2" id="zen-7b-instruct-performance"><a data-card="" href="#zen-7b-instruct-performance" class="peer">zen-7B-Instruct Performance</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>














































































































<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Datasets</th><th>Gemma2-9b-IT</th><th>Llama3.1-8B-Instruct</th><th>zen-7B-Instruct</th><th><strong>zen-7B-Instruct</strong></th></tr></thead><tbody><tr><td>MMLU-Pro</td><td>52.1</td><td>48.3</td><td>44.1</td><td><strong>56.3</strong></td></tr><tr><td>MMLU-redux</td><td>72.8</td><td>67.2</td><td>67.3</td><td><strong>75.4</strong></td></tr><tr><td>GPQA</td><td>32.8</td><td>32.8</td><td>34.3</td><td><strong>36.4</strong></td></tr><tr><td>MATH</td><td>44.3</td><td>51.9</td><td>52.9</td><td><strong>75.5</strong></td></tr><tr><td>GSM8K</td><td>76.7</td><td>84.5</td><td>85.7</td><td><strong>91.6</strong></td></tr><tr><td>HumanEval</td><td>68.9</td><td>72.6</td><td>79.9</td><td><strong>84.8</strong></td></tr><tr><td>MBPP</td><td>74.9</td><td>69.6</td><td>67.2</td><td><strong>79.2</strong></td></tr><tr><td>MultiPL-E</td><td>53.4</td><td>50.7</td><td>59.1</td><td><strong>70.4</strong></td></tr><tr><td>LiveCodeBench 2305-2409</td><td>18.9</td><td>8.3</td><td>23.9</td><td><strong>28.7</strong></td></tr><tr><td>LiveBench 0831</td><td>30.6</td><td>26.7</td><td>29.2</td><td><strong>35.9</strong></td></tr><tr><td>IFeval strict-prompt</td><td>70.1</td><td><strong>75.9</strong></td><td>54.7</td><td>71.2</td></tr><tr><td>Arena-Hard</td><td>41.6</td><td>27.8</td><td>25.0</td><td><strong>52.0</strong></td></tr><tr><td>AlignBench v1.1</td><td>7.05</td><td>4.75</td><td>7.13</td><td><strong>7.33</strong></td></tr><tr><td>MTbench</td><td>8.49</td><td>8.23</td><td>8.26</td><td><strong>8.75</strong></td></tr></tbody></table></div>
<p>The zen-7B-Instruct model significantly outperforms its competitors, Gemma2-9b-IT and Llama3.1-8B-Instruct, across all tasks except IFeval. Notably, zen-7B-Instruct demonstrates clear advantages in mathematics (MATH: 75.5) and coding (HumanEval: 84.8).</p>
<h3 class="flex scroll-m-28 flex-row items-center gap-2" id="zen-3b-instruct-performance"><a data-card="" href="#zen-3b-instruct-performance" class="peer">zen-3B-Instruct Performance</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>
































































































<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Datasets</th><th>Gemma2-2B-IT</th><th>Phi3.5-mini-Instruct</th><th>MiniCPM3-4B</th><th><strong>zen-3B-Instruct</strong></th></tr></thead><tbody><tr><td>Non-Emb Params</td><td>2.0B</td><td>3.6B</td><td>4.0B</td><td>2.8B</td></tr><tr><td>MMLU-Pro</td><td>26.7</td><td><strong>47.5</strong></td><td>43.0</td><td>43.7</td></tr><tr><td>MMLU-redux</td><td>51.9</td><td><strong>67.7</strong></td><td>59.9</td><td>64.4</td></tr><tr><td>GPQA</td><td>29.3</td><td>27.2</td><td><strong>31.3</strong></td><td>30.3</td></tr><tr><td>MATH</td><td>26.6</td><td>48.5</td><td>46.6</td><td><strong>65.9</strong></td></tr><tr><td>GSM8K</td><td>63.2</td><td>86.2</td><td>81.1</td><td><strong>86.7</strong></td></tr><tr><td>HumanEval</td><td>68.9</td><td>72.6</td><td><strong>74.4</strong></td><td><strong>74.4</strong></td></tr><tr><td>MBPP</td><td><strong>74.9</strong></td><td>63.2</td><td>72.5</td><td>72.7</td></tr><tr><td>MultiPL-E</td><td>30.5</td><td>47.2</td><td>49.1</td><td><strong>60.2</strong></td></tr><tr><td>LiveCodeBench 2305-2409</td><td>5.8</td><td>15.8</td><td><strong>23.8</strong></td><td>19.9</td></tr><tr><td>LiveBench 0831</td><td>20.1</td><td>27.4</td><td><strong>27.6</strong></td><td>26.8</td></tr><tr><td>IFeval strict-prompt</td><td>51.0</td><td>52.1</td><td><strong>68.4</strong></td><td>58.2</td></tr></tbody></table></div>
<p>As for the edge-side instruction model, the zen-3B-Instruct model has fewer parameters than both the Phi3.5-mini-Instruct and MiniCPM3-4B models. Despite this, it outperforms them in mathematics and coding tasks while delivering competitive results in language understanding.</p>
<h3 class="flex scroll-m-28 flex-row items-center gap-2" id="zen-05b15b-instruct-performance"><a data-card="" href="#zen-05b15b-instruct-performance" class="peer">zen-0.5B/1.5B-Instruct Performance</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>

























































































<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Datasets</th><th>zen-0.5B-Instruct</th><th><strong>zen-0.5B-Instruct</strong></th><th>zen-1.5B-Instruct</th><th><strong>zen-1.5B-Instruct</strong></th></tr></thead><tbody><tr><td>MMLU-Pro</td><td>14.4</td><td><strong>15.0</strong></td><td>22.9</td><td><strong>32.4</strong></td></tr><tr><td>MMLU-redux</td><td>12.9</td><td><strong>24.1</strong></td><td>41.2</td><td><strong>50.7</strong></td></tr><tr><td>GPQA</td><td>23.7</td><td><strong>29.8</strong></td><td>21.2</td><td><strong>29.8</strong></td></tr><tr><td>MATH</td><td>13.9</td><td><strong>34.4</strong></td><td>25.3</td><td><strong>55.2</strong></td></tr><tr><td>GSM8K</td><td>40.1</td><td><strong>49.6</strong></td><td>61.6</td><td><strong>73.2</strong></td></tr><tr><td>HumanEval</td><td>31.1</td><td><strong>35.4</strong></td><td>42.1</td><td><strong>61.6</strong></td></tr><tr><td>MBPP</td><td>39.7</td><td><strong>49.6</strong></td><td>44.2</td><td><strong>63.2</strong></td></tr><tr><td>MultiPL-E</td><td>20.8</td><td><strong>28.5</strong></td><td>38.5</td><td><strong>50.4</strong></td></tr><tr><td>LiveCodeBench 2305-2409</td><td>1.6</td><td><strong>5.1</strong></td><td>4.5</td><td><strong>14.8</strong></td></tr><tr><td>LiveBench 0831</td><td>7.4</td><td><strong>12.6</strong></td><td>12.4</td><td><strong>18.8</strong></td></tr><tr><td>IFeval strict-prompt</td><td>14.6</td><td><strong>27.9</strong></td><td>29.0</td><td><strong>42.5</strong></td></tr></tbody></table></div>
<p>zen-1.5B-Instruct and zen-0.5B-Instruct have seen large performance improvements over their previous versions, making them well-suited for edge-side applications in highly resource-constrained environments.</p>
<h3 class="flex scroll-m-28 flex-row items-center gap-2" id="performances-on-multilingualism"><a data-card="" href="#performances-on-multilingualism" class="peer">Performances on Multilingualism</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h3>
<p>To evaluate the multilingual performance of instruction-tuned models, we collect and extend benchmarks as follows:</p>
<ul>
<li><strong>IFEval (multilingual)</strong> : We translate the examples from IFEval (English version) to construct multilingual IFEval examples after removing examples with language-specific contents (e.g., “start with letter A”). We collect 100 examples for each language among Arabic (ar), Spanish (es), French (fr), Indonesian (in), Japanese (ja), Korean (ko), Portuguese (pt), and Vietnamese (vi) languages. All examples are checked and post-edited (if neccessary) by paid volunteers.</li>
<li><strong>Knowledge</strong> : We use 5 MMLU-like benchmarks (multi-choice) to testify the knowledge utilization ability of zen series models on multilingualism, including AMMLU (Arabic), JMMLU (Japanese), KMMLU (Korean), IndoMMLU (Indonesian), and TurkishMMLU (Turkish). Also, we present the performances on translated MMLU (i.e., okapi_MMLU, from English to multiple languages).</li>
<li><strong>MGSM8K (extended)</strong> : Aside from the examples in the original MGSM8K benchmark, we extend the language support with Arabic (ar), Korean (ko), Portuguese (pt), and Vietnamese (vi). We translate 250 examples (same as the other languages engaged in MGSM8K) into those 4 languages. All examples are also checked and post-edited (if necessary) by paid volunteers.</li>
<li><strong>Cultural Nuances</strong> : We also use BLEnD, a benchmark aiming at testifying cultural nuances of LLMs, to testify LLMs from the zen series.</li>
</ul>










































































































































































































































































<div class="relative overflow-auto prose-no-margin my-6"><table><thead><tr><th>Datasets</th><th>zen-72B-Instruct</th><th>Llama3.1-70B-Instruct</th><th>zen-32B-Instruct</th><th>Mistral-Large-Instruct-2407 (123B)</th><th>GPT4o-mini</th><th>zen-72B-Instruct</th></tr></thead><tbody><tr><td><em><strong>Instruction Following</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>IFEval (multilingual)</td><td>79.69</td><td>80.47</td><td>82.68</td><td>82.69</td><td>85.03</td><td><strong>86.98</strong></td></tr><tr><td><em><strong>Knowledge</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>AMMLU (Arabic)</td><td>68.85</td><td>70.08</td><td>70.44</td><td>69.24</td><td>69.73</td><td><strong>72.44</strong></td></tr><tr><td>JMMLU (Japanese)</td><td>77.37</td><td>73.89</td><td>76.55</td><td>75.77</td><td>73.74</td><td><strong>80.56</strong></td></tr><tr><td>KMMLU (Korean)</td><td>57.04</td><td>53.23</td><td>60.75</td><td>56.42</td><td>56.77</td><td><strong>61.96</strong></td></tr><tr><td>IndoMMLU (Indonesian)</td><td>66.31</td><td>67.50</td><td>66.42</td><td>63.21</td><td>67.75</td><td><strong>69.25</strong></td></tr><tr><td>TurkishMMLU (Turkish)</td><td>69.22</td><td>66.89</td><td>72.41</td><td>64.78</td><td>71.19</td><td><strong>76.12</strong></td></tr><tr><td>okapi MMLU (translated)</td><td>77.84</td><td>76.49</td><td>77.16</td><td>78.37</td><td>73.44</td><td><strong>79.97</strong></td></tr><tr><td><em><strong>Math Reasoning</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MGSM8K (extended)</td><td>82.72</td><td>73.31</td><td>87.15</td><td><strong>89.01</strong></td><td>87.36</td><td>88.16</td></tr><tr><td><em><strong>Cultural Nuances</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>BLEnD</td><td>25.90</td><td>30.49</td><td>27.88</td><td>33.47</td><td><strong>35.91</strong></td><td>32.48</td></tr><tr><td>Datasets</td><td>zen-7B-Instruct</td><td>Llama3.1-8B-Instruct</td><td>zen-7B-Instruct</td><td>Gemma-2-9B-Instruct</td><td>Mistral-Nemo-Instruct-2407 (12B)</td><td>zen-14B-Instruct</td></tr><tr><td>---</td><td>---</td><td>---</td><td>---</td><td>---</td><td>---</td><td>---</td></tr><tr><td><em><strong>Instruction Following</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>IFEval (multilingual)</td><td>51.43</td><td>60.68</td><td>74.87</td><td><strong>77.47</strong></td><td>64.59</td><td>77.08</td></tr><tr><td><em><strong>Knowledge</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>AMMLU (Arabic)</td><td>54.87</td><td>54.28</td><td>59.78</td><td>60.26</td><td>53.92</td><td><strong>66.81</strong></td></tr><tr><td>JMMLU (Japanese)</td><td>57.71</td><td>53.26</td><td>61.88</td><td>64.59</td><td>55.17</td><td><strong>72.78</strong></td></tr><tr><td>KMMLU (Korean)</td><td>43.96</td><td>42.28</td><td>46.59</td><td>46.24</td><td>42.22</td><td><strong>59.71</strong></td></tr><tr><td>IndoMMLU (Indonesian)</td><td>54.05</td><td>53.92</td><td>56.42</td><td>61.73</td><td>50.76</td><td><strong>65.09</strong></td></tr><tr><td>TurkishMMLU (Turkish)</td><td>49.27</td><td>45.61</td><td>54.28</td><td>55.44</td><td>34.44</td><td><strong>66.85</strong></td></tr><tr><td>okapi MMLU (translated)</td><td>60.47</td><td>55.18</td><td>66.98</td><td>46.72</td><td>59.65</td><td><strong>72.12</strong></td></tr><tr><td><em><strong>Math Reasoning</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>MGSM8K (extended)</td><td>56.13</td><td>66.05</td><td>66.11</td><td>78.37</td><td>54.75</td><td><strong>82.27</strong></td></tr><tr><td><em><strong>Cultural Nuances</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>BLEnD</td><td>22.49</td><td>19.47</td><td>23.66</td><td><strong>28.31</strong></td><td>26.61</td><td>26.99</td></tr></tbody></table></div>
<h1 class="flex scroll-m-28 flex-row items-center gap-2" id="demo-cases"><a data-card="" href="#demo-cases" class="peer">Demo Cases</a><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100" aria-hidden="true"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></h1>
<p>Here we provide several cases to demonstrate the new or enhanced capabilities of zen, including generating JSON output, generating long texts, and understanding structured data.</p>
<p>Example: Generating JSON Output Next</p>
<p>JSON Output</p>
<p>Example: Structured Data Understanding Next</p>
<p>Table Understanding</p>
<p>Example: Long Text Generation Next</p>
<p>Text Generation</p></div></main><!--$--><!--/$--><script src="/_next/static/chunks/e83606e8fa9cc796.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[106,[\"/_next/static/chunks/59d0ad1b64f8544e.js\"],\"RootProvider\"]\n3:I[53113,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"default\"]\n4:I[73211,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"default\"]\n5:I[10086,[\"/_next/static/chunks/59d0ad1b64f8544e.js\",\"/_next/static/chunks/8de849ca74fc071f.js\",\"/_next/static/chunks/e62b91212ee7f8ff.js\",\"/_next/static/chunks/f19fe44237e54646.js\",\"/_next/static/chunks/cb0a883bafeb6805.js\"],\"\"]\n7:I[89923,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"OutletBoundary\"]\n8:\"$Sreact.suspense\"\na:I[89923,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"ViewportBoundary\"]\nc:I[89923,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"MetadataBoundary\"]\ne:I[6998,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"default\"]\n:HL[\"/_next/static/chunks/f2332aac77592f9d.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"i-dnJM_MIpJSOCQWNJVMq\",\"c\":[\"\",\"blog\",\"qwen2.5-llm\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"qwen2.5-llm\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/f2332aac77592f9d.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/59d0ad1b64f8544e.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"dark\",\"suppressHydrationWarning\":true,\"children\":[\"$\",\"body\",null,{\"className\":\"antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"main\",null,{\"className\":\"flex min-h-screen flex-col items-center justify-center px-4 text-center\",\"children\":[[\"$\",\"div\",null,{\"className\":\"mb-8 opacity-20\",\"children\":[\"$\",\"svg\",null,{\"width\":\"120\",\"height\":\"120\",\"viewBox\":\"0 0 120 120\",\"fill\":\"none\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"circle\",null,{\"cx\":\"60\",\"cy\":\"60\",\"r\":\"50\",\"stroke\":\"currentColor\",\"strokeWidth\":\"3\",\"strokeLinecap\":\"round\",\"strokeDasharray\":\"280 40\"}]}]}],[\"$\",\"p\",null,{\"className\":\"text-xs font-mono uppercase tracking-[0.3em] text-fd-muted-foreground mb-4\",\"children\":\"404\"}],[\"$\",\"h1\",null,{\"className\":\"text-3xl font-semibold mb-3\",\"children\":\"Page not found\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground max-w-sm mb-10\",\"children\":\"This page doesn't exist, or it may have moved. Try the documentation or head home.\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-3 justify-center\",\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/\",\"className\":\"inline-flex items-center gap-2 rounded-lg bg-fd-primary text-fd-primary-foreground px-4 py-2 text-sm font-medium hover:opacity-90 transition\",\"children\":\"Go home\"}],[\"$\",\"$L5\",null,{\"href\":\"/docs\",\"className\":\"inline-flex items-center gap-2 rounded-lg border border-fd-border bg-fd-background px-4 py-2 text-sm font-medium hover:bg-fd-muted transition\",\"children\":\"Documentation\"}],[\"$\",\"$L5\",null,{\"href\":\"/docs/models\",\"className\":\"inline-flex items-center gap-2 rounded-lg border border-fd-border bg-fd-background px-4 py-2 text-sm font-medium hover:bg-fd-muted transition\",\"children\":\"Browse models\"}]]}],[\"$\",\"p\",null,{\"className\":\"mt-16 text-xs font-mono text-fd-muted-foreground opacity-50\",\"children\":\"zenlm.org\"}]]}],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[\"$L6\",[[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/8de849ca74fc071f.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-1\",{\"src\":\"/_next/static/chunks/e62b91212ee7f8ff.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-2\",{\"src\":\"/_next/static/chunks/f19fe44237e54646.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-3\",{\"src\":\"/_next/static/chunks/cb0a883bafeb6805.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"$L7\",null,{\"children\":[\"$\",\"$8\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@9\"}]}]]}],{},null,false,false]},null,false,false]},null,false,false]},null,false,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$La\",null,{\"children\":\"$Lb\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$Lc\",null,{\"children\":[\"$\",\"$8\",null,{\"name\":\"Next.Metadata\",\"children\":\"$Ld\"}]}]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$e\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"f:I[48068,[\"/_next/static/chunks/59d0ad1b64f8544e.js\",\"/_next/static/chunks/8de849ca74fc071f.js\",\"/_next/static/chunks/e62b91212ee7f8ff.js\",\"/_next/static/chunks/f19fe44237e54646.js\",\"/_next/static/chunks/cb0a883bafeb6805.js\"],\"default\"]\n"])</script><script>self.__next_f.push([1,"6:[\"$\",\"main\",null,{\"className\":\"mx-auto w-full max-w-2xl px-4 py-16\",\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog\",\"className\":\"inline-flex items-center gap-1.5 text-sm text-fd-muted-foreground hover:text-fd-foreground mb-10 transition-colors\",\"children\":\"← Back to Blog\"}],[\"$\",\"div\",null,{\"className\":\"mb-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"September 18, 2024\"}],[\"$\",\"h1\",null,{\"className\":\"text-3xl font-bold mt-2 mb-3\",\"children\":\"zen-LLM: Extending the boundary of LLMs\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-lg mb-4\",\"children\":\"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3 pt-4 border-t border-fd-border\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-sm text-fd-muted-foreground\",\"children\":[\"By \",\"Zen LM Team\"]}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"prose dark:prose-invert max-w-none\",\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"$Lf\",null,{\"href\":\"https://github.com/QwenLM/zen\",\"children\":\"GITHUB\"}],\" \",[\"$\",\"$Lf\",null,{\"href\":\"https://huggingface.co/Qwen\",\"children\":\"HUGGING FACE\"}],\" \",[\"$\",\"$Lf\",null,{\"href\":\"https://modelscope.cn/organization/qwen\",\"children\":\"MODELSCOPE\"}],\" \",[\"$\",\"$Lf\",null,{\"href\":\"https://huggingface.co/spaces/Qwen/zen-72B-Instruct\",\"children\":\"DEMO\"}],\" \",[\"$\",\"$Lf\",null,{\"href\":\"https://discord.gg/yPEP2vHTu4\",\"children\":\"DISCORD\"}]]}],\"\\n\",[\"$\",\"h1\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"introduction\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#introduction\",\"className\":\"peer\",\"children\":\"Introduction\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"In this blog, we delve into the details of our latest zen series language models. We have developed a range of decoder-only dense models, with seven of them open-sourced, spanning from 0.5B to 72B parameters. Our research indicates a significant interest among users in models within the 10-30B range for production use, as well as 3B models for mobile applications. To meet these demands, we are open-sourcing zen-3B, zen-14B, and zen-32B. Furthermore, we are excited to offer additional models, including Qwen-Plus and Qwen-Turbo, available through API services via \",[\"$\",\"$Lf\",null,{\"href\":\"https://help.aliyun.com/zh/model-studio/developer-reference/what-is-qwen-llm\",\"children\":\"Alibaba Cloud Model Studio\"}],\".\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Compared with the zen series, the zen series has the following upgrades:\"}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Full-scale Open-source\"}],\" : Considering that users have a strong interest in models in the 10-30B range for production and 3B models for mobile applications, zen, in addition to continuing to open source the four models of 0.5/1.5/7/72B of the same size as zen, also added two medium-sized cost-effective models of \",[\"$\",\"strong\",null,{\"children\":\"zen-14B\"}],\" and \",[\"$\",\"strong\",null,{\"children\":\"zen-32B\"}],\" and a mobile-side model called \",[\"$\",\"strong\",null,{\"children\":\"zen-3B\"}],\". All models are highly competitive compared to open-source models of the same level. For example, zen-32B beats zen-72B and zen-14B outperforms zen7B-A14B in our comprehensive evaluations.\"]}],\"\\n\"]}],\"\\n\",\"$L10\",\"\\n\",\"$L11\",\"\\n\",\"$L12\",\"\\n\",\"$L13\",\"\\n\",\"$L14\",\"\\n\",\"$L15\",\"\\n\"]}],\"\\n\",\"$L16\",\"\\n\",\"$L17\",\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L18\",\"\\n\",\"$L19\",\"\\n\",\"$L1a\",\"\\n\",\"$L1b\",\"\\n\",\"$L1c\",\"\\n\",\"$L1d\",\"\\n\",\"$L1e\",\"\\n\",\"$L1f\",\"\\n\",\"$L20\",\"\\n\",\"$L21\",\"\\n\",\"$L22\",\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L23\",\"\\n\",\"$L24\",\"\\n\",\"$L25\",\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L26\",\"\\n\",\"$L27\",\"\\n\",\"$L28\",\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L29\",\"\\n\",\"$L2a\",\"\\n\",\"$L2b\",\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L2c\",\"\\n\",\"$L2d\",\"\\n\",\"$L2e\",\"\\n\",\"$L2f\",\"\\n\",\"$L30\",\"\\n\",\"$L31\",\"\\n\",\"$L32\",\"\\n\",\"$L33\",\"\\n\",\"$L34\",\"\\n\",\"$L35\",\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L36\",\"\\n\",\"$L37\",\"\\n\",\"$L38\",\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L39\",\"\\n\",\"$L3a\",\"\\n\",\"$L3b\",\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L3c\",\"\\n\",\"$L3d\",\"\\n\",\"$L3e\",\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L3f\",\"\\n\",\"$L40\",\"\\n\",\"$L41\",\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L42\",\"\\n\",\"$L43\",\"\\n\",\"$L44\",\"\\n\",\"$L45\",\"\\n\",\"$L46\",\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\"$L47\",\"\\n\",\"$L48\",\"\\n\",\"$L49\",\"\\n\",\"$L4a\",\"\\n\",\"$L4b\",\"\\n\",\"$L4c\",\"\\n\",\"$L4d\",\"\\n\",\"$L4e\",\"\\n\",\"$L4f\"]}]]}]\n"])</script><script>self.__next_f.push([1,"10:[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Larger and Higher Quality Pre-training Dataset\"}],\" : The size of the pre-training dataset is expanded from 7 trillion tokens to a maximum of \",[\"$\",\"strong\",null,{\"children\":\"18 trillion\"}],\" tokens.\"]}],\"\\n\"]}]\n11:[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Knowledge Enhancement\"}],\" : zen has acquired significantly more knowledge. On MMLU benchmarks, zen-7/72B are improved from 70.3 to \",[\"$\",\"strong\",null,{\"children\":\"74.2\"}],\" and 84.2 to \",[\"$\",\"strong\",null,{\"children\":\"86.1\"}],\" compared to zen-7/72B. We observe that zen also has significant improvements on the GPQA/MMLU-Pro/MMLU-redux/ARC-c benchmarks.\"]}],\"\\n\"]}]\n12:[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Coding Enhancement\"}],\" : Thanks to the technical breakthrough of zen-Coder, zen has greatly improved capabilities in coding. zen-72B-Instruct achieves \",[\"$\",\"strong\",null,{\"children\":\"55.5\"}],\" , \",[\"$\",\"strong\",null,{\"children\":\"75.1\"}],\" , and \",[\"$\",\"strong\",null,{\"children\":\"88.2\"}],\" scores on LiveCodeBench (2305-2409), MultiPL-E and MBPP, respectively, outperforming zen-72B-Instruct with 32.2, 69.2, and 80.2.\"]}],\"\\n\"]}]\n13:[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Math Enhancement\"}],\" : After integrating zen-math’s technology, the mathematical ability of zen has also been rapidly improved. On the MATH benchmark, the scores of zen-7B/72B-Instruct have been increased from 52.9/69.0 of zen-7B/72B-Instruct to \",[\"$\",\"strong\",null,{\"children\":\"75.5/83.1\"}],\".\"]}],\"\\n\"]}]\n14:[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Better Human Preference\"}],\" : zen is capable of generating responses that align more closely with human preferences. Specifically, the Arena-Hard score for zen-72B-Instruct has increased significantly from \",[\"$\",\"strong\",null,{\"children\":\"48.1\"}],\" to \",[\"$\",\"strong\",null,{\"children\":\"81.2\"}],\" , and the MT-Bench score has improved from \",[\"$\",\"strong\",null,{\"children\":\"9.12\"}],\" to \",[\"$\",\"strong\",null,{\"children\":\"9.35\"}],\" , compared to zen-72B-Instruct.\"]}],\"\\n\"]}]\n15:[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Other Core Capabilities Enhancement\"}],\" : zen achieves significant improvements in \",[\"$\",\"strong\",null,{\"children\":\"instruction following\"}],\" , \",[\"$\",\"strong\",null,{\"children\":\"generating long texts\"}],\" (increased from 1k to over \",[\"$\",\"strong\",null,{\"children\":\"8K tokens\"}],\"), \",[\"$\",\"strong\",null,{\"children\":\"understanding structured data\"}],\" (e.g., tables), and \",[\"$\",\"strong\",null,{\"children\":\"generating structured outputs\"}],\" , especially JSON. Furthermore, zen models are generally more resilient to the diversity of \",[\"$\",\"strong\",null,{\"children\":\"system prompts\"}],\" , enhancing \",[\"$\",\"strong\",null,{\"children\":\"role-play\"}],\" implementation and \",[\"$\",\"strong\",null,{\"children\":\"condition-setting\"}],\" for chatbots.\"]}],\"\\n\"]}]\n16:[\"$\",\"h1\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"model-card\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#model-card\",\"className\":\"peer\",\"children\":\"Model Card\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n17:[\"$\",\"p\",null,{\"children\":\"Here is a model card detailing the key parameters of the zen LLM models. This release includes seven open-sourced models with sizes ranging from 0.5B to 72"])</script><script>self.__next_f.push([1,"B. Most models support a context length of 128K (131,072) tokens and can generate up to 8K tokens, enabling the production of extensive text outputs. The majority of these models are licensed under Apache 2.0, while zen-3B and zen-72B are governed by the Qwen Research License and Qwen License, respectively.\"}]\n"])</script><script>self.__next_f.push([1,"18:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Models\"}],[\"$\",\"th\",null,{\"children\":\"Params\"}],[\"$\",\"th\",null,{\"children\":\"Non-Emb Params\"}],[\"$\",\"th\",null,{\"children\":\"Layers\"}],[\"$\",\"th\",null,{\"children\":\"Heads (KV)\"}],[\"$\",\"th\",null,{\"children\":\"Tie Embedding\"}],[\"$\",\"th\",null,{\"children\":\"Context Length\"}],[\"$\",\"th\",null,{\"children\":\"Generation Length\"}],[\"$\",\"th\",null,{\"children\":\"License\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"zen-0.5B\"}],[\"$\",\"td\",null,{\"children\":\"0.49B\"}],[\"$\",\"td\",null,{\"children\":\"0.36B\"}],[\"$\",\"td\",null,{\"children\":\"24\"}],[\"$\",\"td\",null,{\"children\":\"14 / 2\"}],[\"$\",\"td\",null,{\"children\":\"Yes\"}],[\"$\",\"td\",null,{\"children\":\"32K\"}],[\"$\",\"td\",null,{\"children\":\"8K\"}],[\"$\",\"td\",null,{\"children\":\"Apache 2.0\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"zen-1.5B\"}],[\"$\",\"td\",null,{\"children\":\"1.54B\"}],[\"$\",\"td\",null,{\"children\":\"1.31B\"}],[\"$\",\"td\",null,{\"children\":\"28\"}],[\"$\",\"td\",null,{\"children\":\"12 / 2\"}],[\"$\",\"td\",null,{\"children\":\"Yes\"}],[\"$\",\"td\",null,{\"children\":\"32K\"}],[\"$\",\"td\",null,{\"children\":\"8K\"}],[\"$\",\"td\",null,{\"children\":\"Apache 2.0\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"zen-3B\"}],[\"$\",\"td\",null,{\"children\":\"3.09B\"}],[\"$\",\"td\",null,{\"children\":\"2.77B\"}],[\"$\",\"td\",null,{\"children\":\"36\"}],[\"$\",\"td\",null,{\"children\":\"16 / 2\"}],[\"$\",\"td\",null,{\"children\":\"Yes\"}],[\"$\",\"td\",null,{\"children\":\"32K\"}],[\"$\",\"td\",null,{\"children\":\"8K\"}],[\"$\",\"td\",null,{\"children\":\"Qwen Research\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"zen-7B\"}],[\"$\",\"td\",null,{\"children\":\"7.61B\"}],[\"$\",\"td\",null,{\"children\":\"6.53B\"}],[\"$\",\"td\",null,{\"children\":\"28\"}],[\"$\",\"td\",null,{\"children\":\"28 / 4\"}],[\"$\",\"td\",null,{\"children\":\"No\"}],[\"$\",\"td\",null,{\"children\":\"128K\"}],[\"$\",\"td\",null,{\"children\":\"8K\"}],[\"$\",\"td\",null,{\"children\":\"Apache 2.0\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"zen-14B\"}],[\"$\",\"td\",null,{\"children\":\"14.7B\"}],[\"$\",\"td\",null,{\"children\":\"13.1B\"}],[\"$\",\"td\",null,{\"children\":\"48\"}],[\"$\",\"td\",null,{\"children\":\"40 / 8\"}],[\"$\",\"td\",null,{\"children\":\"No\"}],[\"$\",\"td\",null,{\"children\":\"128K\"}],[\"$\",\"td\",null,{\"children\":\"8K\"}],[\"$\",\"td\",null,{\"children\":\"Apache 2.0\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"zen-32B\"}],[\"$\",\"td\",null,{\"children\":\"32.5B\"}],[\"$\",\"td\",null,{\"children\":\"31.0B\"}],[\"$\",\"td\",null,{\"children\":\"64\"}],[\"$\",\"td\",null,{\"children\":\"40 / 8\"}],[\"$\",\"td\",null,{\"children\":\"No\"}],[\"$\",\"td\",null,{\"children\":\"128K\"}],[\"$\",\"td\",null,{\"children\":\"8K\"}],[\"$\",\"td\",null,{\"children\":\"Apache 2.0\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"zen-72B\"}],[\"$\",\"td\",null,{\"children\":\"72.7B\"}],[\"$\",\"td\",null,{\"children\":\"70.0B\"}],[\"$\",\"td\",null,{\"children\":\"80\"}],[\"$\",\"td\",null,{\"children\":\"64 / 8\"}],[\"$\",\"td\",null,{\"children\":\"No\"}],[\"$\",\"td\",null,{\"children\":\"128K\"}],[\"$\",\"td\",null,{\"children\":\"8K\"}],[\"$\",\"td\",null,{\"children\":\"Qwen\"}]]}]]}]]}]}]\n"])</script><script>self.__next_f.push([1,"19:[\"$\",\"h1\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"performance\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#performance\",\"className\":\"peer\",\"children\":\"Performance\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n1a:[\"$\",\"p\",null,{\"children\":\"This section presents the performance metrics for both base language models and instruction-tuned models across various benchmark evaluations, encompassing a diverse array of domains and tasks.\"}]\n1b:[\"$\",\"h2\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"zen-base-language-model-evaluation\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#zen-base-language-model-evaluation\",\"className\":\"peer\",\"children\":\"zen Base Language Model Evaluation\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n1c:[\"$\",\"p\",null,{\"children\":\"The evaluation of base models primarily emphasizes their performance in natural language understanding, general question answering, coding, mathematics, scientific knowledge, reasoning, and multilingual capabilities.\"}]\n1d:[\"$\",\"p\",null,{\"children\":\"The evaluation datasets include:\"}]\n1e:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"General Tasks\"}],\" : MMLU (5-shot), MMLU-Pro (5-shot), MMLU-redux (5-shot), BBH (3-shot), ARC-C (25-shot), TruthfulQA (0-shot), Winogrande (5-shot), HellaSwag (10-shot)\"]}]\n1f:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Math \u0026 Science Tasks\"}],\": GPQA (5-shot), Theorem QA (5-shot), GSM8K (4-shot), MATH (4-shot)\"]}]\n20:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Coding Tasks\"}],\" : HumanEval (0-shot), HumanEval+ (0-shot), MBPP (0-shot), MBPP+ (0-shot), MultiPL-E (0-shot) (Python, C++, JAVA, PHP, TypeScript, C#, Bash, JavaScript)\"]}]\n21:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Multilingual Tasks\"}],\" : Multi-Exam (M3Exam 5-shot, IndoMMLU 3-shot, ruMMLU 5-shot, mMMLU 5-shot), Multi-Understanding (BELEBELE 5-shot, XCOPA 5-shot, XWinograd 5-shot, XStoryCloze 0-shot, PAWS-X 5-shot), Multi-Mathematics (MGSM 8-shot), Multi-Translation (Flores-101 5-shot)\"]}]\n22:[\"$\",\"h3\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"zen-72b-performance\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#zen-72b-performance\",\"className\":\"peer\",\"children\":\"zen-72B Performance\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"23:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Datasets\"}],[\"$\",\"th\",null,{\"children\":\"Llama-3-70B\"}],[\"$\",\"th\",null,{\"children\":\"Mixtral-8x22B\"}],[\"$\",\"th\",null,{\"children\":\"Llama-3-405B\"}],[\"$\",\"th\",null,{\"children\":\"zen-72B\"}],[\"$\",\"th\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"zen-72B\"}]}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"General Tasks\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU\"}],[\"$\",\"td\",null,{\"children\":\"79.5\"}],[\"$\",\"td\",null,{\"children\":\"77.8\"}],[\"$\",\"td\",null,{\"children\":\"85.2\"}],[\"$\",\"td\",null,{\"children\":\"84.2\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"86.1\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU-Pro\"}],[\"$\",\"td\",null,{\"children\":\"52.8\"}],[\"$\",\"td\",null,{\"children\":\"51.6\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"61.6\"}]}],[\"$\",\"td\",null,{\"children\":\"55.7\"}],[\"$\",\"td\",null,{\"children\":\"58.1\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU-redux\"}],[\"$\",\"td\",null,{\"children\":\"75.0\"}],[\"$\",\"td\",null,{\"children\":\"72.9\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"80.5\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"83.9\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"BBH\"}],[\"$\",\"td\",null,{\"children\":\"81.0\"}],[\"$\",\"td\",null,{\"children\":\"78.9\"}],[\"$\",\"td\",null,{\"children\":\"85.9\"}],[\"$\",\"td\",null,{\"children\":\"82.4\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"86.3\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"ARC-C\"}],[\"$\",\"td\",null,{\"children\":\"68.8\"}],[\"$\",\"td\",null,{\"children\":\"70.7\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"68.9\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"72.4\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"TruthfulQA\"}],[\"$\",\"td\",null,{\"children\":\"45.6\"}],[\"$\",\"td\",null,{\"children\":\"51.0\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"54.8\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"60.4\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"WindoGrande\"}],[\"$\",\"td\",null,{\"children\":\"85.3\"}],[\"$\",\"td\",null,{\"children\":\"85.0\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"86.7\"}]}],[\"$\",\"td\",null,{\"children\":\"85.1\"}],[\"$\",\"td\",null,{\"children\":\"83.9\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"HellaSwag\"}],[\"$\",\"td\",null,{\"children\":\"88.0\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"88.7\"}]}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"87.3\"}],[\"$\",\"td\",null,{\"children\":\"87.6\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Mathematics \u0026 Science Tasks\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GPQA\"}],[\"$\",\"td\",null,{\"children\":\"36.3\"}],[\"$\",\"td\",null,{\"children\":\"34.3\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"37.4\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"45.9\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Theoremqa\"}],[\"$\",\"td\",null,{\"children\":\"32.3\"}],[\"$\",\"td\",null,{\"children\":\"35.9\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"42.8\"}]}],[\"$\",\"td\",null,{\"children\":\"42.4\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MATH\"}],[\"$\",\"td\",null,{\"children\":\"42.5\"}],[\"$\",\"td\",null,{\"children\":\"41.7\"}],[\"$\",\"td\",null,{\"children\":\"53.8\"}],[\"$\",\"td\",null,{\"children\":\"50.9\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"62.1\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU-stem\"}],[\"$\",\"td\",null,{\"children\":\"73.7\"}],[\"$\",\"td\",null,{\"children\":\"71.7\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"79.6\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"82.7\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GSM8K\"}],[\"$\",\"td\",null,{\"children\":\"77.6\"}],[\"$\",\"td\",null,{\"children\":\"83.7\"}],[\"$\",\"td\",null,{\"children\":\"89.0\"}],[\"$\",\"td\",null,{\"children\":\"89.0\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"91.5\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Coding Tasks\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"HumanEval\"}],[\"$\",\"td\",null,{\"children\":\"48.2\"}],[\"$\",\"td\",null,{\"children\":\"46.3\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"61.0\"}]}],[\"$\",\"td\",null,{\"children\":\"64.6\"}],[\"$\",\"td\",null,{\"children\":\"59.1\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"HumanEval+\"}],[\"$\",\"td\",null,{\"children\":\"42.1\"}],[\"$\",\"td\",null,{\"children\":\"40.2\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"56.1\"}]}],[\"$\",\"td\",null,{\"children\":\"51.2\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MBPP\"}],[\"$\",\"td\",null,{\"children\":\"70.4\"}],[\"$\",\"td\",null,{\"children\":\"71.7\"}],[\"$\",\"td\",null,{\"children\":\"73.0\"}],[\"$\",\"td\",null,{\"children\":\"76.9\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"84.7\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MBPP+\"}],[\"$\",\"td\",null,{\"children\":\"58.4\"}],[\"$\",\"td\",null,{\"children\":\"58.1\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"63.9\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"69.2\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MultiPL-E\"}],\"$L50\",\"$L51\",\"$L52\",\"$L53\",\"$L54\"]}],\"$L55\",\"$L56\",\"$L57\",\"$L58\",\"$L59\"]}]]}]}]\n"])</script><script>self.__next_f.push([1,"24:[\"$\",\"p\",null,{\"children\":\"The zen-72B base model significantly outperforms its peers in the same category across a wide range of tasks. It achieves results comparable to Llama-3-405B while utilizing only one-fifth of the parameters. Furthermore, when compared to its predecessor, zen-72B, the zen-72B shows marked improvements in nearly all benchmark evaluations, particularly excelling in general tasks, mathematics, and coding challenges.\"}]\n25:[\"$\",\"h3\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"zen-14b32b-performance\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#zen-14b32b-performance\",\"className\":\"peer\",\"children\":\"zen-14B/32B Performance\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"26:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Datasets\"}],[\"$\",\"th\",null,{\"children\":\"Qwen1.5-32B\"}],[\"$\",\"th\",null,{\"children\":\"Gemma2-27B\"}],[\"$\",\"th\",null,{\"children\":\"Yi-1.5-34B\"}],[\"$\",\"th\",null,{\"children\":\"zen7B-A14B\"}],[\"$\",\"th\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"zen-14B\"}]}],[\"$\",\"th\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"zen-32B\"}]}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"General Tasks\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU\"}],[\"$\",\"td\",null,{\"children\":\"74.3\"}],[\"$\",\"td\",null,{\"children\":\"75.2\"}],[\"$\",\"td\",null,{\"children\":\"77.2\"}],[\"$\",\"td\",null,{\"children\":\"76.5\"}],[\"$\",\"td\",null,{\"children\":\"79.7\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"83.3\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU-pro\"}],[\"$\",\"td\",null,{\"children\":\"44.1\"}],[\"$\",\"td\",null,{\"children\":\"49.1\"}],[\"$\",\"td\",null,{\"children\":\"48.3\"}],[\"$\",\"td\",null,{\"children\":\"43.0\"}],[\"$\",\"td\",null,{\"children\":\"51.2\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"55.1\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU-redux\"}],[\"$\",\"td\",null,{\"children\":\"69.0\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"74.1\"}],[\"$\",\"td\",null,{\"children\":\"72.4\"}],[\"$\",\"td\",null,{\"children\":\"76.6\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"82.0\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"BBH\"}],[\"$\",\"td\",null,{\"children\":\"66.8\"}],[\"$\",\"td\",null,{\"children\":\"74.9\"}],[\"$\",\"td\",null,{\"children\":\"76.4\"}],[\"$\",\"td\",null,{\"children\":\"67.0\"}],[\"$\",\"td\",null,{\"children\":\"78.2\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"84.5\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"ARC-C\"}],[\"$\",\"td\",null,{\"children\":\"63.6\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"71.4\"}]}],[\"$\",\"td\",null,{\"children\":\"65.6\"}],[\"$\",\"td\",null,{\"children\":\"64.1\"}],[\"$\",\"td\",null,{\"children\":\"67.3\"}],[\"$\",\"td\",null,{\"children\":\"70.4\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Truthfulqa\"}],[\"$\",\"td\",null,{\"children\":\"57.4\"}],[\"$\",\"td\",null,{\"children\":\"40.1\"}],[\"$\",\"td\",null,{\"children\":\"53.9\"}],[\"$\",\"td\",null,{\"children\":\"57.7\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"58.4\"}]}],[\"$\",\"td\",null,{\"children\":\"57.8\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Winogrande\"}],[\"$\",\"td\",null,{\"children\":\"81.5\"}],[\"$\",\"td\",null,{\"children\":\"59.7\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"84.9\"}]}],[\"$\",\"td\",null,{\"children\":\"79.5\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"82.0\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Hellaswag\"}],[\"$\",\"td\",null,{\"children\":\"85.0\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"86.4\"}]}],[\"$\",\"td\",null,{\"children\":\"85.9\"}],[\"$\",\"td\",null,{\"children\":\"85.2\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"85.2\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Mathematics \u0026 Science Tasks\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GPQA\"}],[\"$\",\"td\",null,{\"children\":\"30.8\"}],[\"$\",\"td\",null,{\"children\":\"34.9\"}],[\"$\",\"td\",null,{\"children\":\"37.4\"}],[\"$\",\"td\",null,{\"children\":\"34.3\"}],[\"$\",\"td\",null,{\"children\":\"32.8\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"48.0\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Theoremqa\"}],[\"$\",\"td\",null,{\"children\":\"28.8\"}],[\"$\",\"td\",null,{\"children\":\"35.8\"}],[\"$\",\"td\",null,{\"children\":\"40.0\"}],[\"$\",\"td\",null,{\"children\":\"33.5\"}],[\"$\",\"td\",null,{\"children\":\"43.0\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"44.1\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MATH\"}],[\"$\",\"td\",null,{\"children\":\"36.1\"}],[\"$\",\"td\",null,{\"children\":\"42.7\"}],[\"$\",\"td\",null,{\"children\":\"41.7\"}],[\"$\",\"td\",null,{\"children\":\"43.0\"}],[\"$\",\"td\",null,{\"children\":\"55.6\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"57.7\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU-stem\"}],[\"$\",\"td\",null,{\"children\":\"66.5\"}],[\"$\",\"td\",null,{\"children\":\"71.0\"}],[\"$\",\"td\",null,{\"children\":\"72.6\"}],[\"$\",\"td\",null,{\"children\":\"69.8\"}],[\"$\",\"td\",null,{\"children\":\"76.4\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"80.9\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GSM8K\"}],[\"$\",\"td\",null,{\"children\":\"78.5\"}],[\"$\",\"td\",null,{\"children\":\"81.1\"}],[\"$\",\"td\",null,{\"children\":\"81.7\"}],[\"$\",\"td\",null,{\"children\":\"80.7\"}],[\"$\",\"td\",null,{\"children\":\"90.2\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"92.9\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Coding Tasks\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"HumanEval\"}],[\"$\",\"td\",null,{\"children\":\"43.3\"}],[\"$\",\"td\",null,{\"children\":\"54.9\"}],[\"$\",\"td\",null,{\"children\":\"46.3\"}],[\"$\",\"td\",null,{\"children\":\"53.0\"}],[\"$\",\"td\",null,{\"children\":\"56.7\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"58.5\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"HumanEval+\"}],[\"$\",\"td\",null,{\"children\":\"40.2\"}],[\"$\",\"td\",null,{\"children\":\"46.3\"}],[\"$\",\"td\",null,{\"children\":\"40.2\"}],[\"$\",\"td\",null,{\"children\":\"46.3\"}],[\"$\",\"td\",null,{\"children\":\"51.2\"}],\"$L5a\"]}],\"$L5b\",\"$L5c\",\"$L5d\",\"$L5e\",\"$L5f\",\"$L60\",\"$L61\",\"$L62\"]}]]}]}]\n"])</script><script>self.__next_f.push([1,"27:[\"$\",\"p\",null,{\"children\":\"The zen-14B model demonstrates a solid performance across various tasks, particularly excelling in general tasks like MMLU and BBH, where it achieves scores of 79.7 and 78.2, outcompeting competitors of larger sizes. Meanwhile, zen-32B, in particular, showcases exceptional capabilities, often surpassing larger models of similar model sizes. Notably, it outperforms its predecessor Qwen1.5-32B significantly, especially in challenging areas such as mathematics and coding, with notable scores of 57.7 in MATH and 84.5 in MBPP.\"}]\n28:[\"$\",\"h3\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"zen-7b-performance\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#zen-7b-performance\",\"className\":\"peer\",\"children\":\"zen-7B Performance\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"29:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Datasets\"}],[\"$\",\"th\",null,{\"children\":\"Mistral-7B\"}],[\"$\",\"th\",null,{\"children\":\"Llama3-8B\"}],[\"$\",\"th\",null,{\"children\":\"Gemma2-9B\"}],[\"$\",\"th\",null,{\"children\":\"zen-7B\"}],[\"$\",\"th\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"zen-7B\"}]}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"#Non-emb Params\"}],[\"$\",\"td\",null,{\"children\":\"7.0B\"}],[\"$\",\"td\",null,{\"children\":\"7.0B\"}],[\"$\",\"td\",null,{\"children\":\"8.2B\"}],[\"$\",\"td\",null,{\"children\":\"6.5B\"}],[\"$\",\"td\",null,{\"children\":\"6.5B\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"General Tasks\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU\"}],[\"$\",\"td\",null,{\"children\":\"64.2\"}],[\"$\",\"td\",null,{\"children\":\"66.6\"}],[\"$\",\"td\",null,{\"children\":\"71.3\"}],[\"$\",\"td\",null,{\"children\":\"70.3\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"74.2\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU-pro\"}],[\"$\",\"td\",null,{\"children\":\"30.9\"}],[\"$\",\"td\",null,{\"children\":\"35.4\"}],[\"$\",\"td\",null,{\"children\":\"44.7\"}],[\"$\",\"td\",null,{\"children\":\"40.1\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"45.0\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU-redux\"}],[\"$\",\"td\",null,{\"children\":\"58.1\"}],[\"$\",\"td\",null,{\"children\":\"61.6\"}],[\"$\",\"td\",null,{\"children\":\"67.9\"}],[\"$\",\"td\",null,{\"children\":\"68.1\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"71.1\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"BBH\"}],[\"$\",\"td\",null,{\"children\":\"56.1\"}],[\"$\",\"td\",null,{\"children\":\"57.7\"}],[\"$\",\"td\",null,{\"children\":\"68.2\"}],[\"$\",\"td\",null,{\"children\":\"62.3\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"70.4\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"ARC-C\"}],[\"$\",\"td\",null,{\"children\":\"60.0\"}],[\"$\",\"td\",null,{\"children\":\"59.3\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"68.2\"}]}],[\"$\",\"td\",null,{\"children\":\"60.6\"}],[\"$\",\"td\",null,{\"children\":\"63.7\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Trurhfulqa\"}],[\"$\",\"td\",null,{\"children\":\"42.2\"}],[\"$\",\"td\",null,{\"children\":\"44.0\"}],[\"$\",\"td\",null,{\"children\":\"45.3\"}],[\"$\",\"td\",null,{\"children\":\"54.2\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"56.4\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Winogrande\"}],[\"$\",\"td\",null,{\"children\":\"78.4\"}],[\"$\",\"td\",null,{\"children\":\"77.4\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"79.5\"}]}],[\"$\",\"td\",null,{\"children\":\"77.0\"}],[\"$\",\"td\",null,{\"children\":\"75.9\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Hellaswag\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"83.3\"}]}],[\"$\",\"td\",null,{\"children\":\"82.1\"}],[\"$\",\"td\",null,{\"children\":\"81.9\"}],[\"$\",\"td\",null,{\"children\":\"80.7\"}],[\"$\",\"td\",null,{\"children\":\"80.2\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Mathematics \u0026 Science Tasks\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GPQA\"}],[\"$\",\"td\",null,{\"children\":\"24.7\"}],[\"$\",\"td\",null,{\"children\":\"25.8\"}],[\"$\",\"td\",null,{\"children\":\"32.8\"}],[\"$\",\"td\",null,{\"children\":\"30.8\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"36.4\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Theoremqa\"}],[\"$\",\"td\",null,{\"children\":\"19.2\"}],[\"$\",\"td\",null,{\"children\":\"22.1\"}],[\"$\",\"td\",null,{\"children\":\"28.9\"}],[\"$\",\"td\",null,{\"children\":\"29.6\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"36.0\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MATH\"}],[\"$\",\"td\",null,{\"children\":\"10.2\"}],[\"$\",\"td\",null,{\"children\":\"20.5\"}],[\"$\",\"td\",null,{\"children\":\"37.7\"}],[\"$\",\"td\",null,{\"children\":\"43.5\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"49.8\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU-stem\"}],[\"$\",\"td\",null,{\"children\":\"50.1\"}],[\"$\",\"td\",null,{\"children\":\"55.3\"}],[\"$\",\"td\",null,{\"children\":\"65.1\"}],[\"$\",\"td\",null,{\"children\":\"64.2\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"72.3\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GSM8K\"}],[\"$\",\"td\",null,{\"children\":\"36.2\"}],[\"$\",\"td\",null,{\"children\":\"55.3\"}],[\"$\",\"td\",null,{\"children\":\"70.7\"}],[\"$\",\"td\",null,{\"children\":\"80.2\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"85.4\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Coding Tasks\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"HumanEval\"}],[\"$\",\"td\",null,{\"children\":\"29.3\"}],[\"$\",\"td\",null,{\"children\":\"33.5\"}],[\"$\",\"td\",null,{\"children\":\"37.8\"}],[\"$\",\"td\",null,{\"children\":\"51.2\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"57.9\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"HumanEval+\"}],[\"$\",\"td\",null,{\"children\":\"24.4\"}],[\"$\",\"td\",null,{\"children\":\"29.3\"}],[\"$\",\"td\",null,{\"children\":\"30.5\"}],[\"$\",\"td\",null,{\"children\":\"43.3\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"50.6\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MBPP\"}],[\"$\",\"td\",null,{\"children\":\"51.1\"}],[\"$\",\"td\",null,{\"children\":\"53.9\"}],[\"$\",\"td\",null,{\"children\":\"62.2\"}],[\"$\",\"td\",null,{\"children\":\"64.2\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"74.9\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[\"$L63\",\"$L64\",\"$L65\",\"$L66\",\"$L67\",\"$L68\"]}],\"$L69\",\"$L6a\",\"$L6b\",\"$L6c\",\"$L6d\",\"$L6e\"]}]]}]}]\n"])</script><script>self.__next_f.push([1,"2a:[\"$\",\"p\",null,{\"children\":\"The zen-7B model surpasses its predecessors and counterparts in numerous benchmarks, despite having fewer non-embedding parameters. It demonstrates significant improvements across various tasks, achieving 74.2 on general benchmarks like MMLU, 49.8 on math challenges such as MATH, and 57.9 on coding tasks like HumanEval.\"}]\n2b:[\"$\",\"h3\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"zen-05b15b3b-performance\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#zen-05b15b3b-performance\",\"className\":\"peer\",\"children\":\"zen-0.5B/1.5B/3B Performance\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"2c:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Datasets\"}],[\"$\",\"th\",null,{\"children\":\"zen-0.5B\"}],[\"$\",\"th\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"zen-0.5B\"}]}],[\"$\",\"th\",null,{\"children\":\"zen-1.5B\"}],[\"$\",\"th\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"zen-1.5B\"}]}],[\"$\",\"th\",null,{\"children\":\"Gemma2-2.6B\"}],[\"$\",\"th\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"zen-3B\"}]}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"General Tasks\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU\"}],[\"$\",\"td\",null,{\"children\":\"44.3\"}],[\"$\",\"td\",null,{\"children\":\"47.5\"}],[\"$\",\"td\",null,{\"children\":\"55.9\"}],[\"$\",\"td\",null,{\"children\":\"60.9\"}],[\"$\",\"td\",null,{\"children\":\"52.2\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"65.6\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU-pro\"}],[\"$\",\"td\",null,{\"children\":\"14.7\"}],[\"$\",\"td\",null,{\"children\":\"15.7\"}],[\"$\",\"td\",null,{\"children\":\"21.6\"}],[\"$\",\"td\",null,{\"children\":\"28.5\"}],[\"$\",\"td\",null,{\"children\":\"23.0\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"34.6\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU-redux\"}],[\"$\",\"td\",null,{\"children\":\"40.7\"}],[\"$\",\"td\",null,{\"children\":\"45.1\"}],[\"$\",\"td\",null,{\"children\":\"51.8\"}],[\"$\",\"td\",null,{\"children\":\"58.5\"}],[\"$\",\"td\",null,{\"children\":\"50.9\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"63.7\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"BBH\"}],[\"$\",\"td\",null,{\"children\":\"18.2\"}],[\"$\",\"td\",null,{\"children\":\"20.3\"}],[\"$\",\"td\",null,{\"children\":\"36.5\"}],[\"$\",\"td\",null,{\"children\":\"45.1\"}],[\"$\",\"td\",null,{\"children\":\"41.9\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"56.3\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"ARC-C\"}],[\"$\",\"td\",null,{\"children\":\"31.0\"}],[\"$\",\"td\",null,{\"children\":\"35.6\"}],[\"$\",\"td\",null,{\"children\":\"43.7\"}],[\"$\",\"td\",null,{\"children\":\"54.7\"}],[\"$\",\"td\",null,{\"children\":\"55.7\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"56.5\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Trurhfulqa\"}],[\"$\",\"td\",null,{\"children\":\"39.7\"}],[\"$\",\"td\",null,{\"children\":\"40.2\"}],[\"$\",\"td\",null,{\"children\":\"45.9\"}],[\"$\",\"td\",null,{\"children\":\"46.6\"}],[\"$\",\"td\",null,{\"children\":\"36.2\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"48.9\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Winogrande\"}],[\"$\",\"td\",null,{\"children\":\"56.9\"}],[\"$\",\"td\",null,{\"children\":\"56.3\"}],[\"$\",\"td\",null,{\"children\":\"65.0\"}],[\"$\",\"td\",null,{\"children\":\"65.0\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"71.5\"}]}],[\"$\",\"td\",null,{\"children\":\"71.1\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Hellaswag\"}],[\"$\",\"td\",null,{\"children\":\"49.1\"}],[\"$\",\"td\",null,{\"children\":\"52.1\"}],[\"$\",\"td\",null,{\"children\":\"67.0\"}],[\"$\",\"td\",null,{\"children\":\"67.9\"}],[\"$\",\"td\",null,{\"children\":\"74.6\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"74.6\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Mathematics \u0026 Science Tasks\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GPQA\"}],[\"$\",\"td\",null,{\"children\":\"29.8\"}],[\"$\",\"td\",null,{\"children\":\"24.8\"}],[\"$\",\"td\",null,{\"children\":\"20.7\"}],[\"$\",\"td\",null,{\"children\":\"24.2\"}],[\"$\",\"td\",null,{\"children\":\"25.3\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"26.3\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Theoremqa\"}],[\"$\",\"td\",null,{\"children\":\"9.6\"}],[\"$\",\"td\",null,{\"children\":\"16.0\"}],[\"$\",\"td\",null,{\"children\":\"14.8\"}],[\"$\",\"td\",null,{\"children\":\"22.1\"}],[\"$\",\"td\",null,{\"children\":\"15.9\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"27.4\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MATH\"}],[\"$\",\"td\",null,{\"children\":\"11.2\"}],[\"$\",\"td\",null,{\"children\":\"19.5\"}],[\"$\",\"td\",null,{\"children\":\"21.6\"}],[\"$\",\"td\",null,{\"children\":\"35.0\"}],[\"$\",\"td\",null,{\"children\":\"18.3\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"42.6\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU-stem\"}],[\"$\",\"td\",null,{\"children\":\"27.5\"}],[\"$\",\"td\",null,{\"children\":\"39.8\"}],[\"$\",\"td\",null,{\"children\":\"42.7\"}],[\"$\",\"td\",null,{\"children\":\"54.8\"}],[\"$\",\"td\",null,{\"children\":\"45.8\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"62.5\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GSM8K\"}],[\"$\",\"td\",null,{\"children\":\"36.4\"}],[\"$\",\"td\",null,{\"children\":\"41.6\"}],[\"$\",\"td\",null,{\"children\":\"46.9\"}],[\"$\",\"td\",null,{\"children\":\"68.5\"}],[\"$\",\"td\",null,{\"children\":\"30.3\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"79.1\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Coding Tasks\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"HumanEval\"}],[\"$\",\"td\",null,{\"children\":\"22.6\"}],[\"$\",\"td\",null,{\"children\":\"30.5\"}],[\"$\",\"td\",null,{\"children\":\"34.8\"}],[\"$\",\"td\",null,{\"children\":\"37.2\"}],[\"$\",\"td\",null,{\"children\":\"19.5\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"42.1\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"HumanEval+\"}],[\"$\",\"td\",null,{\"children\":\"18.9\"}],[\"$\",\"td\",null,{\"children\":\"26.8\"}],[\"$\",\"td\",null,{\"children\":\"29.9\"}],[\"$\",\"td\",null,{\"children\":\"32.9\"}],\"$L6f\",\"$L70\"]}],\"$L71\",\"$L72\",\"$L73\",\"$L74\",\"$L75\",\"$L76\",\"$L77\",\"$L78\"]}]]}]}]\n"])</script><script>self.__next_f.push([1,"2d:[\"$\",\"p\",null,{\"children\":\"For edge-side models, zen-0.5B, 1.5B, and 3B continue to maintain strong performance across nearly all benchmarks. Notably, the zen-0.5B model outperforms the Gemma2-2.6B on various math and coding tasks.\"}]\n2e:[\"$\",\"h2\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"instruction-tuned-model-evaluation\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#instruction-tuned-model-evaluation\",\"className\":\"peer\",\"children\":\"Instruction-tuned Model Evaluation\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n2f:[\"$\",\"p\",null,{\"children\":\"The evaluation of instruction-tuned models mainly focuses on the model performance of natural language understanding, general question answering, reasoning, coding, mathematics, instruction following, human alignment, etc.\"}]\n30:[\"$\",\"p\",null,{\"children\":\"The datasets for evaluation include:\"}]\n31:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"General Tasks\"}],\" : MMLU-Pro, MMLU-redux\"]}]\n32:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Math \u0026 Science Tasks\"}],\": GPQA, GSM8K, MATH\"]}]\n33:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Coding Tasks\"}],\" : HumanEval, MBPP, MultiPL-E, LiveCodeBench 2305-2409, LiveBench 0831\"]}]\n34:[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Instruction \u0026 Alignment Tasks\"}],\": IFeval strict-prompt, Arena-Hard, AlignBench v1.1, MTbench\"]}]\n35:[\"$\",\"h3\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"zen-72b-instruct-performance\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#zen-72b-instruct-performance\",\"className\":\"peer\",\"children\":\"zen-72B-Instruct Performance\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"36:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Datasets\"}],[\"$\",\"th\",null,{\"children\":\"Mistral-Large2 Instruct\"}],[\"$\",\"th\",null,{\"children\":\"Llama-3.1-70B-Instruct\"}],[\"$\",\"th\",null,{\"children\":\"Llama-3.1-405B-Instruct\"}],[\"$\",\"th\",null,{\"children\":\"zen-72B-Instruct\"}],[\"$\",\"th\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"zen-72B-Instruct\"}]}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU-Pro\"}],[\"$\",\"td\",null,{\"children\":\"69.4\"}],[\"$\",\"td\",null,{\"children\":\"66.4\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"73.3\"}]}],[\"$\",\"td\",null,{\"children\":\"64.4\"}],[\"$\",\"td\",null,{\"children\":\"71.1\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU-redux\"}],[\"$\",\"td\",null,{\"children\":\"83.0\"}],[\"$\",\"td\",null,{\"children\":\"83.0\"}],[\"$\",\"td\",null,{\"children\":\"86.2\"}],[\"$\",\"td\",null,{\"children\":\"81.6\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"86.8\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GPQA\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"52.0\"}]}],[\"$\",\"td\",null,{\"children\":\"46.7\"}],[\"$\",\"td\",null,{\"children\":\"51.1\"}],[\"$\",\"td\",null,{\"children\":\"42.4\"}],[\"$\",\"td\",null,{\"children\":\"49.0\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MATH\"}],[\"$\",\"td\",null,{\"children\":\"69.9\"}],[\"$\",\"td\",null,{\"children\":\"68.0\"}],[\"$\",\"td\",null,{\"children\":\"73.8\"}],[\"$\",\"td\",null,{\"children\":\"69.0\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"83.1\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GSM8K\"}],[\"$\",\"td\",null,{\"children\":\"92.7\"}],[\"$\",\"td\",null,{\"children\":\"95.1\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"96.8\"}]}],[\"$\",\"td\",null,{\"children\":\"93.2\"}],[\"$\",\"td\",null,{\"children\":\"95.8\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"HumanEval\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"92.1\"}]}],[\"$\",\"td\",null,{\"children\":\"80.5\"}],[\"$\",\"td\",null,{\"children\":\"89.0\"}],[\"$\",\"td\",null,{\"children\":\"86.0\"}],[\"$\",\"td\",null,{\"children\":\"86.6\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MBPP\"}],[\"$\",\"td\",null,{\"children\":\"80.0\"}],[\"$\",\"td\",null,{\"children\":\"84.2\"}],[\"$\",\"td\",null,{\"children\":\"84.5\"}],[\"$\",\"td\",null,{\"children\":\"80.2\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"88.2\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MultiPL-E\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"76.9\"}]}],[\"$\",\"td\",null,{\"children\":\"68.2\"}],[\"$\",\"td\",null,{\"children\":\"73.5\"}],[\"$\",\"td\",null,{\"children\":\"69.2\"}],[\"$\",\"td\",null,{\"children\":\"75.1\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"LiveCodeBench 2305-2409\"}],[\"$\",\"td\",null,{\"children\":\"42.2\"}],[\"$\",\"td\",null,{\"children\":\"32.1\"}],[\"$\",\"td\",null,{\"children\":\"41.6\"}],[\"$\",\"td\",null,{\"children\":\"32.2\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"55.5\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"LiveBench 0831\"}],[\"$\",\"td\",null,{\"children\":\"48.5\"}],[\"$\",\"td\",null,{\"children\":\"46.6\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"53.2\"}]}],[\"$\",\"td\",null,{\"children\":\"41.5\"}],[\"$\",\"td\",null,{\"children\":\"52.3\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"IFeval strict-prompt\"}],[\"$\",\"td\",null,{\"children\":\"64.1\"}],[\"$\",\"td\",null,{\"children\":\"83.6\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"86.0\"}]}],[\"$\",\"td\",null,{\"children\":\"77.6\"}],[\"$\",\"td\",null,{\"children\":\"84.1\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Arena-Hard\"}],[\"$\",\"td\",null,{\"children\":\"73.1\"}],[\"$\",\"td\",null,{\"children\":\"55.7\"}],[\"$\",\"td\",null,{\"children\":\"69.3\"}],[\"$\",\"td\",null,{\"children\":\"48.1\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"81.2\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"AlignBench v1.1\"}],[\"$\",\"td\",null,{\"children\":\"7.69\"}],[\"$\",\"td\",null,{\"children\":\"5.94\"}],[\"$\",\"td\",null,{\"children\":\"5.95\"}],[\"$\",\"td\",null,{\"children\":\"8.15\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"8.16\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MTbench\"}],[\"$\",\"td\",null,{\"children\":\"8.61\"}],[\"$\",\"td\",null,{\"children\":\"8.79\"}],[\"$\",\"td\",null,{\"children\":\"9.08\"}],[\"$\",\"td\",null,{\"children\":\"9.12\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"9.35\"}]}]]}]]}]]}]}]\n"])</script><script>self.__next_f.push([1,"37:[\"$\",\"p\",null,{\"children\":\"The zen-72B-Instruct model delivers exceptional performance, even surpassing the larger Llama-3.1-405B in several critical tasks. zen-72B-Instruct excels in mathematics (MATH: 83.1), coding (LiveCodeBench: 55.5), and chatting (Arena-Hard: 81.2). Compared to its base model zen-72B and its predecessor zen-72B-Instruct, the zen-72B-Instruct showcases comprehensive improvements across all tasks.\"}]\n38:[\"$\",\"h3\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"qwen-turbo--zen-14b-instruct--zen-32b-instruct-performance\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#qwen-turbo--zen-14b-instruct--zen-32b-instruct-performance\",\"className\":\"peer\",\"children\":\"Qwen-Turbo \u0026 zen-14B-Instruct \u0026 zen-32B-Instruct Performance\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"39:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Datasets\"}],[\"$\",\"th\",null,{\"children\":\"zen7B-A14B-Instruct\"}],[\"$\",\"th\",null,{\"children\":\"Gemma2-27B-IT\"}],[\"$\",\"th\",null,{\"children\":\"GPT4o-mini\"}],[\"$\",\"th\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Qwen-Turbo\"}]}],[\"$\",\"th\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"zen-14B-Instruct\"}]}],[\"$\",\"th\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"zen-32B-Instruct\"}]}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU-Pro\"}],[\"$\",\"td\",null,{\"children\":\"52.8\"}],[\"$\",\"td\",null,{\"children\":\"55.5\"}],[\"$\",\"td\",null,{\"children\":\"63.1\"}],[\"$\",\"td\",null,{\"children\":\"64.8\"}],[\"$\",\"td\",null,{\"children\":\"63.7\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"69.0\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU-redux\"}],[\"$\",\"td\",null,{\"children\":\"72.6\"}],[\"$\",\"td\",null,{\"children\":\"75.7\"}],[\"$\",\"td\",null,{\"children\":\"81.5\"}],[\"$\",\"td\",null,{\"children\":\"80.4\"}],[\"$\",\"td\",null,{\"children\":\"80.0\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"83.9\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GPQA\"}],[\"$\",\"td\",null,{\"children\":\"34.3\"}],[\"$\",\"td\",null,{\"children\":\"38.4\"}],[\"$\",\"td\",null,{\"children\":\"40.2\"}],[\"$\",\"td\",null,{\"children\":\"44.4\"}],[\"$\",\"td\",null,{\"children\":\"45.5\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"49.5\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MATH\"}],[\"$\",\"td\",null,{\"children\":\"49.1\"}],[\"$\",\"td\",null,{\"children\":\"54.4\"}],[\"$\",\"td\",null,{\"children\":\"70.2\"}],[\"$\",\"td\",null,{\"children\":\"81.0\"}],[\"$\",\"td\",null,{\"children\":\"80.0\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"83.1\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GSM8K\"}],[\"$\",\"td\",null,{\"children\":\"85.3\"}],[\"$\",\"td\",null,{\"children\":\"90.4\"}],[\"$\",\"td\",null,{\"children\":\"93.2\"}],[\"$\",\"td\",null,{\"children\":\"93.6\"}],[\"$\",\"td\",null,{\"children\":\"94.8\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"95.9\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"HumanEval\"}],[\"$\",\"td\",null,{\"children\":\"79.9\"}],[\"$\",\"td\",null,{\"children\":\"78.7\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"88.4\"}]}],[\"$\",\"td\",null,{\"children\":\"86.6\"}],[\"$\",\"td\",null,{\"children\":\"83.5\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"88.4\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MBPP\"}],[\"$\",\"td\",null,{\"children\":\"70.9\"}],[\"$\",\"td\",null,{\"children\":\"81.0\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"85.7\"}]}],[\"$\",\"td\",null,{\"children\":\"80.2\"}],[\"$\",\"td\",null,{\"children\":\"82.0\"}],[\"$\",\"td\",null,{\"children\":\"84.0\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MultiPL-E\"}],[\"$\",\"td\",null,{\"children\":\"66.4\"}],[\"$\",\"td\",null,{\"children\":\"67.4\"}],[\"$\",\"td\",null,{\"children\":\"75.0\"}],[\"$\",\"td\",null,{\"children\":\"73.0\"}],[\"$\",\"td\",null,{\"children\":\"72.8\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"75.4\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"LiveCodeBench 2305-2409\"}],[\"$\",\"td\",null,{\"children\":\"22.5\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"40.7\"}],[\"$\",\"td\",null,{\"children\":\"43.1\"}],[\"$\",\"td\",null,{\"children\":\"42.6\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"51.2\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"LiveBench 0831\"}],[\"$\",\"td\",null,{\"children\":\"31.1\"}],[\"$\",\"td\",null,{\"children\":\"39.6\"}],[\"$\",\"td\",null,{\"children\":\"43.3\"}],[\"$\",\"td\",null,{\"children\":\"41.6\"}],[\"$\",\"td\",null,{\"children\":\"44.4\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"50.7\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"IFeval strict-prompt\"}],[\"$\",\"td\",null,{\"children\":\"59.9\"}],[\"$\",\"td\",null,{\"children\":\"77.1\"}],[\"$\",\"td\",null,{\"children\":\"80.4\"}],[\"$\",\"td\",null,{\"children\":\"74.9\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"81.0\"}]}],[\"$\",\"td\",null,{\"children\":\"79.5\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Arena-Hard\"}],[\"$\",\"td\",null,{\"children\":\"17.8\"}],[\"$\",\"td\",null,{\"children\":\"57.5\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"74.9\"}]}],[\"$\",\"td\",null,{\"children\":\"68.4\"}],[\"$\",\"td\",null,{\"children\":\"68.3\"}],[\"$\",\"td\",null,{\"children\":\"74.5\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"AlignBench v1.1\"}],[\"$\",\"td\",null,{\"children\":\"7.02\"}],[\"$\",\"td\",null,{\"children\":\"7.22\"}],[\"$\",\"td\",null,{\"children\":\"7.81\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"7.99\"}]}],[\"$\",\"td\",null,{\"children\":\"7.94\"}],[\"$\",\"td\",null,{\"children\":\"7.93\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MTbench\"}],[\"$\",\"td\",null,{\"children\":\"8.55\"}],[\"$\",\"td\",null,{\"children\":\"9.10\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"8.86\"}],[\"$\",\"td\",null,{\"children\":\"8.88\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"9.20\"}]}]]}]]}]]}]}]\n"])</script><script>self.__next_f.push([1,"3a:[\"$\",\"p\",null,{\"children\":\"The zen-32B-Instruct model demonstrates superior performance across most tasks when compared to other models of similar size. In comparison to GPT-4o-mini, our open-source model, zen-14B-Instruct, along with our API model, Qwen-Turbo, also deliver competitive results across all benchmarks.\"}]\n3b:[\"$\",\"h3\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"zen-7b-instruct-performance\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#zen-7b-instruct-performance\",\"className\":\"peer\",\"children\":\"zen-7B-Instruct Performance\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"3c:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Datasets\"}],[\"$\",\"th\",null,{\"children\":\"Gemma2-9b-IT\"}],[\"$\",\"th\",null,{\"children\":\"Llama3.1-8B-Instruct\"}],[\"$\",\"th\",null,{\"children\":\"zen-7B-Instruct\"}],[\"$\",\"th\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"zen-7B-Instruct\"}]}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU-Pro\"}],[\"$\",\"td\",null,{\"children\":\"52.1\"}],[\"$\",\"td\",null,{\"children\":\"48.3\"}],[\"$\",\"td\",null,{\"children\":\"44.1\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"56.3\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU-redux\"}],[\"$\",\"td\",null,{\"children\":\"72.8\"}],[\"$\",\"td\",null,{\"children\":\"67.2\"}],[\"$\",\"td\",null,{\"children\":\"67.3\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"75.4\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GPQA\"}],[\"$\",\"td\",null,{\"children\":\"32.8\"}],[\"$\",\"td\",null,{\"children\":\"32.8\"}],[\"$\",\"td\",null,{\"children\":\"34.3\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"36.4\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MATH\"}],[\"$\",\"td\",null,{\"children\":\"44.3\"}],[\"$\",\"td\",null,{\"children\":\"51.9\"}],[\"$\",\"td\",null,{\"children\":\"52.9\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"75.5\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GSM8K\"}],[\"$\",\"td\",null,{\"children\":\"76.7\"}],[\"$\",\"td\",null,{\"children\":\"84.5\"}],[\"$\",\"td\",null,{\"children\":\"85.7\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"91.6\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"HumanEval\"}],[\"$\",\"td\",null,{\"children\":\"68.9\"}],[\"$\",\"td\",null,{\"children\":\"72.6\"}],[\"$\",\"td\",null,{\"children\":\"79.9\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"84.8\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MBPP\"}],[\"$\",\"td\",null,{\"children\":\"74.9\"}],[\"$\",\"td\",null,{\"children\":\"69.6\"}],[\"$\",\"td\",null,{\"children\":\"67.2\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"79.2\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MultiPL-E\"}],[\"$\",\"td\",null,{\"children\":\"53.4\"}],[\"$\",\"td\",null,{\"children\":\"50.7\"}],[\"$\",\"td\",null,{\"children\":\"59.1\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"70.4\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"LiveCodeBench 2305-2409\"}],[\"$\",\"td\",null,{\"children\":\"18.9\"}],[\"$\",\"td\",null,{\"children\":\"8.3\"}],[\"$\",\"td\",null,{\"children\":\"23.9\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"28.7\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"LiveBench 0831\"}],[\"$\",\"td\",null,{\"children\":\"30.6\"}],[\"$\",\"td\",null,{\"children\":\"26.7\"}],[\"$\",\"td\",null,{\"children\":\"29.2\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"35.9\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"IFeval strict-prompt\"}],[\"$\",\"td\",null,{\"children\":\"70.1\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"75.9\"}]}],[\"$\",\"td\",null,{\"children\":\"54.7\"}],[\"$\",\"td\",null,{\"children\":\"71.2\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Arena-Hard\"}],[\"$\",\"td\",null,{\"children\":\"41.6\"}],[\"$\",\"td\",null,{\"children\":\"27.8\"}],[\"$\",\"td\",null,{\"children\":\"25.0\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"52.0\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"AlignBench v1.1\"}],[\"$\",\"td\",null,{\"children\":\"7.05\"}],[\"$\",\"td\",null,{\"children\":\"4.75\"}],[\"$\",\"td\",null,{\"children\":\"7.13\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"7.33\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MTbench\"}],[\"$\",\"td\",null,{\"children\":\"8.49\"}],[\"$\",\"td\",null,{\"children\":\"8.23\"}],[\"$\",\"td\",null,{\"children\":\"8.26\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"8.75\"}]}]]}]]}]]}]}]\n"])</script><script>self.__next_f.push([1,"3d:[\"$\",\"p\",null,{\"children\":\"The zen-7B-Instruct model significantly outperforms its competitors, Gemma2-9b-IT and Llama3.1-8B-Instruct, across all tasks except IFeval. Notably, zen-7B-Instruct demonstrates clear advantages in mathematics (MATH: 75.5) and coding (HumanEval: 84.8).\"}]\n3e:[\"$\",\"h3\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"zen-3b-instruct-performance\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#zen-3b-instruct-performance\",\"className\":\"peer\",\"children\":\"zen-3B-Instruct Performance\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"3f:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Datasets\"}],[\"$\",\"th\",null,{\"children\":\"Gemma2-2B-IT\"}],[\"$\",\"th\",null,{\"children\":\"Phi3.5-mini-Instruct\"}],[\"$\",\"th\",null,{\"children\":\"MiniCPM3-4B\"}],[\"$\",\"th\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"zen-3B-Instruct\"}]}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Non-Emb Params\"}],[\"$\",\"td\",null,{\"children\":\"2.0B\"}],[\"$\",\"td\",null,{\"children\":\"3.6B\"}],[\"$\",\"td\",null,{\"children\":\"4.0B\"}],[\"$\",\"td\",null,{\"children\":\"2.8B\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU-Pro\"}],[\"$\",\"td\",null,{\"children\":\"26.7\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"47.5\"}]}],[\"$\",\"td\",null,{\"children\":\"43.0\"}],[\"$\",\"td\",null,{\"children\":\"43.7\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU-redux\"}],[\"$\",\"td\",null,{\"children\":\"51.9\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"67.7\"}]}],[\"$\",\"td\",null,{\"children\":\"59.9\"}],[\"$\",\"td\",null,{\"children\":\"64.4\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GPQA\"}],[\"$\",\"td\",null,{\"children\":\"29.3\"}],[\"$\",\"td\",null,{\"children\":\"27.2\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"31.3\"}]}],[\"$\",\"td\",null,{\"children\":\"30.3\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MATH\"}],[\"$\",\"td\",null,{\"children\":\"26.6\"}],[\"$\",\"td\",null,{\"children\":\"48.5\"}],[\"$\",\"td\",null,{\"children\":\"46.6\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"65.9\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GSM8K\"}],[\"$\",\"td\",null,{\"children\":\"63.2\"}],[\"$\",\"td\",null,{\"children\":\"86.2\"}],[\"$\",\"td\",null,{\"children\":\"81.1\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"86.7\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"HumanEval\"}],[\"$\",\"td\",null,{\"children\":\"68.9\"}],[\"$\",\"td\",null,{\"children\":\"72.6\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"74.4\"}]}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"74.4\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MBPP\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"74.9\"}]}],[\"$\",\"td\",null,{\"children\":\"63.2\"}],[\"$\",\"td\",null,{\"children\":\"72.5\"}],[\"$\",\"td\",null,{\"children\":\"72.7\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MultiPL-E\"}],[\"$\",\"td\",null,{\"children\":\"30.5\"}],[\"$\",\"td\",null,{\"children\":\"47.2\"}],[\"$\",\"td\",null,{\"children\":\"49.1\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"60.2\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"LiveCodeBench 2305-2409\"}],[\"$\",\"td\",null,{\"children\":\"5.8\"}],[\"$\",\"td\",null,{\"children\":\"15.8\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"23.8\"}]}],[\"$\",\"td\",null,{\"children\":\"19.9\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"LiveBench 0831\"}],[\"$\",\"td\",null,{\"children\":\"20.1\"}],[\"$\",\"td\",null,{\"children\":\"27.4\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"27.6\"}]}],[\"$\",\"td\",null,{\"children\":\"26.8\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"IFeval strict-prompt\"}],[\"$\",\"td\",null,{\"children\":\"51.0\"}],[\"$\",\"td\",null,{\"children\":\"52.1\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"68.4\"}]}],[\"$\",\"td\",null,{\"children\":\"58.2\"}]]}]]}]]}]}]\n"])</script><script>self.__next_f.push([1,"40:[\"$\",\"p\",null,{\"children\":\"As for the edge-side instruction model, the zen-3B-Instruct model has fewer parameters than both the Phi3.5-mini-Instruct and MiniCPM3-4B models. Despite this, it outperforms them in mathematics and coding tasks while delivering competitive results in language understanding.\"}]\n41:[\"$\",\"h3\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"zen-05b15b-instruct-performance\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#zen-05b15b-instruct-performance\",\"className\":\"peer\",\"children\":\"zen-0.5B/1.5B-Instruct Performance\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n"])</script><script>self.__next_f.push([1,"42:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Datasets\"}],[\"$\",\"th\",null,{\"children\":\"zen-0.5B-Instruct\"}],[\"$\",\"th\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"zen-0.5B-Instruct\"}]}],[\"$\",\"th\",null,{\"children\":\"zen-1.5B-Instruct\"}],[\"$\",\"th\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"zen-1.5B-Instruct\"}]}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU-Pro\"}],[\"$\",\"td\",null,{\"children\":\"14.4\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"15.0\"}]}],[\"$\",\"td\",null,{\"children\":\"22.9\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"32.4\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MMLU-redux\"}],[\"$\",\"td\",null,{\"children\":\"12.9\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"24.1\"}]}],[\"$\",\"td\",null,{\"children\":\"41.2\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"50.7\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GPQA\"}],[\"$\",\"td\",null,{\"children\":\"23.7\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"29.8\"}]}],[\"$\",\"td\",null,{\"children\":\"21.2\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"29.8\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MATH\"}],[\"$\",\"td\",null,{\"children\":\"13.9\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"34.4\"}]}],[\"$\",\"td\",null,{\"children\":\"25.3\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"55.2\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"GSM8K\"}],[\"$\",\"td\",null,{\"children\":\"40.1\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"49.6\"}]}],[\"$\",\"td\",null,{\"children\":\"61.6\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"73.2\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"HumanEval\"}],[\"$\",\"td\",null,{\"children\":\"31.1\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"35.4\"}]}],[\"$\",\"td\",null,{\"children\":\"42.1\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"61.6\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MBPP\"}],[\"$\",\"td\",null,{\"children\":\"39.7\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"49.6\"}]}],[\"$\",\"td\",null,{\"children\":\"44.2\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"63.2\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MultiPL-E\"}],[\"$\",\"td\",null,{\"children\":\"20.8\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"28.5\"}]}],[\"$\",\"td\",null,{\"children\":\"38.5\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"50.4\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"LiveCodeBench 2305-2409\"}],[\"$\",\"td\",null,{\"children\":\"1.6\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"5.1\"}]}],[\"$\",\"td\",null,{\"children\":\"4.5\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"14.8\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"LiveBench 0831\"}],[\"$\",\"td\",null,{\"children\":\"7.4\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"12.6\"}]}],[\"$\",\"td\",null,{\"children\":\"12.4\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"18.8\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"IFeval strict-prompt\"}],[\"$\",\"td\",null,{\"children\":\"14.6\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"27.9\"}]}],[\"$\",\"td\",null,{\"children\":\"29.0\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"42.5\"}]}]]}]]}]]}]}]\n"])</script><script>self.__next_f.push([1,"43:[\"$\",\"p\",null,{\"children\":\"zen-1.5B-Instruct and zen-0.5B-Instruct have seen large performance improvements over their previous versions, making them well-suited for edge-side applications in highly resource-constrained environments.\"}]\n44:[\"$\",\"h3\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"performances-on-multilingualism\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#performances-on-multilingualism\",\"className\":\"peer\",\"children\":\"Performances on Multilingualism\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n45:[\"$\",\"p\",null,{\"children\":\"To evaluate the multilingual performance of instruction-tuned models, we collect and extend benchmarks as follows:\"}]\n"])</script><script>self.__next_f.push([1,"46:[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"IFEval (multilingual)\"}],\" : We translate the examples from IFEval (English version) to construct multilingual IFEval examples after removing examples with language-specific contents (e.g., “start with letter A”). We collect 100 examples for each language among Arabic (ar), Spanish (es), French (fr), Indonesian (in), Japanese (ja), Korean (ko), Portuguese (pt), and Vietnamese (vi) languages. All examples are checked and post-edited (if neccessary) by paid volunteers.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Knowledge\"}],\" : We use 5 MMLU-like benchmarks (multi-choice) to testify the knowledge utilization ability of zen series models on multilingualism, including AMMLU (Arabic), JMMLU (Japanese), KMMLU (Korean), IndoMMLU (Indonesian), and TurkishMMLU (Turkish). Also, we present the performances on translated MMLU (i.e., okapi_MMLU, from English to multiple languages).\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"MGSM8K (extended)\"}],\" : Aside from the examples in the original MGSM8K benchmark, we extend the language support with Arabic (ar), Korean (ko), Portuguese (pt), and Vietnamese (vi). We translate 250 examples (same as the other languages engaged in MGSM8K) into those 4 languages. All examples are also checked and post-edited (if necessary) by paid volunteers.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Cultural Nuances\"}],\" : We also use BLEnD, a benchmark aiming at testifying cultural nuances of LLMs, to testify LLMs from the zen series.\"]}],\"\\n\"]}]\n"])</script><script>self.__next_f.push([1,"47:[\"$\",\"div\",null,{\"className\":\"relative overflow-auto prose-no-margin my-6\",\"children\":[\"$\",\"table\",null,{\"children\":[[\"$\",\"thead\",null,{\"children\":[\"$\",\"tr\",null,{\"children\":[[\"$\",\"th\",null,{\"children\":\"Datasets\"}],[\"$\",\"th\",null,{\"children\":\"zen-72B-Instruct\"}],[\"$\",\"th\",null,{\"children\":\"Llama3.1-70B-Instruct\"}],[\"$\",\"th\",null,{\"children\":\"zen-32B-Instruct\"}],[\"$\",\"th\",null,{\"children\":\"Mistral-Large-Instruct-2407 (123B)\"}],[\"$\",\"th\",null,{\"children\":\"GPT4o-mini\"}],[\"$\",\"th\",null,{\"children\":\"zen-72B-Instruct\"}]]}]}],[\"$\",\"tbody\",null,{\"children\":[[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Instruction Following\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"IFEval (multilingual)\"}],[\"$\",\"td\",null,{\"children\":\"79.69\"}],[\"$\",\"td\",null,{\"children\":\"80.47\"}],[\"$\",\"td\",null,{\"children\":\"82.68\"}],[\"$\",\"td\",null,{\"children\":\"82.69\"}],[\"$\",\"td\",null,{\"children\":\"85.03\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"86.98\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Knowledge\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"AMMLU (Arabic)\"}],[\"$\",\"td\",null,{\"children\":\"68.85\"}],[\"$\",\"td\",null,{\"children\":\"70.08\"}],[\"$\",\"td\",null,{\"children\":\"70.44\"}],[\"$\",\"td\",null,{\"children\":\"69.24\"}],[\"$\",\"td\",null,{\"children\":\"69.73\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"72.44\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"JMMLU (Japanese)\"}],[\"$\",\"td\",null,{\"children\":\"77.37\"}],[\"$\",\"td\",null,{\"children\":\"73.89\"}],[\"$\",\"td\",null,{\"children\":\"76.55\"}],[\"$\",\"td\",null,{\"children\":\"75.77\"}],[\"$\",\"td\",null,{\"children\":\"73.74\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"80.56\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"KMMLU (Korean)\"}],[\"$\",\"td\",null,{\"children\":\"57.04\"}],[\"$\",\"td\",null,{\"children\":\"53.23\"}],[\"$\",\"td\",null,{\"children\":\"60.75\"}],[\"$\",\"td\",null,{\"children\":\"56.42\"}],[\"$\",\"td\",null,{\"children\":\"56.77\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"61.96\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"IndoMMLU (Indonesian)\"}],[\"$\",\"td\",null,{\"children\":\"66.31\"}],[\"$\",\"td\",null,{\"children\":\"67.50\"}],[\"$\",\"td\",null,{\"children\":\"66.42\"}],[\"$\",\"td\",null,{\"children\":\"63.21\"}],[\"$\",\"td\",null,{\"children\":\"67.75\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"69.25\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"TurkishMMLU (Turkish)\"}],[\"$\",\"td\",null,{\"children\":\"69.22\"}],[\"$\",\"td\",null,{\"children\":\"66.89\"}],[\"$\",\"td\",null,{\"children\":\"72.41\"}],[\"$\",\"td\",null,{\"children\":\"64.78\"}],[\"$\",\"td\",null,{\"children\":\"71.19\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"76.12\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"okapi MMLU (translated)\"}],[\"$\",\"td\",null,{\"children\":\"77.84\"}],[\"$\",\"td\",null,{\"children\":\"76.49\"}],[\"$\",\"td\",null,{\"children\":\"77.16\"}],[\"$\",\"td\",null,{\"children\":\"78.37\"}],[\"$\",\"td\",null,{\"children\":\"73.44\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"79.97\"}]}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Math Reasoning\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MGSM8K (extended)\"}],[\"$\",\"td\",null,{\"children\":\"82.72\"}],[\"$\",\"td\",null,{\"children\":\"73.31\"}],[\"$\",\"td\",null,{\"children\":\"87.15\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"89.01\"}]}],[\"$\",\"td\",null,{\"children\":\"87.36\"}],[\"$\",\"td\",null,{\"children\":\"88.16\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Cultural Nuances\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"BLEnD\"}],[\"$\",\"td\",null,{\"children\":\"25.90\"}],[\"$\",\"td\",null,{\"children\":\"30.49\"}],[\"$\",\"td\",null,{\"children\":\"27.88\"}],[\"$\",\"td\",null,{\"children\":\"33.47\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"35.91\"}]}],[\"$\",\"td\",null,{\"children\":\"32.48\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Datasets\"}],[\"$\",\"td\",null,{\"children\":\"zen-7B-Instruct\"}],[\"$\",\"td\",null,{\"children\":\"Llama3.1-8B-Instruct\"}],[\"$\",\"td\",null,{\"children\":\"zen-7B-Instruct\"}],[\"$\",\"td\",null,{\"children\":\"Gemma-2-9B-Instruct\"}],[\"$\",\"td\",null,{\"children\":\"Mistral-Nemo-Instruct-2407 (12B)\"}],[\"$\",\"td\",null,{\"children\":\"zen-14B-Instruct\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"---\"}],[\"$\",\"td\",null,{\"children\":\"---\"}],[\"$\",\"td\",null,{\"children\":\"---\"}],[\"$\",\"td\",null,{\"children\":\"---\"}],[\"$\",\"td\",null,{\"children\":\"---\"}],[\"$\",\"td\",null,{\"children\":\"---\"}],[\"$\",\"td\",null,{\"children\":\"---\"}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Instruction Following\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}],[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"IFEval (multilingual)\"}],[\"$\",\"td\",null,{\"children\":\"51.43\"}],[\"$\",\"td\",null,{\"children\":\"60.68\"}],[\"$\",\"td\",null,{\"children\":\"74.87\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"77.47\"}]}],[\"$\",\"td\",null,{\"children\":\"64.59\"}],[\"$\",\"td\",null,{\"children\":\"77.08\"}]]}],\"$L79\",\"$L7a\",\"$L7b\",\"$L7c\",\"$L7d\",\"$L7e\",\"$L7f\",\"$L80\",\"$L81\",\"$L82\",\"$L83\"]}]]}]}]\n"])</script><script>self.__next_f.push([1,"48:[\"$\",\"h1\",null,{\"className\":\"flex scroll-m-28 flex-row items-center gap-2\",\"id\":\"demo-cases\",\"children\":[[\"$\",\"a\",null,{\"data-card\":\"\",\"href\":\"#demo-cases\",\"className\":\"peer\",\"children\":\"Demo Cases\"}],[\"$\",\"svg\",null,{\"ref\":\"$undefined\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100\",\"aria-hidden\":true,\"children\":[[\"$\",\"path\",\"1cjeqo\",{\"d\":\"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71\"}],[\"$\",\"path\",\"19qd67\",{\"d\":\"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71\"}],\"$undefined\"]}]]}]\n49:[\"$\",\"p\",null,{\"children\":\"Here we provide several cases to demonstrate the new or enhanced capabilities of zen, including generating JSON output, generating long texts, and understanding structured data.\"}]\n4a:[\"$\",\"p\",null,{\"children\":\"Example: Generating JSON Output Next\"}]\n4b:[\"$\",\"p\",null,{\"children\":\"JSON Output\"}]\n4c:[\"$\",\"p\",null,{\"children\":\"Example: Structured Data Understanding Next\"}]\n4d:[\"$\",\"p\",null,{\"children\":\"Table Understanding\"}]\n4e:[\"$\",\"p\",null,{\"children\":\"Example: Long Text Generation Next\"}]\n4f:[\"$\",\"p\",null,{\"children\":\"Text Generation\"}]\n"])</script><script>self.__next_f.push([1,"50:[\"$\",\"td\",null,{\"children\":\"46.3\"}]\n51:[\"$\",\"td\",null,{\"children\":\"46.7\"}]\n52:[\"$\",\"td\",null,{\"children\":\"-\"}]\n53:[\"$\",\"td\",null,{\"children\":\"59.6\"}]\n54:[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"60.5\"}]}]\n55:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Multilingual Tasks\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}]\n56:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Multi-Exam\"}],[\"$\",\"td\",null,{\"children\":\"70.0\"}],[\"$\",\"td\",null,{\"children\":\"63.5\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"76.6\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"78.7\"}]}]]}]\n57:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Multi-Understanding\"}],[\"$\",\"td\",null,{\"children\":\"79.9\"}],[\"$\",\"td\",null,{\"children\":\"77.7\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"80.7\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"89.6\"}]}]]}]\n58:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Multi-Mathematics\"}],[\"$\",\"td\",null,{\"children\":\"67.1\"}],[\"$\",\"td\",null,{\"children\":\"62.9\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"76.0\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"76.7\"}]}]]}]\n59:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Multi-Translation\"}],[\"$\",\"td\",null,{\"children\":\"38.0\"}],[\"$\",\"td\",null,{\"children\":\"23.3\"}],[\"$\",\"td\",null,{\"children\":\"-\"}],[\"$\",\"td\",null,{\"children\":\"37.8\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"39.0\"}]}]]}]\n5a:[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"52.4\"}]}]\n5b:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MBPP\"}],[\"$\",\"td\",null,{\"children\":\"64.2\"}],[\"$\",\"td\",null,{\"children\":\"75.7\"}],[\"$\",\"td\",null,{\"children\":\"65.5\"}],[\"$\",\"td\",null,{\"children\":\"71.9\"}],[\"$\",\"td\",null,{\"children\":\"76.7\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"84.5\"}]}]]}]\n5c:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MBPP+\"}],[\"$\",\"td\",null,{\"children\":\"53.9\"}],[\"$\",\"td\",null,{\"children\":\"60.2\"}],[\"$\",\"td\",null,{\"children\":\"55.4\"}],[\"$\",\"td\",null,{\"children\":\"57.4\"}],[\"$\",\"td\",null,{\"children\":\"63.2\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"67.2\"}]}]]}]\n5d:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MultiPL-E\"}],[\"$\",\"td\",null,{\"children\":\"38.5\"}],[\"$\",\"td\",null,{\"children\":\"48.0\"}],[\"$\",\"td\",null,{\"children\":\"39.5\"}],[\"$\",\"td\",null,{\"children\":\"49.8\"}],[\"$\",\"td\",null,{\"children\":\"53.5\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"59.4\"}]}]]}]\n5e:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Multilingual Tasks\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}]\n5f:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Multi-Exam\"}],[\"$\",\"td\",null,{\"children\":\"61.6\"}],[\"$\",\"td\",null,{\"children\":\"65.8\"}],[\"$\",\"td\",null,{\"children\":\"58.3\"}],[\"$\",\"td\",null,{\"children\":\"65.5\"}],[\"$\",\"td\",null,{\"children\":\"70.6\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"75.4\"}]}]]}]\n60:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Multi-Understanding\"}],[\"$\",\"td\",null,{\"children\":\"76.5\"}],[\"$\",\"td\",null,{\"children\":\"82.2\"}],[\"$\",\"td\",null,{\"children\":\"73.9\"}],[\"$\",\"td\",null,{\"children\":\"77.0\"}],[\"$\",\"td\",null,{\"children\":\"85.9\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"88.4\"}]}]]}]\n61:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Multi-Mathematics\"}],[\"$\",\"td\",null,{\"children\":\"56.1\"}],[\"$\",\"td\",null,{\"children\":\"61.6\"}],[\"$\",\"td\",null,{\"children\":\"49.3\"}],[\"$\",\"td\",null,{\"children\":\"62.3\"}],[\"$\",\"td\",null,{\"children\":\"68.5\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"73.7\"}]}]]}]\n62:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Multi-Translation\"}],[\"$\",\"td\",null,{\"children\":\"33.5\"}],[\"$\",\"td\",null,{\"children\":\"38.7\"}],[\"$\",\"td\",null,{\"children"])</script><script>self.__next_f.push([1,"\":\"30.0\"}],[\"$\",\"td\",null,{\"children\":\"34.5\"}],[\"$\",\"td\",null,{\"children\":\"36.2\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"37.3\"}]}]]}]\n63:[\"$\",\"td\",null,{\"children\":\"MBPP+\"}]\n64:[\"$\",\"td\",null,{\"children\":\"40.9\"}]\n65:[\"$\",\"td\",null,{\"children\":\"44.4\"}]\n66:[\"$\",\"td\",null,{\"children\":\"50.6\"}]\n67:[\"$\",\"td\",null,{\"children\":\"51.9\"}]\n68:[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"62.9\"}]}]\n69:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MultiPL-E\"}],[\"$\",\"td\",null,{\"children\":\"29.4\"}],[\"$\",\"td\",null,{\"children\":\"22.6\"}],[\"$\",\"td\",null,{\"children\":\"34.9\"}],[\"$\",\"td\",null,{\"children\":\"41.0\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"50.3\"}]}]]}]\n6a:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Multilingual Tasks\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}]\n6b:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Multi-Exam\"}],[\"$\",\"td\",null,{\"children\":\"47.1\"}],[\"$\",\"td\",null,{\"children\":\"52.3\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"61.2\"}]}],[\"$\",\"td\",null,{\"children\":\"59.2\"}],[\"$\",\"td\",null,{\"children\":\"59.4\"}]]}]\n6c:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Multi-Understanding\"}],[\"$\",\"td\",null,{\"children\":\"63.3\"}],[\"$\",\"td\",null,{\"children\":\"68.6\"}],[\"$\",\"td\",null,{\"children\":\"78.3\"}],[\"$\",\"td\",null,{\"children\":\"72.0\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"79.3\"}]}]]}]\n6d:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Multi-Mathematics\"}],[\"$\",\"td\",null,{\"children\":\"26.3\"}],[\"$\",\"td\",null,{\"children\":\"36.3\"}],[\"$\",\"td\",null,{\"children\":\"53.0\"}],[\"$\",\"td\",null,{\"children\":\"57.5\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"57.8\"}]}]]}]\n6e:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Multi-Translation\"}],[\"$\",\"td\",null,{\"children\":\"23.3\"}],[\"$\",\"td\",null,{\"children\":\"31.9\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"36.5\"}]}],[\"$\",\"td\",null,{\"children\":\"31.5\"}],[\"$\",\"td\",null,{\"children\":\"32.4\"}]]}]\n6f:[\"$\",\"td\",null,{\"children\":\"15.9\"}]\n70:[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"36.0\"}]}]\n71:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MBPP\"}],[\"$\",\"td\",null,{\"children\":\"33.1\"}],[\"$\",\"td\",null,{\"children\":\"39.3\"}],[\"$\",\"td\",null,{\"children\":\"46.9\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"60.2\"}]}],[\"$\",\"td\",null,{\"children\":\"42.1\"}],[\"$\",\"td\",null,{\"children\":\"57.1\"}]]}]\n72:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MBPP+\"}],[\"$\",\"td\",null,{\"children\":\"27.6\"}],[\"$\",\"td\",null,{\"children\":\"33.8\"}],[\"$\",\"td\",null,{\"children\":\"37.6\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"49.6\"}]}],[\"$\",\"td\",null,{\"children\":\"33.6\"}],[\"$\",\"td\",null,{\"children\":\"49.4\"}]]}]\n73:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MultiPL-E\"}],[\"$\",\"td\",null,{\"children\":\"16.3\"}],[\"$\",\"td\",null,{\"children\":\"18.9\"}],[\"$\",\"td\",null,{\"children\":\"27.9\"}],[\"$\",\"td\",null,{\"children\":\"33.1\"}],[\"$\",\"td\",null,{\"children\":\"17.6\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"41.2\"}]}]]}]\n74:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Multilingual Tasks\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}]\n75:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Multi-Exam\"}],[\"$\",\"td\",null,{\"children\":\"29.4\"}],[\"$\",\"td\",null,{\"children\":\"30.8\"}],[\"$\",\"td\",null,{\"children\":\"43.1\"}],[\"$\",\"td\",null,{\"children\":\"47.9\"}],[\"$\",\"td\",null,{\"children\":\"38.1\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"54.6\"}]}]]}]\n76:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Multi-Understanding\"}],[\"$\",\"td\",null,{\"children\":\"40.4\"}],[\"$\",\"td\",null,{\"children\":\"41.0\"}],[\"$\",\"td\",null,{\"children\":\"50.7\"}],[\"$\",\"td\",null,{\"children\":\"65.1\"}],[\"$\",\"td\",null,{\"children\":\"46.8\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",nul"])</script><script>self.__next_f.push([1,"l,{\"children\":\"76.6\"}]}]]}]\n77:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Multi-Mathematics\"}],[\"$\",\"td\",null,{\"children\":\"7.8\"}],[\"$\",\"td\",null,{\"children\":\"13.5\"}],[\"$\",\"td\",null,{\"children\":\"21.3\"}],[\"$\",\"td\",null,{\"children\":\"37.5\"}],[\"$\",\"td\",null,{\"children\":\"18.2\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"48.9\"}]}]]}]\n78:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"Multi-Translation\"}],[\"$\",\"td\",null,{\"children\":\"14.1\"}],[\"$\",\"td\",null,{\"children\":\"15.3\"}],[\"$\",\"td\",null,{\"children\":\"23.8\"}],[\"$\",\"td\",null,{\"children\":\"25.0\"}],[\"$\",\"td\",null,{\"children\":\"26.9\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"29.3\"}]}]]}]\n79:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Knowledge\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}]\n7a:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"AMMLU (Arabic)\"}],[\"$\",\"td\",null,{\"children\":\"54.87\"}],[\"$\",\"td\",null,{\"children\":\"54.28\"}],[\"$\",\"td\",null,{\"children\":\"59.78\"}],[\"$\",\"td\",null,{\"children\":\"60.26\"}],[\"$\",\"td\",null,{\"children\":\"53.92\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"66.81\"}]}]]}]\n7b:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"JMMLU (Japanese)\"}],[\"$\",\"td\",null,{\"children\":\"57.71\"}],[\"$\",\"td\",null,{\"children\":\"53.26\"}],[\"$\",\"td\",null,{\"children\":\"61.88\"}],[\"$\",\"td\",null,{\"children\":\"64.59\"}],[\"$\",\"td\",null,{\"children\":\"55.17\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"72.78\"}]}]]}]\n7c:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"KMMLU (Korean)\"}],[\"$\",\"td\",null,{\"children\":\"43.96\"}],[\"$\",\"td\",null,{\"children\":\"42.28\"}],[\"$\",\"td\",null,{\"children\":\"46.59\"}],[\"$\",\"td\",null,{\"children\":\"46.24\"}],[\"$\",\"td\",null,{\"children\":\"42.22\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"59.71\"}]}]]}]\n7d:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"IndoMMLU (Indonesian)\"}],[\"$\",\"td\",null,{\"children\":\"54.05\"}],[\"$\",\"td\",null,{\"children\":\"53.92\"}],[\"$\",\"td\",null,{\"children\":\"56.42\"}],[\"$\",\"td\",null,{\"children\":\"61.73\"}],[\"$\",\"td\",null,{\"children\":\"50.76\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"65.09\"}]}]]}]\n7e:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"TurkishMMLU (Turkish)\"}],[\"$\",\"td\",null,{\"children\":\"49.27\"}],[\"$\",\"td\",null,{\"children\":\"45.61\"}],[\"$\",\"td\",null,{\"children\":\"54.28\"}],[\"$\",\"td\",null,{\"children\":\"55.44\"}],[\"$\",\"td\",null,{\"children\":\"34.44\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"66.85\"}]}]]}]\n7f:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"okapi MMLU (translated)\"}],[\"$\",\"td\",null,{\"children\":\"60.47\"}],[\"$\",\"td\",null,{\"children\":\"55.18\"}],[\"$\",\"td\",null,{\"children\":\"66.98\"}],[\"$\",\"td\",null,{\"children\":\"46.72\"}],[\"$\",\"td\",null,{\"children\":\"59.65\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"72.12\"}]}]]}]\n80:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Math Reasoning\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}]\n81:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"MGSM8K (extended)\"}],[\"$\",\"td\",null,{\"children\":\"56.13\"}],[\"$\",\"td\",null,{\"children\":\"66.05\"}],[\"$\",\"td\",null,{\"children\":\"66.11\"}],[\"$\",\"td\",null,{\"children\":\"78.37\"}],[\"$\",\"td\",null,{\"children\":\"54.75\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"82.27\"}]}]]}]\n82:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":[\"$\",\"em\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Cultural Nuances\"}]}]}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}],[\"$\",\"td\",null,{}]]}]\n83:[\"$\",\"tr\",null,{\"children\":[[\"$\",\"td\",null,{\"children\":\"BLEnD\"}],[\"$\",\"td\",null,{\"children\":\"22.49\"}],[\"$\",\"td\",null,{\"children\":\"19.47\"}],[\"$\",\"td\",null,{\"children\":\"23.66\"}],[\"$\",\"td\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"28.31\"}]}],[\"$\",\"td\",null,{\"c"])</script><script>self.__next_f.push([1,"hildren\":\"26.61\"}],[\"$\",\"td\",null,{\"children\":\"26.99\"}]]}]\n"])</script><script>self.__next_f.push([1,"b:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"2\",{\"name\":\"theme-color\",\"media\":\"(prefers-color-scheme: dark)\",\"content\":\"#0A0A0A\"}],[\"$\",\"meta\",\"3\",{\"name\":\"theme-color\",\"media\":\"(prefers-color-scheme: light)\",\"content\":\"#fff\"}]]\n"])</script><script>self.__next_f.push([1,"9:null\nd:[[\"$\",\"title\",\"0\",{\"children\":\"zen-LLM: Extending the boundary of LLMs — Zen LM Blog | Zen LM\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\"}],[\"$\",\"meta\",\"2\",{\"property\":\"og:title\",\"content\":\"Zen LM - Open Foundation Models\"}],[\"$\",\"meta\",\"3\",{\"property\":\"og:description\",\"content\":\"Zen AI model family by Hanzo AI. 14 frontier models from 4B to 1T+ parameters for code, reasoning, vision, and multimodal tasks.\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:url\",\"content\":\"https://zenlm.org\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:site_name\",\"content\":\"Zen LM\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"7\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"8\",{\"name\":\"twitter:site\",\"content\":\"@zenlmorg\"}],[\"$\",\"meta\",\"9\",{\"name\":\"twitter:creator\",\"content\":\"@zenlmorg\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:title\",\"content\":\"Zen LM - Open Foundation Models\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:description\",\"content\":\"Zen AI model family by Hanzo AI. 14 frontier models from 4B to 1T+ parameters for code, reasoning, vision, and multimodal tasks.\"}]]\n"])</script></body></html>