<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Zen MoDE: Mixture of Distilled Experts | Zen LM</title><meta name=keywords content="Architecture,Research,MoE"><meta name=description content="A technical deep dive into Zen MoDE — Mixture of Distilled Experts — the architecture underlying all Zen models."><meta name=author content="Zen LM Team"><link rel=canonical href=https://zenlm.org/blog/zen-mode-architecture/><link crossorigin=anonymous href=/assets/css/stylesheet.6c0865a4bc8dbcbd27d78777eb33b404568f7fbf3185cca7b978e5275a4dd049.css integrity="sha256-bAhlpLyNvL0n14d36zO0BFaPf78xhcynuXjlJ1pN0Ek=" rel="preload stylesheet" as=style><link rel=icon href=https://zenlm.org/favicon.png><link rel=apple-touch-icon href=https://zenlm.org/favicon.png><link rel=manifest href=https://zenlm.org/site.webmanifest><meta name=theme-color content="#615CED"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:title" content="Zen MoDE: Mixture of Distilled Experts"><meta property="og:description" content="A technical deep dive into Zen MoDE — Mixture of Distilled Experts — the architecture underlying all Zen models."><meta property="og:type" content="article"><meta property="og:url" content="https://zenlm.org/blog/zen-mode-architecture/"><meta property="og:image" content="https://zenlm.org/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2026-01-16T09:00:00-08:00"><meta property="article:modified_time" content="2026-01-16T09:00:00-08:00"><meta property="og:site_name" content="Zen LM"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://zenlm.org/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Zen MoDE: Mixture of Distilled Experts"><meta name=twitter:description content="A technical deep dive into Zen MoDE — Mixture of Distilled Experts — the architecture underlying all Zen models."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://zenlm.org/blog/"},{"@type":"ListItem","position":2,"name":"Zen MoDE: Mixture of Distilled Experts","item":"https://zenlm.org/blog/zen-mode-architecture/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Zen MoDE: Mixture of Distilled Experts","name":"Zen MoDE: Mixture of Distilled Experts","description":"A technical deep dive into Zen MoDE — Mixture of Distilled Experts — the architecture underlying all Zen models.","keywords":["Architecture","Research","MoE"],"articleBody":"GITHUB HUGGING FACE\nAll Zen models are built on Zen MoDE: Mixture of Distilled Experts. This post explains the architecture, why we chose it, and how distillation and expert routing interact to deliver frontier capability at practical inference cost.\nThe Core Problem There is a fundamental tension in large model design:\nMore parameters → better capability More parameters → higher inference cost Dense scaling laws are well established. Doubling parameters roughly halves perplexity (with sufficient data), but doubles inference FLOP. For production deployment, this is often prohibitive.\nSparse mixture-of-experts (MoE) architectures break this tradeoff. A model with $N$ total parameters activates only $k$ parameters per token. Capability scales with $N$; inference cost scales with $k$.\nThe challenge: sparse models are harder to train. Expert collapse (all routing to a small subset) and load imbalance are common failure modes. Distillation from strong dense teachers is our solution.\nExpert Routing In Zen MoDE, each transformer layer contains $E$ feed-forward expert networks. A lightweight router assigns each token to the top-$k$ experts:\n$$\\text{route}(x) = \\text{TopK}(\\text{softmax}(W_r x), k)$$\nThe final output for a token is the weighted combination of selected expert outputs:\n$$\\text{FFN}(x) = \\sum_{i \\in \\text{TopK}} g_i \\cdot E_i(x)$$\nwhere $g_i$ are the gating scores from the router.\nFor Zen4 Ultra (480B total, 35B active):\n$E = 128$ experts per layer $k = 8$ experts selected per token Active parameter fraction: ~7.3% Load Balancing Expert collapse is a training failure mode where a small subset of experts receives nearly all routing weight. We prevent this with a combination of:\nAuxiliary load-balancing loss: Penalizes variance in expert utilization across a batch Expert-choice routing in the first two layers: experts select tokens rather than tokens selecting experts Gradient clipping per-expert: Prevents any single expert from dominating early training Our training logs show utilization entropy $\u003e 0.95$ of maximum for all layers by step 10K.\nDistillation The “Distilled” in MoDE refers to how we initialize and refine experts. Standard MoE training from random initialization is unstable and slow. Our approach:\nPhase 1: Teacher Pre-Training A dense teacher model is trained to convergence on the full corpus. This teacher encodes rich representations of the training distribution and serves as the knowledge source for distillation.\nPhase 2: Expert Initialization Experts are initialized by clustering the teacher’s FFN weight matrices. We use $k$-means clustering on the weight space to partition the teacher’s knowledge into $E$ expert “seeds.” This gives each expert a distinct initialization that covers a different region of the input distribution.\nPhase 3: Joint Training with Distillation Loss The MoE student trains against two objectives simultaneously:\n$$\\mathcal{L} = \\mathcal{L}{\\text{LM}} + \\lambda \\cdot \\mathcal{L}{\\text{distill}}$$\nThe distillation loss minimizes KL divergence between student and teacher output distributions:\n$$\\mathcal{L}{\\text{distill}} = \\text{KL}(p{\\text{teacher}} | p_{\\text{student}})$$\nThis keeps experts aligned with the teacher’s knowledge while allowing specialization to emerge naturally through routing.\nPhase 4: Expert Specialization As training progresses, experts naturally specialize. We observe consistent patterns:\nSome experts specialize on code tokens Others on mathematical notation Others on conversational context Some appear to specialize on specific languages We do not explicitly enforce this specialization — it emerges from the routing gradient.\nWhy MoE Beats Dense for Capability/Cost Consider Zen Max (72B dense equivalent) vs. Zen4 Pro (32B MoE, 22B active). At similar inference cost:\nZen Max 72B Zen4 Pro 32B MoE Total params 72B 32B Active params 72B 22B Inference FLOP 1x 0.31x MMLU 87.1 85.3 MATH 73.2 71.8 HumanEval 82.4 80.9 At 69% lower inference cost, the MoE model is within 2 points on all benchmarks. For most production use cases, this tradeoff is clearly favorable.\nContext Handling Zen MoDE uses grouped-query attention (GQA) for memory efficiency:\n8 KV heads shared across 64 query heads ~8x reduction in KV cache memory Enables longer effective context at the same VRAM budget For Zen4 Ultra, the native context window is 256K tokens, extending to 1M with YaRN (Yet another RoPE extensioN) scaling.\nTokenizer All Zen4 models share a unified tokenizer:\n151,936 vocabulary size Byte-level BPE with UTF-8 pre-tokenization Special tokens for chat, tool-use, and reasoning modes Code-optimized: reserved token ranges for common code patterns Compared to smaller vocabularies, this reduces average token count per document by 12% for English and 25% for code.\nTraining Infrastructure Zen models are trained on the Zoo Compute Network:\nDistributed across H100/A100 clusters contributed by network participants ZeRO-3 parallelism for memory efficiency Pipeline parallelism for very large models Gradient checkpointing enabled by default Training Zen4 Ultra (480B) required 2.1 million GPU-hours. This was funded through the Zoo Labs Foundation treasury (ZIP-72 governance vote) and completed over 6 months.\nOpen Weights and Reproducibility We release all weights, tokenizer configurations, and training hyperparameters. We do not release training data (privacy and licensing constraints), but we publish detailed data composition statistics and filtering methodology.\nArchitecture configurations are available in the Hugging Face model cards under hanzoai/.\nQuestions? Open an issue on GitHub or join the Discord.\n","wordCount":"814","inLanguage":"en","datePublished":"2026-01-16T09:00:00-08:00","dateModified":"2026-01-16T09:00:00-08:00","author":{"@type":"Person","name":"Zen LM Team"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://zenlm.org/blog/zen-mode-architecture/"},"publisher":{"@type":"Organization","name":"Zen LM","logo":{"@type":"ImageObject","url":"https://zenlm.org/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Zen LM (Alt + H)"><img src=https://zenlm.org/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://zeekay.blog title=zeekay><span>zeekay</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://hanzo.blog title=hanzo.blog><span>hanzo.blog</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://hanzo.ai/chat title="Try Zen Chat"><span>Try Zen Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://blog.zoo.ngo title="zoo blog"><span>zoo blog</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Zen MoDE: Mixture of Distilled Experts</h1><div class=post-description>A technical deep dive into Zen MoDE — Mixture of Distilled Experts — the architecture underlying all Zen models.</div><div class=post-meta><span title='2026-01-16 09:00:00 -0800 -0800'>January 16, 2026</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;814 words&nbsp;·&nbsp;Zen LM Team</div></div></div><main class=main><article class=post-single><div class=post-content><p><a href=https://github.com/hanzoai class="btn external" target=_blank>GITHUB</a>
<a href=https://huggingface.co/hanzoai class="btn external" target=_blank>HUGGING FACE</a></p><p>All Zen models are built on <strong>Zen MoDE</strong>: Mixture of Distilled Experts. This post explains the architecture, why we chose it, and how distillation and expert routing interact to deliver frontier capability at practical inference cost.</p><h2 id=the-core-problem>The Core Problem<a hidden class=anchor aria-hidden=true href=#the-core-problem>#</a></h2><p>There is a fundamental tension in large model design:</p><ul><li>More parameters → better capability</li><li>More parameters → higher inference cost</li></ul><p>Dense scaling laws are well established. Doubling parameters roughly halves perplexity (with sufficient data), but doubles inference FLOP. For production deployment, this is often prohibitive.</p><p>Sparse mixture-of-experts (MoE) architectures break this tradeoff. A model with $N$ total parameters activates only $k$ parameters per token. Capability scales with $N$; inference cost scales with $k$.</p><p>The challenge: sparse models are harder to train. Expert collapse (all routing to a small subset) and load imbalance are common failure modes. Distillation from strong dense teachers is our solution.</p><h2 id=expert-routing>Expert Routing<a hidden class=anchor aria-hidden=true href=#expert-routing>#</a></h2><p>In Zen MoDE, each transformer layer contains $E$ feed-forward expert networks. A lightweight router assigns each token to the top-$k$ experts:</p><p>$$\text{route}(x) = \text{TopK}(\text{softmax}(W_r x), k)$$</p><p>The final output for a token is the weighted combination of selected expert outputs:</p><p>$$\text{FFN}(x) = \sum_{i \in \text{TopK}} g_i \cdot E_i(x)$$</p><p>where $g_i$ are the gating scores from the router.</p><p>For Zen4 Ultra (480B total, 35B active):</p><ul><li>$E = 128$ experts per layer</li><li>$k = 8$ experts selected per token</li><li>Active parameter fraction: ~7.3%</li></ul><h3 id=load-balancing>Load Balancing<a hidden class=anchor aria-hidden=true href=#load-balancing>#</a></h3><p>Expert collapse is a training failure mode where a small subset of experts receives nearly all routing weight. We prevent this with a combination of:</p><ol><li><strong>Auxiliary load-balancing loss</strong>: Penalizes variance in expert utilization across a batch</li><li><strong>Expert-choice routing</strong> in the first two layers: experts select tokens rather than tokens selecting experts</li><li><strong>Gradient clipping per-expert</strong>: Prevents any single expert from dominating early training</li></ol><p>Our training logs show utilization entropy $> 0.95$ of maximum for all layers by step 10K.</p><h2 id=distillation>Distillation<a hidden class=anchor aria-hidden=true href=#distillation>#</a></h2><p>The &ldquo;Distilled&rdquo; in MoDE refers to how we initialize and refine experts. Standard MoE training from random initialization is unstable and slow. Our approach:</p><h3 id=phase-1-teacher-pre-training>Phase 1: Teacher Pre-Training<a hidden class=anchor aria-hidden=true href=#phase-1-teacher-pre-training>#</a></h3><p>A dense teacher model is trained to convergence on the full corpus. This teacher encodes rich representations of the training distribution and serves as the knowledge source for distillation.</p><h3 id=phase-2-expert-initialization>Phase 2: Expert Initialization<a hidden class=anchor aria-hidden=true href=#phase-2-expert-initialization>#</a></h3><p>Experts are initialized by clustering the teacher&rsquo;s FFN weight matrices. We use $k$-means clustering on the weight space to partition the teacher&rsquo;s knowledge into $E$ expert &ldquo;seeds.&rdquo; This gives each expert a distinct initialization that covers a different region of the input distribution.</p><h3 id=phase-3-joint-training-with-distillation-loss>Phase 3: Joint Training with Distillation Loss<a hidden class=anchor aria-hidden=true href=#phase-3-joint-training-with-distillation-loss>#</a></h3><p>The MoE student trains against two objectives simultaneously:</p><p>$$\mathcal{L} = \mathcal{L}<em>{\text{LM}} + \lambda \cdot \mathcal{L}</em>{\text{distill}}$$</p><p>The distillation loss minimizes KL divergence between student and teacher output distributions:</p><p>$$\mathcal{L}<em>{\text{distill}} = \text{KL}(p</em>{\text{teacher}} | p_{\text{student}})$$</p><p>This keeps experts aligned with the teacher&rsquo;s knowledge while allowing specialization to emerge naturally through routing.</p><h3 id=phase-4-expert-specialization>Phase 4: Expert Specialization<a hidden class=anchor aria-hidden=true href=#phase-4-expert-specialization>#</a></h3><p>As training progresses, experts naturally specialize. We observe consistent patterns:</p><ul><li>Some experts specialize on code tokens</li><li>Others on mathematical notation</li><li>Others on conversational context</li><li>Some appear to specialize on specific languages</li></ul><p>We do not explicitly enforce this specialization — it emerges from the routing gradient.</p><h2 id=why-moe-beats-dense-for-capabilitycost>Why MoE Beats Dense for Capability/Cost<a hidden class=anchor aria-hidden=true href=#why-moe-beats-dense-for-capabilitycost>#</a></h2><p>Consider Zen Max (72B dense equivalent) vs. Zen4 Pro (32B MoE, 22B active). At similar inference cost:</p><table><thead><tr><th></th><th>Zen Max 72B</th><th>Zen4 Pro 32B MoE</th></tr></thead><tbody><tr><td>Total params</td><td>72B</td><td>32B</td></tr><tr><td>Active params</td><td>72B</td><td>22B</td></tr><tr><td>Inference FLOP</td><td>1x</td><td>0.31x</td></tr><tr><td>MMLU</td><td>87.1</td><td>85.3</td></tr><tr><td>MATH</td><td>73.2</td><td>71.8</td></tr><tr><td>HumanEval</td><td>82.4</td><td>80.9</td></tr></tbody></table><p>At 69% lower inference cost, the MoE model is within 2 points on all benchmarks. For most production use cases, this tradeoff is clearly favorable.</p><h2 id=context-handling>Context Handling<a hidden class=anchor aria-hidden=true href=#context-handling>#</a></h2><p>Zen MoDE uses grouped-query attention (GQA) for memory efficiency:</p><ul><li>8 KV heads shared across 64 query heads</li><li>~8x reduction in KV cache memory</li><li>Enables longer effective context at the same VRAM budget</li></ul><p>For Zen4 Ultra, the native context window is 256K tokens, extending to 1M with YaRN (Yet another RoPE extensioN) scaling.</p><h2 id=tokenizer>Tokenizer<a hidden class=anchor aria-hidden=true href=#tokenizer>#</a></h2><p>All Zen4 models share a unified tokenizer:</p><ul><li>151,936 vocabulary size</li><li>Byte-level BPE with UTF-8 pre-tokenization</li><li>Special tokens for chat, tool-use, and reasoning modes</li><li>Code-optimized: reserved token ranges for common code patterns</li></ul><p>Compared to smaller vocabularies, this reduces average token count per document by 12% for English and 25% for code.</p><h2 id=training-infrastructure>Training Infrastructure<a hidden class=anchor aria-hidden=true href=#training-infrastructure>#</a></h2><p>Zen models are trained on the Zoo Compute Network:</p><ul><li>Distributed across H100/A100 clusters contributed by network participants</li><li>ZeRO-3 parallelism for memory efficiency</li><li>Pipeline parallelism for very large models</li><li>Gradient checkpointing enabled by default</li></ul><p>Training Zen4 Ultra (480B) required 2.1 million GPU-hours. This was funded through the Zoo Labs Foundation treasury (ZIP-72 governance vote) and completed over 6 months.</p><h2 id=open-weights-and-reproducibility>Open Weights and Reproducibility<a hidden class=anchor aria-hidden=true href=#open-weights-and-reproducibility>#</a></h2><p>We release all weights, tokenizer configurations, and training hyperparameters. We do not release training data (privacy and licensing constraints), but we publish detailed data composition statistics and filtering methodology.</p><p>Architecture configurations are available in the Hugging Face model cards under <code>hanzoai/</code>.</p><hr><p><em>Questions? Open an issue on <a href=https://github.com/hanzoai>GitHub</a> or join the <a href=https://discord.gg/hanzoai>Discord</a>.</em></p></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://zenlm.org/>Zen LM</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>