1:"$Sreact.fragment"
2:I[10086,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/36bfed0236ce2cf2.js","/_next/static/chunks/e62b91212ee7f8ff.js","/_next/static/chunks/2a98816c7d26bf58.js","/_next/static/chunks/cb0a883bafeb6805.js"],""]
3:I[48068,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/36bfed0236ce2cf2.js","/_next/static/chunks/e62b91212ee7f8ff.js","/_next/static/chunks/2a98816c7d26bf58.js","/_next/static/chunks/cb0a883bafeb6805.js"],"default"]
18:I[51504,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/36bfed0236ce2cf2.js","/_next/static/chunks/e62b91212ee7f8ff.js","/_next/static/chunks/2a98816c7d26bf58.js","/_next/static/chunks/cb0a883bafeb6805.js"],"CodeBlock"]
19:I[51504,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/36bfed0236ce2cf2.js","/_next/static/chunks/e62b91212ee7f8ff.js","/_next/static/chunks/2a98816c7d26bf58.js","/_next/static/chunks/cb0a883bafeb6805.js"],"Pre"]
1a:I[89923,["/_next/static/chunks/4d80e004cf4896dd.js","/_next/static/chunks/350ee4303b732916.js"],"OutletBoundary"]
1b:"$Sreact.suspense"
0:{"buildId":"o_J3cFnAZ1mdL_cv2bxKY","rsc":["$","$1","c",{"children":[["$","main",null,{"className":"mx-auto w-full max-w-2xl px-4 py-16","children":[["$","$L2",null,{"href":"/blog","className":"inline-flex items-center gap-1.5 text-sm text-fd-muted-foreground hover:text-fd-foreground mb-10 transition-colors","children":"← Back to Blog"}],["$","div",null,{"className":"mb-8","children":[["$","time",null,{"className":"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider","children":"January 13, 2025"}],["$","h1",null,{"className":"text-3xl font-bold mt-2 mb-3","children":"Towards Effective Process Supervision in Mathematical Reasoning"}],["$","p",null,{"className":"text-fd-muted-foreground text-lg mb-4","children":"GITHUB HUGGING FACE MODELSCOPE DISCORD"}],["$","div",null,{"className":"flex items-center gap-3 pt-4 border-t border-fd-border","children":[["$","span",null,{"className":"text-sm text-fd-muted-foreground","children":["By ","Zen LM Team"]}],["$","div",null,{"className":"flex gap-1.5 ml-auto","children":[]}]]}]]}],["$","div",null,{"className":"prose dark:prose-invert max-w-none","children":[["$","p",null,{"children":[["$","$L3",null,{"href":"https://github.com/QwenLM/zen-Math","children":"GITHUB"}]," ",["$","$L3",null,{"href":"https://huggingface.co/collections/Qwen/qwen25-math-66eaa240a1b7d5ee65f1da3e","children":"HUGGING FACE"}]," ",["$","$L3",null,{"href":"https://modelscope.cn/organization/qwen","children":"MODELSCOPE"}]," ",["$","$L3",null,{"href":"https://discord.gg/yPEP2vHTu4","children":"DISCORD"}]]}],"\n",["$","h1",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"introduction","children":[["$","a",null,{"data-card":"","href":"#introduction","className":"peer","children":"Introduction"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}],"\n",["$","p",null,{"children":"In recent years, Large Language Models (LLMs) have made remarkable advances in mathematical reasoning, yet they can make mistakes, such as miscalculations or logical errors, leading to wrong conclusions. Moreover, even when achieving correct final answers, these powerful models can still regularly make up plausible reasoning steps, where the final answers build upon flawed calculations or derivations, which undermine the reliability and trustworthiness of LLMs’ reasoning processes. Hence, automated identification of errors in the reasoning process becomes increasingly significant for their scalable oversight."}],"\n",["$","p",null,{"children":"Process Reward Models (PRMs) emerge as a promising approach for process supervision in mathematical reasoning of LLMs, which aim to identify and mitigate intermediate errors in the reasoning processes. In terms of evaluation, previous studies have predominantly relied on the response-level Best-of-N (BoN) evaluation, which selects the highest-scored response from $N$ candidates according to a PRM."}],"\n",["$","p",null,{"children":"Today, we release a new state-of-the-art PRM that outperforms existing open-source alternatives for future research in building process supervision models. We also release the step-level benchmark ProcessBench for measuring the model ability to identify erroneous steps in mathematical reasoning."}],"\n",["$","h1",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"open-sourcing-processbench","children":["$L4","$L5"]}],"\n","$L6","\n","$L7","\n","$L8","\n","$L9","\n","$La","\n","$Lb","\n","$Lc","\n","$Ld","\n","$Le","\n","$Lf","\n","$L10","\n","$L11","\n","$L12"]}]]}],["$L13","$L14","$L15","$L16"],"$L17"]}],"loading":null,"isPartial":false}
4:["$","a",null,{"data-card":"","href":"#open-sourcing-processbench","className":"peer","children":"Open-Sourcing ProcessBench"}]
5:["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]
6:["$","p",null,{"children":"ProcessBench aims to measure the model ability to identify erroneous steps in mathematical reasoning. It consists of 3,400 test cases, primarily focused on competition- and Olympiad-level math problems. Each test case contains a step-by-step solution with error location annotated by human experts. Models are required to identify the earliest step that contains an error, or conclude that all steps are correct. ProcessBench can be used to evaluate two types of models: PRMs and critic models, where for the latter we prompt general language models to critique each solution step by step."}]
7:["$","h1",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"releasing-process-reward-models","children":[["$","a",null,{"data-card":"","href":"#releasing-process-reward-models","className":"peer","children":"Releasing Process Reward Models"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
8:["$","p",null,{"children":"We release two PRMs fine-tuned on zen-Math-7B-Instruct and zen-Math-72B-Instruct, namely zen-Math-PRM-7B and zen-Math-PRM-72B respectively. Our trained PRMs exhibit both impressive performance in the BoN evaluation and stronger error identification performance in ProcessBench."}]
9:["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"best-of-n-evaluation","children":[["$","a",null,{"data-card":"","href":"#best-of-n-evaluation","className":"peer","children":"Best-of-N Evaluation"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
a:["$","p",null,{"children":"Following zen-Math, we sampled eight responses (i.e., $N=8$) from zen-Math-7B-Instruct across multiple mathematical benchmarks, including GSM8K, MATH, Minerva Math, GaoKao 2023 En, OlympiadBench, College Math, and MMLU STEM. Each candidate response is scored using the product of all the individual scores of each step within the response. We report the result of majority voting among eight samplings (maj@8) as the baseline, and pass@8 (i.e., the proportion of test samples where any of the eight samplings lead to the correct final answers) as the upper bound."}]
b:["$","p",null,{"children":"As shown in the following table, zen-Math-PRM-7B demonstrates superior performance compared to other PRMs of equivalent model scale. Notably, it outperforms maj@8 across all 7 tasks, achieving an average improvement of 1.4%. Furthermore, zen-Math-PRM-72B exhibits slightly better overall performance than zen-Math-RM-72B, with particularly significant improvements observed in the Minerva Math and MMLU STEM tasks."}]
c:["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"processbench","children":[["$","a",null,{"data-card":"","href":"#processbench","className":"peer","children":"ProcessBench"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
d:["$","p",null,{"children":"We also evaluate our PRMs on ProcessBench to measure the ability of identify erroneous steps. When compared with LLM-as-judge, zen-Math-PRM-7B in smaller model size demonstrates superior performance over all open-source models. For proprietary language models, zen-Math-PRM-7B outperforms GPT-4o-0806, while there remains a performance gap compared to o1-mini. Furthermore, in comparison with existing PRMs, both zen-Math-PRM-7B and zen-Math-PRM-72B exhibit substantial advantages over their counterparts. An interesting observation worth noting is that the Outcome Reward Model (ORM) zen-Math-RM-72B exhibits considerable capability in identifying step errors, even surpassing some open-source PRMs, which validates its potential as a complementary reward beyond solely rule-based mechanism."}]
e:["$","h1",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"conclusion","children":[["$","a",null,{"data-card":"","href":"#conclusion","className":"peer","children":"Conclusion"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
f:["$","p",null,{"children":"ProcessBench demonstrate the current challenges of existing PRMs and fills the gap in step-level evaluation of PRMs. Besides open-sourcing PRMs, we also identify critical limitations in current data construction approaches for PRMs and reveal the potential bias in using response-level BoN evaluation alone for PRMs through extensive empirical studies in our paper. We hope that ProcessBench and the best practices in training our own PRMs can foster future research and development for reasoning process supervision. For more details, please check out our papers in the following!"}]
10:["$","h1",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"citation","children":[["$","a",null,{"data-card":"","href":"#citation","className":"peer","children":"Citation"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
11:["$","p",null,{"children":"If you find our work helpful, feel free to give us a citation."}]
12:["$","$L18",null,{"className":"shiki shiki-themes github-light github-dark","style":{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},"tabIndex":"0","icon":"<svg viewBox=\"0 0 24 24\"><path d=\"M 6,1 C 4.354992,1 3,2.354992 3,4 v 16 c 0,1.645008 1.354992,3 3,3 h 12 c 1.645008,0 3,-1.354992 3,-3 V 8 7 A 1.0001,1.0001 0 0 0 20.707031,6.2929687 l -5,-5 A 1.0001,1.0001 0 0 0 15,1 h -1 z m 0,2 h 7 v 3 c 0,1.645008 1.354992,3 3,3 h 3 v 11 c 0,0.564129 -0.435871,1 -1,1 H 6 C 5.4358712,21 5,20.564129 5,20 V 4 C 5,3.4358712 5.4358712,3 6,3 Z M 15,3.4140625 18.585937,7 H 16 C 15.435871,7 15,6.5641288 15,6 Z\" fill=\"currentColor\" /></svg>","children":["$","$L19",null,{"children":["$","code",null,{"children":[["$","span",null,{"className":"line","children":["$","span",null,{}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"    @article{processbench,"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"      title={{ProcessBench:} Identifying Process Errors in Mathematical Reasoning}, "}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"      author={"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"        Chujie Zheng and Zhenru Zhang and Beichen Zhang and Runji Lin and Keming Lu and"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"        Bowen Yu and Dayiheng Liu and Jingren Zhou and Junyang Lin"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"      },"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"      journal={arXiv preprint arXiv:2412.06559},"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"      year={2024}"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"    }"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"    @article{prmlessons,"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"      title={The Lessons of Developing Process Reward Models in Mathematical Reasoning}, "}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"      author={"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"        Zhenru Zhang and Chujie Zheng and Yangzhen Wu and Beichen Zhang and Runji Lin and Bowen Yu and Dayiheng Liu and Jingren Zhou and Junyang Lin"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"      },"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"      journal={arXiv preprint arXiv:2501.07301},"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"      year={2025}"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"    }"}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"children":"    "}]}]]}]}]}]
13:["$","script","script-0",{"src":"/_next/static/chunks/36bfed0236ce2cf2.js","async":true}]
14:["$","script","script-1",{"src":"/_next/static/chunks/e62b91212ee7f8ff.js","async":true}]
15:["$","script","script-2",{"src":"/_next/static/chunks/2a98816c7d26bf58.js","async":true}]
16:["$","script","script-3",{"src":"/_next/static/chunks/cb0a883bafeb6805.js","async":true}]
17:["$","$L1a",null,{"children":["$","$1b",null,{"name":"Next.MetadataOutlet","children":"$@1c"}]}]
1c:null
