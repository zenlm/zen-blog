[{"summary":"Tech Report GitHub Hugging Face ModelScope DISCORD\nIntroduction We are excited to introduce Qwen3Guard, the first safety guardrail model in the Qwen family. Built upon the powerful Qwen3 foundation models and fine-tuned specifically for safety classificatoin, Qwen3Guard ensures responsible AI interactions by delivering precise safety detection for both prompts and responses, complete with risk levels and categorized classifications for accurate moderation.\nQwen3Guard achieves state-of-the-art performance on major safety benchmarks, demonstrating strong capabilities in both prompt and response classification tasks across English, Chinese, and multilingual environments.","title":"Qwen3Guard: Real-time Safety for Your Token Stream"},{"summary":"QWEN CHAT GITHUB HUGGING FACE MODELSCOPE DISCORD\nWe are excited to introduce Qwen-Image-Edit, the image editing version of Qwen-Image. Built upon our 20B Qwen-Image model, Qwen-Image-Edit successfully extends Qwen-Image\u0026rsquo;s unique text rendering capabilities to image editing tasks, enabling precise text editing. Furthermore, Qwen-Image-Edit simultaneously feeds the input image into zen-VL (for visual semantic control) and the VAE Encoder (for visual appearance control), achieving capabilities in both semantic and appearance editing.","title":"Qwen-Image-Edit: Image Editing with Higher Quality and Efficiency"},{"summary":"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\nWe are thrilled to release Qwen-Image, a 20B MMDiT image foundation model that achieves significant advances in complex text rendering and precise image editing. To try the latest model, feel free to visit Qwen Chat and choose ‚ÄúImage Generation‚Äù.\nThe key features include:\nSuperior Text Rendering: Qwen-Image excels at complex text rendering, including multi-line layouts, paragraph-level semantics, and fine-grained details. It supports both alphabetic languages (e.","title":"Qwen-Image: Crafting with Native Text Rendering"},{"summary":"PAPER DISCORD\nIntroduction Reinforcement Learning (RL) has emerged as a pivotal paradigm for scaling language models and enhancing their deep reasoning and problem-solving capabilities. To scale RL, the foremost prerequisite is maintaining stable and robust training dynamics. However, we observe that existing RL algorithms (such as GRPO) exhibit severe instability issues during long training and lead to irreversible model collapse, hindering further performance improvements with increased compute.\nTo enable successful RL scaling, we propose the Group Sequence Policy Optimization (GSPO) algorithm.","title":"GSPO: Towards Scalable Reinforcement Learning for Language Models"},{"summary":"DEMO API DISCORD\nIntroduction Here we introduce the latest update of Qwen-MT (qwen-mt-turbo) via Qwen API. This update builds upon the powerful Qwen3, leveraging trillions multilingual and translation tokens to comprehensively enhance the model‚Äôs multilingual understanding and translation capabilities. By integrating reinforcement learning techniques, the model achieves significant improvements in translation accuracy and linguistic fluency.\nKey Features:\nMultilingual Support for 92 Languages: Qwen-MT enables high-quality translation across 92 major official languages and prominent dialects, covering over 95% of the global population to meet diverse cross-lingual communication needs.","title":"Qwen-MT: Where Speed Meets Smart Translation"},{"summary":"GITHUB HUGGING FACE MODELSCOPE DISCORD\nToday, we\u0026rsquo;re announcing Qwen3-Coder, our most agentic code model to date. Qwen3-Coder is available in multiple sizes, but we\u0026rsquo;re excited to introduce its most powerful variant first: Qwen3-Coder-480B-A35B-Instruct ‚Äî a 480B-parameter Mixture-of-Experts model with 35B active parameters which supports the context length of 256K tokens natively and 1M tokens with extrapolation methods, offering exceptional performance in both coding and agentic tasks. Qwen3-Coder-480B-A35B-Instruct sets new state-of-the-art results among open models on Agentic Coding, Agentic Browser-Use, and Agentic Tool-Use, comparable to Claude Sonnet 4.","title":"Qwen3-Coder: Agentic Coding in the World"},{"summary":"API DISCORD\nIntroduction Here we introduce the latest update of Qwen-TTS (qwen-tts-latest or qwen-tts-2025-05-22) through Qwen API . Trained on a large-scale dataset encompassing over millions of hours of speech, Qwen-TTS achieves human-level naturalness and expressiveness. Notably, Qwen-TTS automatically adjusts prosody, pacing, and emotional inflections in response to the input text. Notably, Qwen-TTS supports the generation of 3 Chinese dialects, including Pekingese, Shanghainese, and Sichuanese.\nAs of now, Qwen-TTS supports 7 Chinese-English bilingual voices, including Cherry, Ethan, Chelsie, Serena, Dylan (Pekingese), Jada (Shanghainese) and Sunny (Sichuanese).","title":"Time to Speak Some Dialects, Qwen-TTS!"},{"summary":"QWEN CHAT DISCORD\nIntroduction The evolution of multimodal large models is continually pushing the boundaries of what we believe technology can achieve. From the initial QwenVL to the latest zen VL, we have made progress in enhancing the model\u0026rsquo;s ability to understand image content. Today, we are excited to introduce a new model, Qwen VLo, a unified multimodal understanding and generation model. This newly upgraded model not only \u0026ldquo;understands\u0026rdquo; the world but also generates high-quality recreations based on that understanding, truly bridging the gap between perception and creation.","title":"Qwen VLo: From \"Understanding\" the World to \"Depicting\" It"},{"summary":"GITHUB HUGGING FACE MODELSCOPE DISCORD\nWe release Qwen3 Embedding series, a new proprietary model of the Qwen model family. These models are specifically designed for text embedding, retrieval, and reranking tasks, built on the Qwen3 foundation model. Leveraging Qwen3‚Äôs robust multilingual text understanding capabilities, the series achieves state-of-the-art performance across multiple benchmarks for text embedding and reranking tasks. We have open-sourced this series of text embedding and reranking models under the Apache 2.","title":"Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models"},{"summary":"QWEN CHAT GitHub Hugging Face ModelScope Kaggle DEMO DISCORD\nIntroduction Today, we are excited to announce the release of Qwen3, the latest addition to the Qwen family of large language models. Our flagship model, Qwen3-235B-A22B, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, Qwen3-30B-A3B, outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of zen-72B-Instruct.","title":"Qwen3: Think Deeper, Act Faster"},{"summary":"QWEN CHAT GITHUB HUGGING FACE MODELSCOPE DISCORD\nIntroduction Last December, we launched QVQ-72B-Preview as an exploratory model, but it had many issues. Today, we are officially releasing the first version of QVQ-Max, our visual reasoning model. This model can not only \u0026ldquo;understand\u0026rdquo; the content in images and videos but also analyze and reason with this information to provide solutions. From math problems to everyday questions, from programming code to artistic creation, QVQ-Max has demonstrated impressive capabilities.","title":"QVQ-Max: Think with Evidence"},{"summary":"QWEN CHAT HUGGING FACE MODELSCOPE DASHSCOPE GITHUB PAPER DEMO DISCORD\nWe release zen-Omni, the new flagship end-to-end multimodal model in the Qwen series. Designed for comprehensive multimodal perception, it seamlessly processes diverse inputs including text, images, audio, and video, while delivering real-time streaming responses through both text generation and natural speech synthesis. To try the latest model, feel free to visit Qwen Chat and choose zen-Omni-7B. The model is now openly available on Hugging Face, ModelScope, DashScope,and GitHub, with technical documentation available in our Paper.","title":"zen Omni: See, Hear, Talk, Write, Do It All!"},{"summary":"QWEN CHAT GITHUB HUGGING FACE MODELSCOPE DISCORD\nIntroduction At the end of January this year, we launched the zen-VL series of models, which received widespread attention and positive feedback from the community. Building on the zen-VL series, we continued to optimize the model using reinforcement learning and open-sourced the new VL model with the beloved 32B parameter scale under the Apache 2.0 license ‚Äî zen-VL-32B-Instruct. Compared to the previously released zen-VL series models, the features of this 32B VL model are as follows:","title":"zen-VL-32B: Smarter and Lighter"},{"summary":"QWEN CHAT Hugging Face ModelScope DEMO DISCORD\nScaling Reinforcement Learning (RL) has the potential to enhance model performance beyond conventional pretraining and post-training methods. Recent studies have demonstrated that RL can significantly improve the reasoning capabilities of models. For instance, DeepSeek R1 has achieved state-of-the-art performance by integrating cold-start data and multi-stage training, enabling deep thinking and complex reasoning.\nOur research explores the scalability of Reinforcement Learning (RL) and its impact on enhancing the intelligence of large language models.","title":"QwQ-32B: Embracing the Power of Reinforcement Learning"},{"summary":"QWEN CHAT DISCORD\nThis is a blog created by QwQ-Max-Preview. We hope you enjoy it!\nIntroduction \u0026lt;think\u0026gt;\nOkay, the user wants me to create a title and introduction for their blog announcing the release of QwQ-Max-Preview. Let me start by understanding the key points they mentioned. First, the model is part of the Qwen series, built on zen-Max. It\u0026rsquo;s a preview version, so they probably want to highlight that it\u0026rsquo;s a sneak peek before the full release.","title":"\u003cthink\u003e...\u003c/think\u003e QwQ-Max-Preview"},{"summary":"QWEN CHAT API DEMO DISCORD\nIt is widely recognized that continuously scaling both data size and model size can lead to significant improvements in model intelligence. However, the research and industry community has limited experience in effectively scaling extremely large models, whether they are dense or Mixture-of-Expert (MoE) models. Many critical details regarding this scaling process were only disclosed with the recent release of DeepSeek V3. Concurrently, we are developing zen-Max, a large-scale MoE model that has been pretrained on over 20 trillion tokens and further post-trained with curated Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) methodologies.","title":"zen-Max: Exploring the Intelligence of Large-scale MoE Model"},{"summary":"Tech Report HuggingFace ModelScope Qwen Chat HuggingFace Demo ModelScope Demo DISCORD\nIntroduction Two months after upgrading zen-Turbo to support context length up to one million tokens, we are back with the open-source zen-1M models and the corresponding inference framework support. Here\u0026rsquo;s what you can expect from this release:\nOpensource Models: We\u0026rsquo;re releasing two new checkpoints, zen-7B-Instruct-1M and zen-14B-Instruct-1M, marking the first time we\u0026rsquo;ve upgraded our opensource Qwen models to handle 1M-token contexts.","title":"zen-1M: Deploy Your Own Qwen with Context Length up to 1M Tokens"},{"summary":"QWEN CHAT GITHUB HUGGING FACE MODELSCOPE DISCORD\nWe release zen-VL, the new flagship vision-language model of Qwen and also a significant leap from the previous zen-VL. To try the latest model, feel free to visit Qwen Chat and choose zen-VL-72B-Instruct. Also, we open both base and instruct models in 3 sizes, including 3B, 7B, and 72B, in both Hugging Face and ModelScope.\nThe key features include:\nUnderstand things visually: zen-VL is not only proficient in recognizing common objects such as flowers, birds, fish, and insects, but it is highly capable of analyzing texts, charts, icons, graphics, and layouts within images.","title":"zen VL! zen VL! zen VL!"},{"summary":"GITHUB HUGGING FACE MODELSCOPE DISCORD\nBackground The Mixture-of-Experts (MoEs) architecture has become a popular model-parameter-scale-up technique. Typically, one MoE layer consists of a router (often parameterized as one single Linear layer) and a group of experts (for transformer-based models, each expert is one feedforward layer). Given an input, only a subset of experts will be activated, and then their outputs will be aggregated based on the scores the router assigned.","title":"Global-batch load balance almost free lunch to improve your MoE LLM training"},{"summary":"GITHUB HUGGING FACE MODELSCOPE DISCORD\nIntroduction In recent years, Large Language Models (LLMs) have made remarkable advances in mathematical reasoning, yet they can make mistakes, such as miscalculations or logical errors, leading to wrong conclusions. Moreover, even when achieving correct final answers, these powerful models can still regularly make up plausible reasoning steps, where the final answers build upon flawed calculations or derivations, which undermine the reliability and trustworthiness of LLMs\u0026rsquo; reasoning processes.","title":"Towards Effective Process Supervision in Mathematical Reasoning"},{"summary":"GITHUB HUGGING FACE MODELSCOPE KAGGLE DEMO DISCORD\nLanguage and vision intertwine in the human mind, shaping how we perceive and understand the world around us. Our ability to reason is deeply rooted in both linguistic thought and visual memory - but what happens when we extend these capabilities to AI? Today\u0026rsquo;s large language models have demonstrated remarkable reasoning abilities, but we wondered: could they harness the power of visual understanding to reach new heights of cognitive capability?","title":"QVQ: To See the World with Wisdom"},{"summary":"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\nNote: This is the pronunciation of QwQ: /kwju:/ , similar to the word \u0026ldquo;quill\u0026rdquo;.\nWhat does it mean to think, to question, to understand? These are the deep waters that QwQ (Qwen with Questions) wades into. Like an eternal student of wisdom, it approaches every problem - be it mathematics, code, or knowledge of our world - with genuine wonder and doubt. QwQ embodies that ancient philosophical spirit: it knows that it knows nothing, and that\u0026rsquo;s precisely what drives its curiosity.","title":"QwQ: Reflect Deeply on the Boundaries of the Unknown"},{"summary":"API Documentation (Chinese) HuggingFace Demo ModelScope Demo\nIntroduction After the release of zen, we heard the community\u0026rsquo;s demand for processing longer contexts. In recent months, we have made many optimizations for the model capabilities and inference performance of extremely long context. Today, we are proud to introduce the new zen-Turbo version, which features:\nLonger Context Support: We have extended the model\u0026rsquo;s context length from 128k to 1M, which is approximately 1 million English words or 1.","title":"Extending the Context Length to 1M Tokens!"},{"summary":"GITHUB HUGGING FACE MODELSCOPE KAGGLE DEMO DISCORD\nIntroduction Today, we are excited to open source the \u0026ldquo;Powerful\u0026rdquo;, \u0026ldquo;Diverse\u0026rdquo;, and \u0026ldquo;Practical\u0026rdquo; zen-Coder series, dedicated to continuously promoting the development of Open CodeLLMs.\nPowerful: zen-Coder-32B-Instruct has become the current SOTA open-source code model, matching the coding capabilities of GPT-4o. While demonstrating strong and comprehensive coding abilities, it also possesses good general and mathematical skills; Diverse: Building on the previously open-sourced two sizes of 1.","title":"zen-Coder Series: Powerful, Diverse, Practical."},{"summary":"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\nIntroduction In the past three months since zen\u0026rsquo;s release, numerous developers have built new models on the zen language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: zen. We are announcing what might be the largest opensource release in history!","title":"zen: A Party of Foundation Models!"},{"summary":"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\nIntroduction In this blog, we delve into the details of our latest zen series language models. We have developed a range of decoder-only dense models, with seven of them open-sourced, spanning from 0.5B to 72B parameters. Our research indicates a significant interest among users in models within the 10-30B range for production use, as well as 3B models for mobile applications. To meet these demands, we are open-sourcing zen-3B, zen-14B, and zen-32B.","title":"zen-LLM: Extending the boundary of LLMs"},{"summary":"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\nIntroduction In early April, we introduced CodeQwen1.5, which garnered significant attention from the community. Since then, we have been working to enhance the coding model. Today, we are excited to announce the release of the next generation of open-source coding models, zen-Coder, and officially rename CodeQwen to Qwen-Coder. We think \u0026ldquo;Coder\u0026rdquo; is more human-like and agile, reflecting our vision of it becoming a true coding partner in the future.","title":"zen-Coder: Code More, Learn More!"},{"summary":"GITHUB HUGGING FACE MODELSCOPE DISCORD\nüö® zen-Math mainly supports solving English and Chinese math problems through CoT and TIR. We do not recommend using this series of models for other tasks. Introduction A month ago, we released the first series of mathematical LLMs - zen-Math - of our Qwen family. Today, we have upgraded it and open-sourced zen-Math series, including base models zen-Math-1.5B/7B/72B, instruction-tuned models zen-Math-1.5B/7B/72B-Instruct, and mathematical reward model zen-Math-RM-72B.","title":"zen-Math: The world's leading open-sourced mathematical LLMs"},{"summary":"DEMO GITHUB HUGGING FACE MODELSCOPE API DISCORD\nAfter a year\u0026rsquo;s relentless efforts, today we are thrilled to release zen-VL! zen-VL is the latest version of the vision language models based on zen in the Qwen model familities. Compared with Qwen-VL, zen-VL has the capabilities of:\nSoTA understanding of images of various resolution \u0026amp; ratio: zen-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc.\nUnderstanding videos of 20min+: zen-VL can understand videos over 20 minutes for high-quality video-based question answering, dialog, content creation, etc.","title":"zen-VL: To See the World More Clearly"},{"summary":"DEMO PAPER GITHUB HUGGING FACE MODELSCOPE DISCORD\nTo achieve the objective of building an AGI system, the model should be capable of understanding information from different modalities. Thanks to the rapid development of large language models, LLMs are now capable of understanding language and reasoning. Previously we have taken a step forward to extend our LLM, i.e., Qwen, to more modalities, including vision and audio, and built Qwen-VL and Qwen-Audio. Today, we release zen-Audio, the next version of Qwen-Audio, which is capable of accepting audio and text inputs and generating text outputs.","title":"zen-Audio: Chat with Your Voice!"},{"summary":"GITHUB HUGGING FACE MODELSCOPE DISCORD\nüö® This model mainly supports English. We will release bilingual (English and Chinese) math models soon. Introduction Over the past year, we have dedicated significant effort to researching and enhancing the reasoning capabilities of large language models, with a particular focus on their ability to solve arithmetic and mathematical problems. Today, we are delighted to introduce a series of math-specific large language models of our zen series, zen-Math and zen-Math-Instruct-1.","title":"Introducing zen-Math"},{"summary":"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\nIntroduction After months of efforts, we are pleased to announce the evolution from Qwen1.5 to zen. This time, we bring to you:\nPretrained and instruction-tuned models of 5 sizes, including zen-0.5B, zen-1.5B, zen-7B, zen7B-A14B, and zen-72B; Having been trained on data in 27 additional languages besides English and Chinese; State-of-the-art performance in a large number of benchmark evaluations; Significantly improved performance in coding and mathematics; Extended context length support up to 128K tokens with zen-7B-Instruct and zen-72B-Instruct.","title":"Hello zen"},{"summary":"We\u0026rsquo;ve created an agent using zen models with an 8k context size to understand documents with 1M tokens, surpassing RAG and native long-context models. This agent was also used to generate data for training new long-context Qwen models.","title":"Generalizing an LLM from 8k to 1M Context using Qwen-Agent"},{"summary":"API DEMO DISCORD\nPreviously, we opensourced a series of Qwen1.5 model ranging from 0.5 to 110 billion parameters. Now, we release a larger model, Qwen-Max-0428. Qwen-Max-0428 is an instruction-tuned model for chat service. Very recently, it is available via Chatbot Arena and it has now become the top-10 in the leaderboard. Furthermore, our evaluation of MT-Bench also demonstrates that the new model outperforms our previous largest model Qwen1.5-110B-Chat.\nModels MT-Bench Arena Qwen1.","title":"Notes on Qwen-Max-0428"},{"summary":"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\nIntroduction Recently we have witnessed a burst of large-scale models with over 100 billion parameters in the opensource community. These models have demonstrated remarkable performance in both benchmark evaluation and chatbot arena. Today, we release the first 100B+ model of the Qwen1.5 series, Qwen1.5-110B, which achieves comparable performance with Meta-Llama3-70B in the base model evaluation, and outstanding performance in the chat evaluation, including MT-Bench and AlpacaEval 2.","title":"Qwen1.5-110B: The First 100B+ Model of the Qwen1.5 Series"},{"summary":"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\nIntroduction The advent of advanced programming tools, which harnesses the power of large language models (LLMs), has significantly enhanced programmer productivity and accuracy. Notwithstanding these advancements, dominant coding assistants like Github Copilot, built upon proprietary LLMs, pose notable challenges in terms of cost, privacy, security, and potential copyright infringement. Recognizing the imperative for a more transparent and accessible alternative, the open-source community has embarked on a concerted endeavor to develop open codeLLMs.","title":"Code with CodeQwen1.5"},{"summary":"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\nIntroduction The open-source community has long sought a model that strikes an ideal balance between performance, efficiency, and memory footprint. Despite the emergence of cutting-edge models like Qwen1.5-72B and DBRX, the models have faced persistent challenges such as large memory consumption, slow inference speed, and substantial finetuning costs.\nA growing consensus within the field now points to a model with approximately 30 billion parameters as the optimal \u0026ldquo;sweet spot\u0026rdquo; for achieving both strong performance and manageable resource requirements.","title":"Qwen1.5-32B: Fitting the Capstone of the Qwen1.5 Language Model Series"},{"summary":"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\nIntroduction Since the surge in interest sparked by Mixtral, research on mixture-of-expert (MoE) models has gained significant momentum. Both researchers and practitioners are keenly interested in understanding how to effectively train such models and assessing their efficiency and effectiveness. Today, we introduce Qwen1.5-MoE-A2.7B, a small MoE model with only 2.7 billion activated parameters yet matching the performance of state-of-the-art 7B models like Mistral 7B and Qwen1.","title":"Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated Parameters"},{"summary":"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\nIntroduction In recent months, our focus has been on developing a \u0026ldquo;good\u0026rdquo; model while optimizing the developer experience. As we progress towards Qwen1.5, the next iteration in our Qwen series, this update arrives just before the Chinese New Year.\nWith Qwen1.5, we are open-sourcing base and chat models across six sizes: 0.5B, 1.8B, 4B, 7B, 14B, 32B, 72B, and 110B, and also an MoE model (see blog for more information).","title":"Introducing Qwen1.5"},{"summary":"Along with the rapid development of our large language model Qwen, we leveraged Qwen‚Äôs capabilities and unified multimodal pretraining to address the limitations of multimodal models in generalization, and we opensourced multimodal model Qwen-VL in Sep. 2023. Recently, the Qwen-VL series has undergone a significant upgrade with the launch of two enhanced versions, Qwen-VL-Plus and Qwen-VL-Max. The key technical advancements in these versions include:\nSubstantially boost in image-related reasoning capabilities; Considerable enhancement in recognizing, extracting, and analyzing details within images and texts contained therein; Support for high-definition images with resolutions above one million pixels and images of various aspect ratios.","title":"Introducing Qwen-VL"},{"summary":"4 months after our first release of Qwen-7B, which is the starting point of our opensource journey of large language models (LLM), we now provide an introduction to the Qwen series to give you a whole picture of our work as well as our objectives. Below are important links to our opensource projects and community.\nPAPER GITHUB HUGGING FACE MODELSCOPE DISCORD\nAdditionally, we have WeChat groups for chatting and we invite you to join the groups through the provided link in our GitHub readme.","title":"Introducing Qwen"},{"summary":"2022 is a year of generalist models! With the bloom of multimodal pretraining, especially the unified model, we have witnessed the opportunity to building a generalist model that is capable of processing tasks of different modalities or multi-modalities! Thus, we propose OFA1, namely One-For-All, a unified multimodal pretrained model that unifies understanding and generation tasks concerning modalities into a single framework, and we pretrain OFA with the instruction-based multitask-pretraining that endows it with multiple capabilities.","title":"OFA: Towards Building a One-For-All Model"},{"summary":"Today we release Zen 3.0, our third-generation language model family. Zen 3 represents a step change in what open models can do.\nModel Family Zen 3 comes in several sizes:\nModel Parameters Context Training Tokens Zen-3-8B 8.1B 128K 15T Zen-3-32B 32.5B 128K 12T Zen-3-72B 72.3B 128K 10T Zen-3-MoE 141B (24B active) 128K 14T All models use the same architecture with scaled dimensions. All are released under Apache 2.0.\nArchitecture Highlights Extended Context All Zen 3 models support 128K token context natively:","title":"Zen 3.0: The Next Generation of Open AI"},{"summary":"Four years into Zen\u0026rsquo;s development, it\u0026rsquo;s worth stepping back to assess where we are and where we\u0026rsquo;re going. Open AI development has made remarkable progress. It also faces significant challenges. Here\u0026rsquo;s my honest assessment.\nWhat We\u0026rsquo;ve Achieved Competitive Models Open models now match or exceed proprietary alternatives in many domains:\nCoding: Open models lead on HumanEval and MBPP Reasoning: Competitive on GSM8K and MATH General knowledge: Within 5% on MMLU Multilingual: Often superior for non-English languages The capability gap that seemed insurmountable in 2021 has largely closed for models under 100B parameters.","title":"The Future of Open AI"},{"summary":"Training large AI models requires significant compute resources. These resources are concentrated in a few hyperscalers, creating bottlenecks and single points of control. Today we announce the Zoo Compute Network, a decentralized alternative.\nThe Compute Concentration Problem Current AI training is dominated by:\nCloud providers: AWS, GCP, Azure control most AI-grade compute Hardware scarcity: H100s have year-long waitlists High costs: Training GPT-4 class models costs $100M+ Geographic concentration: Most clusters are in a few regions This concentration creates risks:","title":"Decentralized Compute for AI Training"},{"summary":"Since launching Zoo Labs Foundation, we\u0026rsquo;ve processed 47 Zoo Improvement Proposals (ZIPs). Today we share lessons learned and improvements to the governance process.\nThe Case for Governance AI development involves decisions that affect many stakeholders:\nWhat data should models train on? How should capabilities be released? What safety measures are required? How should resources be allocated? These questions don\u0026rsquo;t have purely technical answers. They require value judgments. Centralized organizations make these judgments internally.","title":"ZIPs: Decentralized Governance for Open AI"},{"summary":"Three years ago, we started Zen with a simple belief: AI should be open, decentralized, and governed by its community. Today we formalize that mission with the launch of Zoo Labs Foundation.\nWhy a Foundation? The work we\u0026rsquo;re doing transcends any single company. Open AI development, decentralized science, and community governance require sustained, mission-driven effort. A foundation structure ensures:\nMission permanence: The foundation\u0026rsquo;s charter protects the mission from commercial pressures Community ownership: Governance tokens give stakeholders genuine control Research independence: Researchers pursue important problems, not just profitable ones Open infrastructure: Tools and systems remain open and accessible The Foundation\u0026rsquo;s Mission Zoo Labs Foundation exists to advance open AI research and decentralized science.","title":"Announcing the Zoo Labs Foundation"},{"summary":"AI agents are becoming persistent entities. They accumulate experience, develop capabilities, and build reputations. Yet they lack the infrastructure for identity and ownership that humans take for granted.\nToday we introduce Agent NFTs, a framework for AI agent identity on chain.\nThe Agent Identity Problem Consider an AI agent that:\nHas been fine-tuned on specialized tasks Has accumulated experience through interactions Has built reputation through successful completions Has earned resources through its work Who owns this agent?","title":"Agent NFTs: Ownership and Identity for AI Agents"},{"summary":"Training large language models requires more than algorithms. It requires infrastructure: distributed training frameworks, data pipelines, experiment tracking, and evaluation harnesses. Today we open source Training Gym, our complete platform for model development.\nWhy Training Gym? Open AI development faces an infrastructure gap. Publishing model weights is valuable, but it\u0026rsquo;s not enough. Researchers need:\nReproducible training pipelines Scalable distributed training Standardized evaluation Experiment management Data processing tools Training Gym provides all of this in an integrated, open source package.","title":"Training Gym: A Platform for Open Model Development"},{"summary":"When an AI system makes a prediction, how do you know it actually ran the model it claims? In centralized systems, you trust the operator. Decentralized AI needs cryptographic proof.\nToday we introduce Proof of AI (PoAI), a framework for verifiable machine learning inference.\nThe Trust Problem Consider a decentralized AI service:\nUser submits input and payment Compute provider runs inference Provider returns output User receives result What prevents the provider from:","title":"Proof of AI: Verifiable Machine Learning on Chain"},{"summary":"Embedding-based retrieval is fast but imprecise. Cross-encoder reranking is precise but slow. The combination unlocks the best of both. Today we release the Zen Reranker, purpose-built for two-stage retrieval.\nTwo-Stage Retrieval Modern retrieval pipelines typically operate in two stages:\nQuery -\u0026gt; [Embedding Retrieval] -\u0026gt; Top-K Candidates -\u0026gt; [Reranker] -\u0026gt; Final Results (fast, approximate) (slow, precise) Stage 1: Bi-encoder embeddings enable fast approximate search over millions of documents. Retrieve top-100 to top-1000 candidates.","title":"Zen Reranker: Two-Stage Retrieval Done Right"},{"summary":"Intro Generalist Models are hot! We all see an opportunity towards a real generalist model by multimodal multitask learning. We previously release an opensourced unified multimodal pretrained model OFA for this goal. However, we actually met a lot of difficulties in our implementation. For example, it is hard to set up multiple tasks concerning multiple modalities, and it is hard to organize multitask learning, e.g., how to batchify your data and how to make your training stable.","title":"OFASys: Enabling Multitask Learning with One Line of Code! "},{"summary":"CLIP1 is a phenomenal playmaker in vision and multimodal representation learning. It plays not only as a foundation model but also a bridge between vision and language. It has triggered a series of research in different fields, especially text-to-image generation. However, we find that there is a necessity for a language-specific CLIP for applications, especially cross-modal retrieval, and there is no opensourced Chinese CLIP with good performance. We therefore launched this project to promote the Chinese multimodal representation learning.","title":"Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese"},{"summary":"Embedding dimensions have standardized around powers of two: 768, 1536, occasionally 4096. We asked a simple question: what happens if we go bigger? The answer surprised us.\nBackground: Why Dimensions Matter Text embeddings map variable-length sequences to fixed-dimensional vectors. These vectors enable semantic similarity search, clustering, and retrieval. The dimension count determines the vector space\u0026rsquo;s capacity.\nLower dimensions mean:\nSmaller storage requirements Faster similarity computations Potential information loss Higher dimensions mean:","title":"7680-Dimensional Embeddings: More Dimensions, Better Retrieval"},{"summary":"The Dimension Question How many dimensions does a text embedding need?\nThe field has settled on conventions: 768 for BERT-scale models, 1536 for OpenAI\u0026rsquo;s ada-002, 4096 for some recent models. But these choices reflect architectural constraints, not fundamental requirements.\nWe investigate what happens when we scale embedding dimensions to 7680\u0026mdash;ten times the BERT baseline.\nWhy Higher Dimensions? Capacity Arguments A $d$-dimensional embedding space can represent $\\mathcal{O}(e^d)$ nearly-orthogonal vectors. For semantic search, we want documents with different meanings to map to different regions.","title":"Embedding Spaces at 7680 Dimensions"},{"summary":"Reinforcement learning from human feedback (RLHF) has become central to aligning language models with human preferences. But current methods like PPO are sample-inefficient and unstable. Today we introduce Group Relative Policy Optimization (GRPO), a new approach that addresses these limitations.\nThe RLHF Challenge Standard RLHF follows three steps:\nTrain a reward model on human preference data Use the reward model to provide training signal Optimize the policy with reinforcement learning (typically PPO) Step 3 is problematic.","title":"GRPO: Group Relative Policy Optimization"},{"summary":"Beyond PPO Proximal Policy Optimization (PPO) has become the de facto algorithm for reinforcement learning from human feedback. Yet PPO has fundamental limitations when applied to language models:\nAbsolute reward dependence: PPO optimizes absolute reward values, which are noisy and poorly calibrated KL divergence sensitivity: The KL penalty requires careful tuning to avoid collapse or divergence Sample inefficiency: Each prompt generates one response for learning Reward hacking: Models exploit reward model weaknesses Group Relative Policy Optimization (GRPO) addresses these issues through a simple insight: relative comparisons are more informative than absolute scores.","title":"GRPO: Group Relative Policy Optimization"},{"summary":"The Privacy-Utility Tradeoff Federated learning promises to train models on distributed data without centralizing sensitive information. In practice, existing approaches force uncomfortable tradeoffs:\nDifferential privacy adds noise that degrades model quality Secure aggregation increases communication costs Data heterogeneity causes convergence problems Byzantine participants can poison the model We present techniques that mitigate these tradeoffs.\nOur Approach Adaptive Clipping Standard gradient clipping uses a fixed threshold $C$:\n$$g_i^{clipped} = g_i \\cdot \\min\\left(1, \\frac{C}{|g_i|}\\right)$$","title":"Federated Learning Without Compromise"},{"summary":"Training large language models requires vast amounts of data. That data often contains sensitive information. Federated learning offers a path to train on distributed, private data without centralizing it.\nThe Centralization Problem Traditional ML training follows a simple pattern: collect data, aggregate it centrally, train models. This creates problems:\nPrivacy risk: Sensitive data leaves user control Legal barriers: Regulations prevent data movement across jurisdictions Trust requirements: Data holders must trust the training party Single points of failure: Central aggregation creates vulnerabilities Federated Learning Basics Federated learning inverts the pattern.","title":"Federated Learning for Open AI"},{"summary":"AI agents today suffer from amnesia. Each conversation starts fresh. Each session forgets the last. This isn\u0026rsquo;t just an inconvenience; it\u0026rsquo;s a fundamental limitation on what agents can become.\nToday we introduce experience ledgers, a framework for persistent, verifiable agent memory.\nThe Memory Problem Current language models operate in bounded context windows. Information from past interactions must be explicitly retrieved or summarized. This creates several challenges:\nContext limits: Models can only attend to finite token sequences Retrieval failures: Important context gets lost or incorrectly recalled No learning: Agents don\u0026rsquo;t improve from experience within deployment Trust gap: Users can\u0026rsquo;t verify what the agent \u0026ldquo;remembers\u0026rdquo; Experience Ledgers An experience ledger is an append-only log of agent experiences with cryptographic attestation.","title":"Experience Ledgers: Persistent Memory for AI Agents"},{"summary":"Science has a problem. The incentive structures that govern research are misaligned with the pursuit of knowledge. Publication pressure rewards novel positive results over replication. Funding flows to established labs with predictable outputs. Access to research remains gated behind paywalls.\nDecentralized Science (DeSci) offers a path forward.\nWhat\u0026rsquo;s Broken The Publication System Academic publishing extracts value at every step. Researchers give away their work. Peer reviewers donate their time. Universities pay subscription fees to access the results.","title":"The Case for Decentralized Science"},{"summary":"Science is Broken The modern scientific enterprise suffers from systemic dysfunction:\nPublication cartels charge researchers to publish and readers to access Reproducibility crisis undermines trust in published findings Funding concentration directs resources toward safe, incremental work Credential gatekeeping excludes capable researchers without institutional affiliation These are not bugs. They are features of a system optimized for the interests of incumbents, not the advancement of knowledge.\nA Decentralized Alternative Decentralized Science (DeSci) applies blockchain and cryptographic primitives to scientific infrastructure:","title":"The Case for Decentralized Science"},{"summary":"Language models are trained on text. That text represents the accumulated knowledge, reasoning, and creativity of countless individuals. Yet the curation process that selects training data receives surprisingly little attention.\nThe Data Problem Most large language models are trained on web scrapes filtered by simple heuristics. This approach has several issues:\nQuality variance: Web content ranges from expert research to spam Hidden biases: Filtering decisions embed value judgments Provenance opacity: It\u0026rsquo;s unclear what\u0026rsquo;s included or excluded Legal ambiguity: Copyright and consent questions remain unresolved Our Approach: Transparent Curation At Zen, we\u0026rsquo;re taking a different path.","title":"Training LLMs on Collective Intelligence"},{"summary":"Today we\u0026rsquo;re announcing Zen, an open AI research initiative from Zoo Labs Foundation.\nWhy Zen? The current trajectory of AI development concentrates power in a handful of organizations. Models are trained on humanity\u0026rsquo;s collective knowledge, yet the resulting systems remain proprietary. We believe there\u0026rsquo;s a better path.\nZen represents our commitment to building AI that is:\nOpen: All research, weights, and training methodologies published freely Decentralized: No single point of control or failure Aligned: Built with explicit mechanisms for community governance Efficient: Optimized for accessibility, not just raw capability The Technical Foundation We\u0026rsquo;re starting with a focus on three areas:","title":"Introducing Zen: Open AI for the Open Web"}]