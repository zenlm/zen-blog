1:"$Sreact.fragment"
2:I[106,["/_next/static/chunks/59d0ad1b64f8544e.js"],"RootProvider"]
3:I[53113,["/_next/static/chunks/4d80e004cf4896dd.js","/_next/static/chunks/350ee4303b732916.js"],"default"]
4:I[73211,["/_next/static/chunks/4d80e004cf4896dd.js","/_next/static/chunks/350ee4303b732916.js"],"default"]
5:I[10086,["/_next/static/chunks/59d0ad1b64f8544e.js"],""]
6:I[25838,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js"],"TreeContextProvider"]
b:I[6998,["/_next/static/chunks/4d80e004cf4896dd.js","/_next/static/chunks/350ee4303b732916.js"],"default"]
:HL["/_next/static/chunks/f2332aac77592f9d.css","style"]
0:{"P":null,"b":"i-dnJM_MIpJSOCQWNJVMq","c":["","docs","models","zen-live"],"q":"","i":false,"f":[[["",{"children":["docs",{"children":[["slug","models/zen-live","oc"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],[["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/chunks/f2332aac77592f9d.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","script","script-0",{"src":"/_next/static/chunks/59d0ad1b64f8544e.js","async":true,"nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"dark","suppressHydrationWarning":true,"children":["$","body",null,{"className":"antialiased","children":["$","$L2",null,{"children":["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","main",null,{"className":"flex min-h-screen flex-col items-center justify-center px-4 text-center","children":[["$","div",null,{"className":"mb-8 opacity-20","children":["$","svg",null,{"width":"120","height":"120","viewBox":"0 0 120 120","fill":"none","aria-hidden":"true","children":["$","circle",null,{"cx":"60","cy":"60","r":"50","stroke":"currentColor","strokeWidth":"3","strokeLinecap":"round","strokeDasharray":"280 40"}]}]}],["$","p",null,{"className":"text-xs font-mono uppercase tracking-[0.3em] text-fd-muted-foreground mb-4","children":"404"}],["$","h1",null,{"className":"text-3xl font-semibold mb-3","children":"Page not found"}],["$","p",null,{"className":"text-fd-muted-foreground max-w-sm mb-10","children":"This page doesn't exist, or it may have moved. Try the documentation or head home."}],["$","div",null,{"className":"flex flex-wrap gap-3 justify-center","children":[["$","$L5",null,{"href":"/","className":"inline-flex items-center gap-2 rounded-lg bg-fd-primary text-fd-primary-foreground px-4 py-2 text-sm font-medium hover:opacity-90 transition","children":"Go home"}],["$","$L5",null,{"href":"/docs","className":"inline-flex items-center gap-2 rounded-lg border border-fd-border bg-fd-background px-4 py-2 text-sm font-medium hover:bg-fd-muted transition","children":"Documentation"}],["$","$L5",null,{"href":"/docs/models","className":"inline-flex items-center gap-2 rounded-lg border border-fd-border bg-fd-background px-4 py-2 text-sm font-medium hover:bg-fd-muted transition","children":"Browse models"}]]}],["$","p",null,{"className":"mt-16 text-xs font-mono text-fd-muted-foreground opacity-50","children":"zenlm.org"}]]}],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]}]]}],{"children":[["$","$1","c",{"children":[[["$","script","script-0",{"src":"/_next/static/chunks/abb1f677d5bfa0d7.js","async":true,"nonce":"$undefined"}],["$","script","script-1",{"src":"/_next/static/chunks/73ee7cc830199358.js","async":true,"nonce":"$undefined"}],["$","script","script-2",{"src":"/_next/static/chunks/713c1b62d4d91e41.js","async":true,"nonce":"$undefined"}],["$","script","script-3",{"src":"/_next/static/chunks/ab3a9da172ad7619.js","async":true,"nonce":"$undefined"}]],["$","$L6",null,{"tree":{"$id":"root","name":"Documentation","children":[{"$id":"index.mdx","type":"page","name":"Introduction","description":"Zen LM by Hanzo AI -- frontier models for code, reasoning, vision, multimodal, embeddings, and safety","icon":"$undefined","url":"/docs","$ref":{"file":"index.mdx"}},{"$id":"_0","type":"separator","icon":"$undefined","name":"Getting Started"},{"type":"folder","name":"Getting started","root":"$undefined","defaultOpen":"$undefined","description":"$undefined","collapsible":"$undefined","children":[{"$id":"getting-started/installation.mdx","type":"page","name":"Installation","description":"Install dependencies for using Zen models","icon":"$undefined","url":"/docs/getting-started/installation","$ref":{"file":"getting-started/installation.mdx"}},{"$id":"getting-started/quickstart.mdx","type":"page","name":"Quickstart","description":"Get started with Zen models in minutes","icon":"$undefined","url":"/docs/getting-started/quickstart","$ref":{"file":"getting-started/quickstart.mdx"}}],"$id":"getting-started","$ref":"$undefined","icon":"$undefined"},{"$id":"_1","type":"separator","icon":"$undefined","name":"API"},{"type":"folder","name":"API Reference","root":"$undefined","defaultOpen":"$undefined","description":"$undefined","collapsible":"$undefined","children":[{"$id":"api/chat-completions.mdx","type":"page","name":"Chat Completions","description":"Generate text with any Zen model using the OpenAI-compatible chat completions endpoint","icon":"$undefined","url":"/docs/api/chat-completions","$ref":{"file":"api/chat-completions.mdx"}},{"$id":"api/embeddings.mdx","type":"page","name":"Embeddings","description":"Generate 3072-dimensional vector embeddings with zen3-embedding","icon":"$undefined","url":"/docs/api/embeddings","$ref":{"file":"api/embeddings.mdx"}},{"$id":"api/models.mdx","type":"page","name":"Models","description":"All Zen models -- capabilities, pricing, and recommended use cases","icon":"$undefined","url":"/docs/api/models","$ref":{"file":"api/models.mdx"}},{"$id":"api/pricing.mdx","type":"page","name":"Pricing","description":"Zen LM API pricing -- transparent at 3x upstream inference cost","icon":"$undefined","url":"/docs/api/pricing","$ref":{"file":"api/pricing.mdx"}}],"$id":"api","$ref":"$undefined","index":{"$id":"api/index.mdx","type":"page","name":"API Reference","description":"Zen LM Cloud API -- OpenAI-compatible endpoints for all Zen models","icon":"$undefined","url":"/docs/api","$ref":{"file":"api/index.mdx"}},"icon":"$undefined"},{"$id":"_2","type":"separator","icon":"$undefined","name":"Models"},{"type":"folder","name":"Models","root":"$undefined","defaultOpen":"$undefined","description":"$undefined","collapsible":"$undefined","children":[{"$id":"models/zen-3d.mdx","type":"page","name":"zen-3d","description":"3D generation model for text-to-3D and image-to-3D asset creation.","icon":"$undefined","url":"/docs/models/zen-3d","$ref":{"file":"models/zen-3d.mdx"}},{"$id":"models/zen-agent.mdx","type":"page","name":"zen-agent","description":"32B dense model with tool use and planning for agentic AI workflows.","icon":"$undefined","url":"/docs/models/zen-agent","$ref":{"file":"models/zen-agent.mdx"}},{"$id":"models/zen-artist-edit.mdx","type":"page","name":"zen-artist-edit","description":"Image editing model for inpainting, outpainting, and edit-by-instruction.","icon":"$undefined","url":"/docs/models/zen-artist-edit","$ref":{"file":"models/zen-artist-edit.mdx"}},{"$id":"models/zen-artist.mdx","type":"page","name":"zen-artist","description":"Image generation model supporting multiple styles and high-resolution output.","icon":"$undefined","url":"/docs/models/zen-artist","$ref":{"file":"models/zen-artist.mdx"}},{"$id":"models/zen-code.mdx","type":"page","name":"zen-code","description":"Legacy 14B dense code model for general programming tasks.","icon":"$undefined","url":"/docs/models/zen-code","$ref":{"file":"models/zen-code.mdx"}},{"$id":"models/zen-coder-flash.mdx","type":"page","name":"zen-coder-flash","description":"Lightweight 7B dense model for low-latency code completions.","icon":"$undefined","url":"/docs/models/zen-coder-flash","$ref":{"file":"models/zen-coder-flash.mdx"}},{"$id":"models/zen-coder.mdx","type":"page","name":"zen-coder","description":"32B dense code model with 131K context for multi-language development.","icon":"$undefined","url":"/docs/models/zen-coder","$ref":{"file":"models/zen-coder.mdx"}},{"$id":"models/zen-designer.mdx","type":"page","name":"zen-designer","description":"Design generation model for UI/UX, graphics, and visual layouts.","icon":"$undefined","url":"/docs/models/zen-designer","$ref":{"file":"models/zen-designer.mdx"}},{"$id":"models/zen-director.mdx","type":"page","name":"zen-director","description":"Text-to-video generation model with cinematic quality output.","icon":"$undefined","url":"/docs/models/zen-director","$ref":{"file":"models/zen-director.mdx"}},{"$id":"models/zen-dub-live.mdx","type":"page","name":"zen-dub-live","description":"Real-time voice synthesis with ultra-low latency for live applications.","icon":"$undefined","url":"/docs/models/zen-dub-live","$ref":{"file":"models/zen-dub-live.mdx"}},{"$id":"models/zen-dub.mdx","type":"page","name":"zen-dub","description":"Voice synthesis and multi-language dubbing model.","icon":"$undefined","url":"/docs/models/zen-dub","$ref":{"file":"models/zen-dub.mdx"}},{"$id":"models/zen-eco.mdx","type":"page","name":"zen-eco","description":"Efficient 4B dense model balancing capability and cost for general-purpose tasks.","icon":"$undefined","url":"/docs/models/zen-eco","$ref":{"file":"models/zen-eco.mdx"}},{"$id":"models/zen-embedding.mdx","type":"page","name":"zen-embedding","description":"Foundation embedding model for search and retrieval.","icon":"$undefined","url":"/docs/models/zen-embedding","$ref":{"file":"models/zen-embedding.mdx"}},{"$id":"models/zen-foley.mdx","type":"page","name":"zen-foley","description":"Sound effects generation model for text-to-SFX production.","icon":"$undefined","url":"/docs/models/zen-foley","$ref":{"file":"models/zen-foley.mdx"}},{"$id":"models/zen-guard-gen.mdx","type":"page","name":"zen-guard-gen","description":"8B dense model for safe text generation with built-in guardrails.","icon":"$undefined","url":"/docs/models/zen-guard-gen","$ref":{"file":"models/zen-guard-gen.mdx"}},{"$id":"models/zen-guard-stream.mdx","type":"page","name":"zen-guard-stream","description":"4B dense model for low-latency streaming content moderation.","icon":"$undefined","url":"/docs/models/zen-guard-stream","$ref":{"file":"models/zen-guard-stream.mdx"}},{"$id":"models/zen-guard.mdx","type":"page","name":"zen-guard","description":"Content safety and moderation classifier.","icon":"$undefined","url":"/docs/models/zen-guard","$ref":{"file":"models/zen-guard.mdx"}},{"$id":"models/zen-live.mdx","type":"page","name":"zen-live","description":"Real-time bidirectional speech translation with ultra-low latency.","icon":"$undefined","url":"/docs/models/zen-live","$ref":{"file":"models/zen-live.mdx"}},{"$id":"models/zen-max.mdx","type":"page","name":"zen-max","description":"Trillion-parameter 1.04T MoE open-weights frontier model. Same model as zen4-max.","icon":"$undefined","url":"/docs/models/zen-max","$ref":{"file":"models/zen-max.mdx"}},{"$id":"models/zen-musician.mdx","type":"page","name":"zen-musician","description":"Music generation model with multi-instrument composition and style control.","icon":"$undefined","url":"/docs/models/zen-musician","$ref":{"file":"models/zen-musician.mdx"}},{"$id":"models/zen-nano.mdx","type":"page","name":"zen-nano","description":"Ultra-compact 0.6B dense model for edge inference at 44K tokens/sec.","icon":"$undefined","url":"/docs/models/zen-nano","$ref":{"file":"models/zen-nano.mdx"}},{"$id":"models/zen-next.mdx","type":"page","name":"zen-next","description":"Next-generation preview model with cutting-edge capabilities.","icon":"$undefined","url":"/docs/models/zen-next","$ref":{"file":"models/zen-next.mdx"}},{"$id":"models/zen-omni.mdx","type":"page","name":"zen-omni","description":"72B dense hypermodal model supporting text, vision, audio, and code.","icon":"$undefined","url":"/docs/models/zen-omni","$ref":{"file":"models/zen-omni.mdx"}},{"$id":"models/zen-pro.mdx","type":"page","name":"zen-pro","description":"Professional-grade 32B dense model with 19K tokens/sec throughput.","icon":"$undefined","url":"/docs/models/zen-pro","$ref":{"file":"models/zen-pro.mdx"}},{"$id":"models/zen-reranker.mdx","type":"page","name":"zen-reranker","description":"568M dense cross-encoder model for search result reranking.","icon":"$undefined","url":"/docs/models/zen-reranker","$ref":{"file":"models/zen-reranker.mdx"}},{"$id":"models/zen-scribe.mdx","type":"page","name":"zen-scribe","description":"Speech-to-text transcription model with multi-language support.","icon":"$undefined","url":"/docs/models/zen-scribe","$ref":{"file":"models/zen-scribe.mdx"}},{"$id":"models/zen-translator.mdx","type":"page","name":"zen-translator","description":"Context-aware translation model supporting 100+ languages.","icon":"$undefined","url":"/docs/models/zen-translator","$ref":{"file":"models/zen-translator.mdx"}},{"$id":"models/zen-video-i2v.mdx","type":"page","name":"zen-video-i2v","description":"Image-to-video animation model that brings still images to life.","icon":"$undefined","url":"/docs/models/zen-video-i2v","$ref":{"file":"models/zen-video-i2v.mdx"}},{"$id":"models/zen-video.mdx","type":"page","name":"zen-video","description":"Video understanding model for frame analysis, captioning, and temporal reasoning.","icon":"$undefined","url":"/docs/models/zen-video","$ref":{"file":"models/zen-video.mdx"}},{"$id":"models/zen-vl.mdx","type":"page","name":"zen-vl","description":"32B dense multimodal model for vision-language understanding.","icon":"$undefined","url":"/docs/models/zen-vl","$ref":{"file":"models/zen-vl.mdx"}},{"$id":"models/zen-voyager.mdx","type":"page","name":"zen-voyager","description":"World model for spatial reasoning and 3D scene understanding.","icon":"$undefined","url":"/docs/models/zen-voyager","$ref":{"file":"models/zen-voyager.mdx"}},{"$id":"models/zen-world.mdx","type":"page","name":"zen-world","description":"World simulation model for spatial reasoning and environment generation.","icon":"$undefined","url":"/docs/models/zen-world","$ref":{"file":"models/zen-world.mdx"}},{"$id":"models/zen.mdx","type":"page","name":"zen","description":"Standard 8-32B dense foundation model for general-purpose AI tasks.","icon":"$undefined","url":"/docs/models/zen","$ref":{"file":"models/zen.mdx"}},{"$id":"models/zen3-asr-v1.mdx","type":"page","name":"zen3-asr-v1","description":"First-generation streaming ASR for legacy compatibility.","icon":"$undefined","url":"/docs/models/zen3-asr-v1","$ref":{"file":"models/zen3-asr-v1.mdx"}},{"$id":"models/zen3-asr.mdx","type":"page","name":"zen3-asr","description":"Real-time streaming speech recognition for live transcription and voice agents.","icon":"$undefined","url":"/docs/models/zen3-asr","$ref":{"file":"models/zen3-asr.mdx"}},{"$id":"models/zen3-audio-fast.mdx","type":"page","name":"zen3-audio-fast","description":"Fastest speech-to-text transcription for high-throughput workloads.","icon":"$undefined","url":"/docs/models/zen3-audio-fast","$ref":{"file":"models/zen3-audio-fast.mdx"}},{"$id":"models/zen3-audio.mdx","type":"page","name":"zen3-audio","description":"Best quality speech-to-text transcription. 100+ languages.","icon":"$undefined","url":"/docs/models/zen3-audio","$ref":{"file":"models/zen3-audio.mdx"}},{"$id":"models/zen3-embedding-medium.mdx","type":"page","name":"zen3-embedding-medium","description":"Balanced embedding model for cost-effective retrieval workloads.","icon":"$undefined","url":"/docs/models/zen3-embedding-medium","$ref":{"file":"models/zen3-embedding-medium.mdx"}},{"$id":"models/zen3-embedding-small.mdx","type":"page","name":"zen3-embedding-small","description":"Lightweight embedding model for high-throughput, low-cost applications.","icon":"$undefined","url":"/docs/models/zen3-embedding-small","$ref":{"file":"models/zen3-embedding-small.mdx"}},{"$id":"models/zen3-embedding.mdx","type":"page","name":"zen3-embedding","description":"High-quality text embedding model with 3072 dimensions and 8K context.","icon":"$undefined","url":"/docs/models/zen3-embedding","$ref":{"file":"models/zen3-embedding.mdx"}},{"$id":"models/zen3-guard.mdx","type":"page","name":"zen3-guard","description":"Content safety classifier with 4B dense architecture. 65K context.","icon":"$undefined","url":"/docs/models/zen3-guard","$ref":{"file":"models/zen3-guard.mdx"}},{"$id":"models/zen3-image-dev.mdx","type":"page","name":"zen3-image-dev","description":"Development model for experimentation and iteration.","icon":"$undefined","url":"/docs/models/zen3-image-dev","$ref":{"file":"models/zen3-image-dev.mdx"}},{"$id":"models/zen3-image-fast.mdx","type":"page","name":"zen3-image-fast","description":"Fastest image model for real-time generation.","icon":"$undefined","url":"/docs/models/zen3-image-fast","$ref":{"file":"models/zen3-image-fast.mdx"}},{"$id":"models/zen3-image-jp.mdx","type":"page","name":"zen3-image-jp","description":"Japanese-specialized image generation model.","icon":"$undefined","url":"/docs/models/zen3-image-jp","$ref":{"file":"models/zen3-image-jp.mdx"}},{"$id":"models/zen3-image-max.mdx","type":"page","name":"zen3-image-max","description":"Maximum quality image generation for professional creative work.","icon":"$undefined","url":"/docs/models/zen3-image-max","$ref":{"file":"models/zen3-image-max.mdx"}},{"$id":"models/zen3-image-playground.mdx","type":"page","name":"zen3-image-playground","description":"Aesthetic model for artistic image generation.","icon":"$undefined","url":"/docs/models/zen3-image-playground","$ref":{"file":"models/zen3-image-playground.mdx"}},{"$id":"models/zen3-image-sdxl.mdx","type":"page","name":"zen3-image-sdxl","description":"High-resolution image generation at 1024px.","icon":"$undefined","url":"/docs/models/zen3-image-sdxl","$ref":{"file":"models/zen3-image-sdxl.mdx"}},{"$id":"models/zen3-image-ssd.mdx","type":"page","name":"zen3-image-ssd","description":"Fastest diffusion model for real-time generation.","icon":"$undefined","url":"/docs/models/zen3-image-ssd","$ref":{"file":"models/zen3-image-ssd.mdx"}},{"$id":"models/zen3-image.mdx","type":"page","name":"zen3-image","description":"Best general-purpose image generation.","icon":"$undefined","url":"/docs/models/zen3-image","$ref":{"file":"models/zen3-image.mdx"}},{"$id":"models/zen3-nano.mdx","type":"page","name":"zen3-nano","description":"Ultra-lightweight 4B dense model for edge deployment. 40K context.","icon":"$undefined","url":"/docs/models/zen3-nano","$ref":{"file":"models/zen3-nano.mdx"}},{"$id":"models/zen3-omni.mdx","type":"page","name":"zen3-omni","description":"Hypermodal ~200B dense model supporting text, vision, and audio. 202K context.","icon":"$undefined","url":"/docs/models/zen3-omni","$ref":{"file":"models/zen3-omni.mdx"}},{"$id":"models/zen3-reranker-medium.mdx","type":"page","name":"zen3-reranker-medium","description":"Balanced reranker for cost-effective retrieval quality improvement.","icon":"$undefined","url":"/docs/models/zen3-reranker-medium","$ref":{"file":"models/zen3-reranker-medium.mdx"}},{"$id":"models/zen3-reranker-small.mdx","type":"page","name":"zen3-reranker-small","description":"Lightweight reranker for high-throughput reranking at minimal cost.","icon":"$undefined","url":"/docs/models/zen3-reranker-small","$ref":{"file":"models/zen3-reranker-small.mdx"}},{"$id":"models/zen3-reranker.mdx","type":"page","name":"zen3-reranker","description":"High-quality reranker for improving retrieval accuracy in RAG pipelines.","icon":"$undefined","url":"/docs/models/zen3-reranker","$ref":{"file":"models/zen3-reranker.mdx"}},{"$id":"models/zen3-tts-fast.mdx","type":"page","name":"zen3-tts-fast","description":"Low-latency text-to-speech for real-time voice agents and interactive applications.","icon":"$undefined","url":"/docs/models/zen3-tts-fast","$ref":{"file":"models/zen3-tts-fast.mdx"}},{"$id":"models/zen3-tts-hd.mdx","type":"page","name":"zen3-tts-hd","description":"Maximum fidelity text-to-speech for broadcast-quality audio production.","icon":"$undefined","url":"/docs/models/zen3-tts-hd","$ref":{"file":"models/zen3-tts-hd.mdx"}},{"$id":"models/zen3-tts.mdx","type":"page","name":"zen3-tts","description":"High-quality text-to-speech with natural prosody. 40+ voices, 8 languages.","icon":"$undefined","url":"/docs/models/zen3-tts","$ref":{"file":"models/zen3-tts.mdx"}},{"$id":"models/zen3-vl.mdx","type":"page","name":"zen3-vl","description":"Vision-language model with 30B (3B active) MoE architecture. 262K context.","icon":"$undefined","url":"/docs/models/zen3-vl","$ref":{"file":"models/zen3-vl.mdx"}},{"$id":"models/zen4-coder-flash.mdx","type":"page","name":"zen4-coder-flash","description":"Fast 30B (3B active) MoE code model with 262K context.","icon":"$undefined","url":"/docs/models/zen4-coder-flash","$ref":{"file":"models/zen4-coder-flash.mdx"}},{"$id":"models/zen4-coder-pro.mdx","type":"page","name":"zen4-coder-pro","description":"Premium 480B full-precision BF16 code model with 131K context.","icon":"$undefined","url":"/docs/models/zen4-coder-pro","$ref":{"file":"models/zen4-coder-pro.mdx"}},{"$id":"models/zen4-coder.mdx","type":"page","name":"zen4-coder","description":"Code generation model with 480B (35B active) MoE and 163K context.","icon":"$undefined","url":"/docs/models/zen4-coder","$ref":{"file":"models/zen4-coder.mdx"}},{"$id":"models/zen4-max.mdx","type":"page","name":"zen4-max","description":"Trillion-parameter frontier MoE model. 1.04T (32B active) with 163K context.","icon":"$undefined","url":"/docs/models/zen4-max","$ref":{"file":"models/zen4-max.mdx"}},{"$id":"models/zen4-mini.mdx","type":"page","name":"zen4-mini","description":"Fast and efficient 8B dense model with 40K context.","icon":"$undefined","url":"/docs/models/zen4-mini","$ref":{"file":"models/zen4-mini.mdx"}},{"$id":"models/zen4-pro.mdx","type":"page","name":"zen4-pro","description":"High capability MoE model. 80B (3B active) with 131K context.","icon":"$undefined","url":"/docs/models/zen4-pro","$ref":{"file":"models/zen4-pro.mdx"}},{"$id":"models/zen4-thinking.mdx","type":"page","name":"zen4-thinking","description":"Deep reasoning model with 80B (3B active) MoE + chain-of-thought. 131K context.","icon":"$undefined","url":"/docs/models/zen4-thinking","$ref":{"file":"models/zen4-thinking.mdx"}},{"$id":"models/zen4-ultra.mdx","type":"page","name":"zen4-ultra","description":"Maximum reasoning model with 744B MoE (40B active) + extended chain-of-thought. 262K context.","icon":"$undefined","url":"/docs/models/zen4-ultra","$ref":{"file":"models/zen4-ultra.mdx"}},{"$id":"models/zen4.1.mdx","type":"page","name":"zen4.1","description":"High-performance 1M context model for long-document analysis, large codebase reasoning, and agentic workflows. Best balance of intelligence and cost at million-token scale.","icon":"$undefined","url":"/docs/models/zen4.1","$ref":{"file":"models/zen4.1.mdx"}},{"$id":"models/zen4.mdx","type":"page","name":"zen4","description":"Flagship 744B MoE model with 40B active parameters and 202K context window.","icon":"$undefined","url":"/docs/models/zen4","$ref":{"file":"models/zen4.mdx"}},{"$id":"models/zen5-coder.mdx","type":"page","name":"zen5-coder","description":"Next-generation code model. Zen5 architecture optimized for agentic programming. Research Preview.","icon":"$undefined","url":"/docs/models/zen5-coder","$ref":{"file":"models/zen5-coder.mdx"}},{"$id":"models/zen5-max.mdx","type":"page","name":"zen5-max","description":"Maximum-scale Zen5 model. Multi-trillion parameter frontier MoE. Research Preview.","icon":"$undefined","url":"/docs/models/zen5-max","$ref":{"file":"models/zen5-max.mdx"}},{"$id":"models/zen5-mini.mdx","type":"page","name":"zen5-mini","description":"Efficient agentic model delivering zen5-class intelligence at a fraction of the cost.","icon":"$undefined","url":"/docs/models/zen5-mini","$ref":{"file":"models/zen5-mini.mdx"}},{"$id":"models/zen5-pro.mdx","type":"page","name":"zen5-pro","description":"High-throughput agentic model for demanding production workloads. Trained on real-world development patterns with deep chain-of-thought reasoning.","icon":"$undefined","url":"/docs/models/zen5-pro","$ref":{"file":"models/zen5-pro.mdx"}},{"$id":"models/zen5-ultra.mdx","type":"page","name":"zen5-ultra","description":"Deepest reasoning model in the Zen family. Multi-pass chain-of-thought with self-verification.","icon":"$undefined","url":"/docs/models/zen5-ultra","$ref":{"file":"models/zen5-ultra.mdx"}},{"$id":"models/zen5.mdx","type":"page","name":"zen5","description":"Next-generation flagship model. 2T+ MoE with on-chain training via NVIDIA TEE. Research Preview.","icon":"$undefined","url":"/docs/models/zen5","$ref":{"file":"models/zen5.mdx"}}],"$id":"models","$ref":"$undefined","icon":"$undefined"},{"type":"folder","name":"Training","root":"$undefined","defaultOpen":"$undefined","description":"$undefined","collapsible":"$undefined","children":[{"$id":"training/cloud.mdx","type":"page","name":"Cloud Training","description":"Full-scale training on 8x H200 GPUs","icon":"$undefined","url":"/docs/training/cloud","$ref":{"file":"training/cloud.mdx"}},{"$id":"training/cuda.mdx","type":"page","name":"CUDA Training","description":"Train locally with NVIDIA GPUs","icon":"$undefined","url":"/docs/training/cuda","$ref":{"file":"training/cuda.mdx"}},{"$id":"training/mlx.mdx","type":"page","name":"MLX Training","description":"Train on Apple Silicon with MLX","icon":"$undefined","url":"/docs/training/mlx","$ref":{"file":"training/mlx.mdx"}},{"$id":"training/overview.mdx","type":"page","name":"Training Overview","description":"Train Zen models with multiple backend options","icon":"$undefined","url":"/docs/training/overview","$ref":{"file":"training/overview.mdx"}}],"$id":"training","$ref":"$undefined","icon":"$undefined"},{"$id":"datasets.mdx","type":"page","name":"Zen Agentic Dataset","description":"Zen Agentic Dataset - 8.47 billion tokens of real-world agentic programming","icon":"$undefined","url":"/docs/datasets","$ref":{"file":"datasets.mdx"}}],"fallback":{"$id":"fallback:root","name":"Docs","children":[{"$id":"fallback:models.mdx","type":"page","name":"Models","description":"Complete Zen LM model family â€” 80+ models across text, code, vision, audio, image, video, 3D, embedding, safety, and agent modalities","icon":"$undefined","url":"/docs/models","$ref":{"file":"models.mdx"}},{"$id":"fallback:training.mdx","type":"page","name":"Fine-tuning Guide","description":"Fine-tune Zen4 models with MLX, Unsloth, or DeepSpeed","icon":"$undefined","url":"/docs/training","$ref":{"file":"training.mdx"}}]}},"children":"$L7"}]]}],{"children":["$L8",{"children":["$L9",{},null,false,false]},null,false,false]},null,false,false]},null,false,false],"$La",false]],"m":"$undefined","G":["$b",[]],"S":true}
c:I[99924,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js"],"LayoutContextProvider"]
d:I[32824,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js"],"SidebarProvider"]
e:I[99924,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js"],"LayoutBody"]
f:I[99924,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js"],"LayoutHeader"]
10:I[48068,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js","/_next/static/chunks/ee7e2f8a8bb143f0.js","/_next/static/chunks/10b41d05c80e617b.js"],"default"]
11:I[80761,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js"],"SearchToggle"]
12:I[32824,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js"],"SidebarTrigger"]
13:I[80157,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js"],"SidebarContent"]
14:I[32824,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js"],"SidebarCollapseTrigger"]
15:I[80761,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js"],"LargeSearchToggle"]
16:I[32824,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js"],"SidebarViewport"]
17:I[80157,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js"],"SidebarLinkItem"]
18:I[80157,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js"],"SidebarPageTree"]
19:I[73332,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js"],"ThemeToggle"]
1a:I[80157,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js"],"SidebarDrawer"]
1d:I[89923,["/_next/static/chunks/4d80e004cf4896dd.js","/_next/static/chunks/350ee4303b732916.js"],"OutletBoundary"]
1e:"$Sreact.suspense"
20:I[89923,["/_next/static/chunks/4d80e004cf4896dd.js","/_next/static/chunks/350ee4303b732916.js"],"ViewportBoundary"]
22:I[89923,["/_next/static/chunks/4d80e004cf4896dd.js","/_next/static/chunks/350ee4303b732916.js"],"MetadataBoundary"]
7:["$","$Lc",null,{"navTransparentMode":"$undefined","children":["$","$Ld",null,{"defaultOpenLevel":"$undefined","prefetch":"$undefined","children":["$","$Le",null,{"children":[["$","$Lf",null,{"id":"nd-subnav","className":"[grid-area:header] sticky top-(--fd-docs-row-1) z-30 flex items-center ps-4 pe-2.5 border-b transition-colors backdrop-blur-sm h-(--fd-header-height) md:hidden max-md:layout:[--fd-header-height:--spacing(14)] data-[transparent=false]:bg-fd-background/80","children":[["$","$L10",null,{"href":"/","className":"inline-flex items-center gap-2.5 font-semibold","children":"ðŸª· Zen LM"}],["$","div",null,{"className":"flex-1","children":"$undefined"}],["$","$L11",null,{"className":"p-2","hideIfDisabled":true}],["$","$L12",null,{"className":"inline-flex items-center justify-center rounded-md text-sm font-medium transition-colors duration-100 disabled:pointer-events-none disabled:opacity-50 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-fd-ring hover:bg-fd-accent hover:text-fd-accent-foreground [&_svg]:size-4.5 p-2","children":["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-panel-left","aria-hidden":"true","children":[["$","rect","afitv7",{"width":"18","height":"18","x":"3","y":"3","rx":"2"}],["$","path","fh3hqa",{"d":"M9 3v18"}],"$undefined"]}]}]]}],[["$","$L13",null,{"children":[["$","div",null,{"className":"flex flex-col gap-3 p-4 pb-2","children":[["$","div",null,{"className":"flex","children":[["$","$L10",null,{"href":"/","className":"inline-flex text-[0.9375rem] items-center gap-2.5 font-medium me-auto","children":"ðŸª· Zen LM"}],"$undefined",["$","$L14",null,{"className":"inline-flex items-center justify-center rounded-md text-sm font-medium transition-colors duration-100 disabled:pointer-events-none disabled:opacity-50 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-fd-ring hover:bg-fd-accent hover:text-fd-accent-foreground p-1.5 [&_svg]:size-4.5 mb-auto text-fd-muted-foreground","children":["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-panel-left","aria-hidden":"true","children":[["$","rect","afitv7",{"width":"18","height":"18","x":"3","y":"3","rx":"2"}],["$","path","fh3hqa",{"d":"M9 3v18"}],"$undefined"]}]}]]}],["$","$L15",null,{"hideIfDisabled":true}],false,["$","div",null,{"className":"p-3 rounded-lg bg-primary/10 text-sm","children":[["$","strong",null,{"children":"zen4-max"}]," â€” 1T+ MoE frontier model"]}]]}],["$","$L16",null,{"children":[[["$","$L17","0",{"item":{"text":"HuggingFace","url":"https://huggingface.co/zenlm"},"className":""}],["$","$L17","1",{"item":{"text":"GitHub","url":"https://github.com/zenlm"},"className":"mb-4"}]],["$","$L18",null,{}]]}],["$","div",null,{"className":"flex flex-col border-t p-4 pt-2 empty:hidden","children":[["$","div",null,{"className":"flex text-fd-muted-foreground items-center empty:hidden","children":[false,[],["$","$L19",null,{"className":"ms-auto p-0","mode":"$undefined"}]]}],"$undefined"]}]]}],["$","$L1a",null,{"children":[["$","div",null,{"className":"flex flex-col gap-3 p-4 pb-2","children":[["$","div",null,{"className":"flex text-fd-muted-foreground items-center gap-1.5","children":[["$","div",null,{"className":"flex flex-1","children":[]}],false,["$","$L19",null,{"className":"p-0","mode":"$undefined"}],["$","$L12",null,{"className":"inline-flex items-center justify-center rounded-md text-sm font-medium transition-colors duration-100 disabled:pointer-events-none disabled:opacity-50 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-fd-ring hover:bg-fd-accent hover:text-fd-accent-foreground [&_svg]:size-4.5 p-2","children":["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-panel-left","aria-hidden":"true","children":[["$","rect","afitv7",{"width":"18","height":"18","x":"3","y":"3","rx":"2"}],["$","path","fh3hqa",{"d":"M9 3v18"}],"$undefined"]}]}]]}],false,"$7:props:children:props:children:props:children:1:0:props:children:0:props:children:3"]}],"$7:props:children:props:children:props:children:1:0:props:children:1",["$","div",null,{"className":"flex flex-col border-t p-4 pt-2 empty:hidden","children":"$undefined"}]]}]],false,"$L1b"]}]}]}]
8:["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}]
9:["$","$1","c",{"children":["$L1c",[["$","script","script-0",{"src":"/_next/static/chunks/ee7e2f8a8bb143f0.js","async":true,"nonce":"$undefined"}],["$","script","script-1",{"src":"/_next/static/chunks/10b41d05c80e617b.js","async":true,"nonce":"$undefined"}]],["$","$L1d",null,{"children":["$","$1e",null,{"name":"Next.MetadataOutlet","children":"$@1f"}]}]]}]
a:["$","$1","h",{"children":[null,["$","$L20",null,{"children":"$L21"}],["$","div",null,{"hidden":true,"children":["$","$L22",null,{"children":["$","$1e",null,{"name":"Next.Metadata","children":"$L23"}]}]}],null]}]
1b:["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]
24:I[70258,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js","/_next/static/chunks/ee7e2f8a8bb143f0.js","/_next/static/chunks/10b41d05c80e617b.js"],"DocsPage"]
25:I[70258,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js","/_next/static/chunks/ee7e2f8a8bb143f0.js","/_next/static/chunks/10b41d05c80e617b.js"],"DocsTitle"]
26:I[70258,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js","/_next/static/chunks/ee7e2f8a8bb143f0.js","/_next/static/chunks/10b41d05c80e617b.js"],"DocsDescription"]
27:I[70258,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js","/_next/static/chunks/ee7e2f8a8bb143f0.js","/_next/static/chunks/10b41d05c80e617b.js"],"DocsBody"]
1c:["$","$L24",null,{"toc":[{"depth":1,"url":"#zen-live","title":"zen-live"},{"depth":2,"url":"#specifications","title":"Specifications"},{"depth":2,"url":"#capabilities","title":"Capabilities"},{"depth":2,"url":"#usage","title":"Usage"},{"depth":2,"url":"#see-also","title":"See Also"}],"children":[["$","$L25",null,{"children":"zen-live"}],["$","$L26",null,{"children":"Real-time bidirectional speech translation with ultra-low latency."}],["$","$L27",null,{"children":[["$","h1",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"zen-live","children":[["$","a",null,{"data-card":"","href":"#zen-live","className":"peer","children":"zen-live"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}],"\n",["$","p",null,{"children":["$","strong",null,{"children":"Real-Time Translation"}]}],"\n",["$","p",null,{"children":"A real-time bidirectional translation model that transcribes, translates, and speaks in a single streaming pipeline. Enables live cross-language conversations with minimal latency."}],"\n",["$","blockquote",null,{"children":["\n",["$","p",null,{"children":["This model is coming soon. Join the waitlist at ",["$","$L10",null,{"href":"https://hanzo.chat","children":"hanzo.chat"}],"."]}],"\n"]}],"\n",["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"specifications","children":[["$","a",null,{"data-card":"","href":"#specifications","className":"peer","children":"Specifications"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}],"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",["$","div",null,{"className":"relative overflow-auto prose-no-margin my-6","children":["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"Property"}],["$","th",null,{"children":"Value"}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"Model ID"}]}],["$","td",null,{"children":["$","code",null,{"children":"zen-live"}]}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"Architecture"}]}],["$","td",null,{"children":"Streaming Multimodal Transformer"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"Latency"}]}],["$","td",null,{"children":"< 500ms end-to-end"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"Languages"}]}],["$","td",null,{"children":"50+"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"Status"}]}],["$","td",null,{"children":"Coming Soon"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"HuggingFace"}]}],["$","td",null,{"children":"--"}]]}]]}]]}]}],"\n",["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"capabilities","children":[["$","a",null,{"data-card":"","href":"#capabilities","className":"peer","children":"Capabilities"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":"Real-time speech-to-speech translation"}],"\n","$L28","\n","$L29","\n","$L2a","\n","$L2b","\n","$L2c","\n"]}],"\n","$L2d","\n","$L2e","\n","$L2f","\n","$L30"]}]]}]
31:I[51504,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js","/_next/static/chunks/ee7e2f8a8bb143f0.js","/_next/static/chunks/10b41d05c80e617b.js"],"CodeBlock"]
33:I[51504,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js","/_next/static/chunks/ee7e2f8a8bb143f0.js","/_next/static/chunks/10b41d05c80e617b.js"],"Pre"]
28:["$","li",null,{"children":"Bidirectional conversation mode"}]
29:["$","li",null,{"children":"Sub-500ms end-to-end latency"}]
2a:["$","li",null,{"children":"Tone and intent preservation"}]
2b:["$","li",null,{"children":"Speaker voice adaptation"}]
2c:["$","li",null,{"children":"Multi-party conference translation"}]
2d:["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"usage","children":[["$","a",null,{"data-card":"","href":"#usage","className":"peer","children":"Usage"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
32:T5c0,<svg viewBox="0 0 24 24"><path d="M14.25.18l.9.2.73.26.59.3.45.32.34.34.25.34.16.33.1.3.04.26.02.2-.01.13V8.5l-.05.63-.13.55-.21.46-.26.38-.3.31-.33.25-.35.19-.35.14-.33.1-.3.07-.26.04-.21.02H8.77l-.69.05-.59.14-.5.22-.41.27-.33.32-.27.35-.2.36-.15.37-.1.35-.07.32-.04.27-.02.21v3.06H3.17l-.21-.03-.28-.07-.32-.12-.35-.18-.36-.26-.36-.36-.35-.46-.32-.59-.28-.73-.21-.88-.14-1.05-.05-1.23.06-1.22.16-1.04.24-.87.32-.71.36-.57.4-.44.42-.33.42-.24.4-.16.36-.1.32-.05.24-.01h.16l.06.01h8.16v-.83H6.18l-.01-2.75-.02-.37.05-.34.11-.31.17-.28.25-.26.31-.23.38-.2.44-.18.51-.15.58-.12.64-.1.71-.06.77-.04.84-.02 1.27.05zm-6.3 1.98l-.23.33-.08.41.08.41.23.34.33.22.41.09.41-.09.33-.22.23-.34.08-.41-.08-.41-.23-.33-.33-.22-.41-.09-.41.09zm13.09 3.95l.28.06.32.12.35.18.36.27.36.35.35.47.32.59.28.73.21.88.14 1.04.05 1.23-.06 1.23-.16 1.04-.24.86-.32.71-.36.57-.4.45-.42.33-.42.24-.4.16-.36.09-.32.05-.24.02-.16-.01h-8.22v.82h5.84l.01 2.76.02.36-.05.34-.11.31-.17.29-.25.25-.31.24-.38.2-.44.17-.51.15-.58.13-.64.09-.71.07-.77.04-.84.01-1.27-.04-1.07-.14-.9-.2-.73-.25-.59-.3-.45-.33-.34-.34-.25-.34-.16-.33-.1-.3-.04-.25-.02-.2.01-.13v-5.34l.05-.64.13-.54.21-.46.26-.38.3-.32.33-.24.35-.2.35-.14.33-.1.3-.06.26-.04.21-.02.13-.01h5.84l.69-.05.59-.14.5-.21.41-.28.33-.32.27-.35.2-.36.15-.36.1-.35.07-.32.04-.28.02-.21V6.07h2.09l.14.01zm-6.47 14.25l-.23.33-.08.41.08.41.23.33.33.23.41.08.41-.08.33-.23.23-.33.08-.41-.08-.41-.23-.33-.33-.23-.41-.08-.41.08z" fill="currentColor" /></svg>2e:["$","$L31",null,{"className":"shiki shiki-themes github-light github-dark","style":{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},"tabIndex":"0","icon":"$32","children":["$","$L33",null,{"children":["$","code",null,{"children":[["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"from"}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":" hanzoai "}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"import"}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":" Hanzo"}]]}],"\n",["$","span",null,{"className":"line"}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"client "}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"="}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":" Hanzo("}],["$","span",null,{"style":{"--shiki-light":"#E36209","--shiki-dark":"#FFAB70"},"children":"api_key"}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"="}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\"hk-your-api-key\""}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":")"}]]}],"\n",["$","span",null,{"className":"line"}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#6A737D","--shiki-dark":"#6A737D"},"children":"# Coming soon -- real-time streaming API"}]}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"session "}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"="}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":" client.realtime.create("}]]}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#E36209","--shiki-dark":"#FFAB70"},"children":"    model"}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"="}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\"zen-live\""}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":","}]]}],"\n",["$","span",null,{"className":"line","children":["$L34","$L35","$L36","$L37"]}],"\n","$L38","\n","$L39","\n","$L3a","\n","$L3b","\n","$L3c","\n","$L3d"]}]}]}]
2f:["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"see-also","children":[["$","a",null,{"data-card":"","href":"#see-also","className":"peer","children":"See Also"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
30:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","$L10",null,{"href":"/docs/models/zen-translator","children":"zen-translator"}]," -- Text translation (100+ languages)"]}],"\n",["$","li",null,{"children":[["$","$L10",null,{"href":"/docs/models/zen-scribe","children":"zen-scribe"}]," -- Speech-to-text transcription"]}],"\n",["$","li",null,{"children":[["$","$L10",null,{"href":"/docs/models/zen-dub-live","children":"zen-dub-live"}]," -- Real-time voice synthesis"]}],"\n"]}]
34:["$","span",null,{"style":{"--shiki-light":"#E36209","--shiki-dark":"#FFAB70"},"children":"    source_language"}]
35:["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"="}]
36:["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\"en\""}]
37:["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":","}]
38:["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#E36209","--shiki-dark":"#FFAB70"},"children":"    target_language"}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"="}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\"ja\""}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":","}]]}]
39:["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":")"}]}]
3a:["$","span",null,{"className":"line"}]
3b:["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#6A737D","--shiki-dark":"#6A737D"},"children":"# Stream audio in, get translated audio out"}]}]
3c:["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"for"}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":" translated_audio "}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"in"}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":" session.stream(microphone_input):"}]]}]
3d:["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"    speaker.play(translated_audio)"}]}]
21:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","2",{"name":"theme-color","media":"(prefers-color-scheme: dark)","content":"#0A0A0A"}],["$","meta","3",{"name":"theme-color","media":"(prefers-color-scheme: light)","content":"#fff"}]]
1f:null
23:[["$","title","0",{"children":"zen-live | Zen LM"}],["$","meta","1",{"name":"description","content":"Real-time bidirectional speech translation with ultra-low latency."}],["$","meta","2",{"property":"og:title","content":"Zen LM - Open Foundation Models"}],["$","meta","3",{"property":"og:description","content":"Zen AI model family by Hanzo AI. 14 frontier models from 4B to 1T+ parameters for code, reasoning, vision, and multimodal tasks."}],["$","meta","4",{"property":"og:url","content":"https://zenlm.org"}],["$","meta","5",{"property":"og:site_name","content":"Zen LM"}],["$","meta","6",{"property":"og:type","content":"website"}],["$","meta","7",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","8",{"name":"twitter:site","content":"@zenlmorg"}],["$","meta","9",{"name":"twitter:creator","content":"@zenlmorg"}],["$","meta","10",{"name":"twitter:title","content":"Zen LM - Open Foundation Models"}],["$","meta","11",{"name":"twitter:description","content":"Zen AI model family by Hanzo AI. 14 frontier models from 4B to 1T+ parameters for code, reasoning, vision, and multimodal tasks."}]]
