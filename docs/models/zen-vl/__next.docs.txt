1:"$Sreact.fragment"
2:I[25838,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js"],"TreeContextProvider"]
4:I[99924,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js"],"LayoutContextProvider"]
5:I[32824,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js"],"SidebarProvider"]
6:I[99924,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js"],"LayoutBody"]
7:I[99924,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js"],"LayoutHeader"]
8:I[48068,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js","/_next/static/chunks/ee7e2f8a8bb143f0.js","/_next/static/chunks/10b41d05c80e617b.js"],"default"]
9:I[80761,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js"],"SearchToggle"]
a:I[32824,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js"],"SidebarTrigger"]
b:I[80157,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js"],"SidebarContent"]
c:I[32824,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js"],"SidebarCollapseTrigger"]
d:I[80761,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js"],"LargeSearchToggle"]
e:I[32824,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js"],"SidebarViewport"]
f:I[80157,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js"],"SidebarLinkItem"]
10:I[80157,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js"],"SidebarPageTree"]
11:I[73332,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js"],"ThemeToggle"]
12:I[80157,["/_next/static/chunks/59d0ad1b64f8544e.js","/_next/static/chunks/abb1f677d5bfa0d7.js","/_next/static/chunks/73ee7cc830199358.js","/_next/static/chunks/713c1b62d4d91e41.js","/_next/static/chunks/ab3a9da172ad7619.js"],"SidebarDrawer"]
13:I[53113,["/_next/static/chunks/4d80e004cf4896dd.js","/_next/static/chunks/350ee4303b732916.js"],"default"]
14:I[73211,["/_next/static/chunks/4d80e004cf4896dd.js","/_next/static/chunks/350ee4303b732916.js"],"default"]
0:{"buildId":"i-dnJM_MIpJSOCQWNJVMq","rsc":["$","$1","c",{"children":[[["$","script","script-0",{"src":"/_next/static/chunks/abb1f677d5bfa0d7.js","async":true}],["$","script","script-1",{"src":"/_next/static/chunks/73ee7cc830199358.js","async":true}],["$","script","script-2",{"src":"/_next/static/chunks/713c1b62d4d91e41.js","async":true}],["$","script","script-3",{"src":"/_next/static/chunks/ab3a9da172ad7619.js","async":true}]],["$","$L2",null,{"tree":{"$id":"root","name":"Documentation","children":[{"$id":"index.mdx","type":"page","name":"Introduction","description":"Zen LM by Hanzo AI -- frontier models for code, reasoning, vision, multimodal, embeddings, and safety","url":"/docs","$ref":{"file":"index.mdx"}},{"$id":"_0","type":"separator","name":"Getting Started"},{"type":"folder","name":"Getting started","children":[{"$id":"getting-started/installation.mdx","type":"page","name":"Installation","description":"Install dependencies for using Zen models","url":"/docs/getting-started/installation","$ref":{"file":"getting-started/installation.mdx"}},{"$id":"getting-started/quickstart.mdx","type":"page","name":"Quickstart","description":"Get started with Zen models in minutes","url":"/docs/getting-started/quickstart","$ref":{"file":"getting-started/quickstart.mdx"}}],"$id":"getting-started"},{"$id":"_1","type":"separator","name":"API"},{"type":"folder","name":"API Reference","children":[{"$id":"api/chat-completions.mdx","type":"page","name":"Chat Completions","description":"Generate text with any Zen model using the OpenAI-compatible chat completions endpoint","url":"/docs/api/chat-completions","$ref":{"file":"api/chat-completions.mdx"}},{"$id":"api/embeddings.mdx","type":"page","name":"Embeddings","description":"Generate 3072-dimensional vector embeddings with zen3-embedding","url":"/docs/api/embeddings","$ref":{"file":"api/embeddings.mdx"}},{"$id":"api/models.mdx","type":"page","name":"Models","description":"All Zen models -- capabilities, pricing, and recommended use cases","url":"/docs/api/models","$ref":{"file":"api/models.mdx"}},{"$id":"api/pricing.mdx","type":"page","name":"Pricing","description":"Zen LM API pricing -- transparent at 3x upstream inference cost","url":"/docs/api/pricing","$ref":{"file":"api/pricing.mdx"}}],"$id":"api","index":{"$id":"api/index.mdx","type":"page","name":"API Reference","description":"Zen LM Cloud API -- OpenAI-compatible endpoints for all Zen models","url":"/docs/api","$ref":{"file":"api/index.mdx"}}},{"$id":"_2","type":"separator","name":"Models"},{"type":"folder","name":"Models","children":[{"$id":"models/zen-3d.mdx","type":"page","name":"zen-3d","description":"3D generation model for text-to-3D and image-to-3D asset creation.","url":"/docs/models/zen-3d","$ref":{"file":"models/zen-3d.mdx"}},{"$id":"models/zen-agent.mdx","type":"page","name":"zen-agent","description":"32B dense model with tool use and planning for agentic AI workflows.","url":"/docs/models/zen-agent","$ref":{"file":"models/zen-agent.mdx"}},{"$id":"models/zen-artist-edit.mdx","type":"page","name":"zen-artist-edit","description":"Image editing model for inpainting, outpainting, and edit-by-instruction.","url":"/docs/models/zen-artist-edit","$ref":{"file":"models/zen-artist-edit.mdx"}},{"$id":"models/zen-artist.mdx","type":"page","name":"zen-artist","description":"Image generation model supporting multiple styles and high-resolution output.","url":"/docs/models/zen-artist","$ref":{"file":"models/zen-artist.mdx"}},{"$id":"models/zen-code.mdx","type":"page","name":"zen-code","description":"Legacy 14B dense code model for general programming tasks.","url":"/docs/models/zen-code","$ref":{"file":"models/zen-code.mdx"}},{"$id":"models/zen-coder-flash.mdx","type":"page","name":"zen-coder-flash","description":"Lightweight 7B dense model for low-latency code completions.","url":"/docs/models/zen-coder-flash","$ref":{"file":"models/zen-coder-flash.mdx"}},{"$id":"models/zen-coder.mdx","type":"page","name":"zen-coder","description":"32B dense code model with 131K context for multi-language development.","url":"/docs/models/zen-coder","$ref":{"file":"models/zen-coder.mdx"}},{"$id":"models/zen-designer.mdx","type":"page","name":"zen-designer","description":"Design generation model for UI/UX, graphics, and visual layouts.","url":"/docs/models/zen-designer","$ref":{"file":"models/zen-designer.mdx"}},{"$id":"models/zen-director.mdx","type":"page","name":"zen-director","description":"Text-to-video generation model with cinematic quality output.","url":"/docs/models/zen-director","$ref":{"file":"models/zen-director.mdx"}},{"$id":"models/zen-dub-live.mdx","type":"page","name":"zen-dub-live","description":"Real-time voice synthesis with ultra-low latency for live applications.","url":"/docs/models/zen-dub-live","$ref":{"file":"models/zen-dub-live.mdx"}},{"$id":"models/zen-dub.mdx","type":"page","name":"zen-dub","description":"Voice synthesis and multi-language dubbing model.","url":"/docs/models/zen-dub","$ref":{"file":"models/zen-dub.mdx"}},{"$id":"models/zen-eco.mdx","type":"page","name":"zen-eco","description":"Efficient 4B dense model balancing capability and cost for general-purpose tasks.","url":"/docs/models/zen-eco","$ref":{"file":"models/zen-eco.mdx"}},{"$id":"models/zen-embedding.mdx","type":"page","name":"zen-embedding","description":"Foundation embedding model for search and retrieval.","url":"/docs/models/zen-embedding","$ref":{"file":"models/zen-embedding.mdx"}},{"$id":"models/zen-foley.mdx","type":"page","name":"zen-foley","description":"Sound effects generation model for text-to-SFX production.","url":"/docs/models/zen-foley","$ref":{"file":"models/zen-foley.mdx"}},{"$id":"models/zen-guard-gen.mdx","type":"page","name":"zen-guard-gen","description":"8B dense model for safe text generation with built-in guardrails.","url":"/docs/models/zen-guard-gen","$ref":{"file":"models/zen-guard-gen.mdx"}},{"$id":"models/zen-guard-stream.mdx","type":"page","name":"zen-guard-stream","description":"4B dense model for low-latency streaming content moderation.","url":"/docs/models/zen-guard-stream","$ref":{"file":"models/zen-guard-stream.mdx"}},{"$id":"models/zen-guard.mdx","type":"page","name":"zen-guard","description":"Content safety and moderation classifier.","url":"/docs/models/zen-guard","$ref":{"file":"models/zen-guard.mdx"}},{"$id":"models/zen-live.mdx","type":"page","name":"zen-live","description":"Real-time bidirectional speech translation with ultra-low latency.","url":"/docs/models/zen-live","$ref":{"file":"models/zen-live.mdx"}},{"$id":"models/zen-max.mdx","type":"page","name":"zen-max","description":"Trillion-parameter 1.04T MoE open-weights frontier model. Same model as zen4-max.","url":"/docs/models/zen-max","$ref":{"file":"models/zen-max.mdx"}},{"$id":"models/zen-musician.mdx","type":"page","name":"zen-musician","description":"Music generation model with multi-instrument composition and style control.","url":"/docs/models/zen-musician","$ref":{"file":"models/zen-musician.mdx"}},{"$id":"models/zen-nano.mdx","type":"page","name":"zen-nano","description":"Ultra-compact 0.6B dense model for edge inference at 44K tokens/sec.","url":"/docs/models/zen-nano","$ref":{"file":"models/zen-nano.mdx"}},{"$id":"models/zen-next.mdx","type":"page","name":"zen-next","description":"Next-generation preview model with cutting-edge capabilities.","url":"/docs/models/zen-next","$ref":{"file":"models/zen-next.mdx"}},{"$id":"models/zen-omni.mdx","type":"page","name":"zen-omni","description":"72B dense hypermodal model supporting text, vision, audio, and code.","url":"/docs/models/zen-omni","$ref":{"file":"models/zen-omni.mdx"}},{"$id":"models/zen-pro.mdx","type":"page","name":"zen-pro","description":"Professional-grade 32B dense model with 19K tokens/sec throughput.","url":"/docs/models/zen-pro","$ref":{"file":"models/zen-pro.mdx"}},{"$id":"models/zen-reranker.mdx","type":"page","name":"zen-reranker","description":"568M dense cross-encoder model for search result reranking.","url":"/docs/models/zen-reranker","$ref":{"file":"models/zen-reranker.mdx"}},{"$id":"models/zen-scribe.mdx","type":"page","name":"zen-scribe","description":"Speech-to-text transcription model with multi-language support.","url":"/docs/models/zen-scribe","$ref":{"file":"models/zen-scribe.mdx"}},{"$id":"models/zen-translator.mdx","type":"page","name":"zen-translator","description":"Context-aware translation model supporting 100+ languages.","url":"/docs/models/zen-translator","$ref":{"file":"models/zen-translator.mdx"}},{"$id":"models/zen-video-i2v.mdx","type":"page","name":"zen-video-i2v","description":"Image-to-video animation model that brings still images to life.","url":"/docs/models/zen-video-i2v","$ref":{"file":"models/zen-video-i2v.mdx"}},{"$id":"models/zen-video.mdx","type":"page","name":"zen-video","description":"Video understanding model for frame analysis, captioning, and temporal reasoning.","url":"/docs/models/zen-video","$ref":{"file":"models/zen-video.mdx"}},{"$id":"models/zen-vl.mdx","type":"page","name":"zen-vl","description":"32B dense multimodal model for vision-language understanding.","url":"/docs/models/zen-vl","$ref":{"file":"models/zen-vl.mdx"}},{"$id":"models/zen-voyager.mdx","type":"page","name":"zen-voyager","description":"World model for spatial reasoning and 3D scene understanding.","url":"/docs/models/zen-voyager","$ref":{"file":"models/zen-voyager.mdx"}},{"$id":"models/zen-world.mdx","type":"page","name":"zen-world","description":"World simulation model for spatial reasoning and environment generation.","url":"/docs/models/zen-world","$ref":{"file":"models/zen-world.mdx"}},{"$id":"models/zen.mdx","type":"page","name":"zen","description":"Standard 8-32B dense foundation model for general-purpose AI tasks.","url":"/docs/models/zen","$ref":{"file":"models/zen.mdx"}},{"$id":"models/zen3-asr-v1.mdx","type":"page","name":"zen3-asr-v1","description":"First-generation streaming ASR for legacy compatibility.","url":"/docs/models/zen3-asr-v1","$ref":{"file":"models/zen3-asr-v1.mdx"}},{"$id":"models/zen3-asr.mdx","type":"page","name":"zen3-asr","description":"Real-time streaming speech recognition for live transcription and voice agents.","url":"/docs/models/zen3-asr","$ref":{"file":"models/zen3-asr.mdx"}},{"$id":"models/zen3-audio-fast.mdx","type":"page","name":"zen3-audio-fast","description":"Fastest speech-to-text transcription for high-throughput workloads.","url":"/docs/models/zen3-audio-fast","$ref":{"file":"models/zen3-audio-fast.mdx"}},{"$id":"models/zen3-audio.mdx","type":"page","name":"zen3-audio","description":"Best quality speech-to-text transcription. 100+ languages.","url":"/docs/models/zen3-audio","$ref":{"file":"models/zen3-audio.mdx"}},{"$id":"models/zen3-embedding-medium.mdx","type":"page","name":"zen3-embedding-medium","description":"Balanced embedding model for cost-effective retrieval workloads.","url":"/docs/models/zen3-embedding-medium","$ref":{"file":"models/zen3-embedding-medium.mdx"}},{"$id":"models/zen3-embedding-small.mdx","type":"page","name":"zen3-embedding-small","description":"Lightweight embedding model for high-throughput, low-cost applications.","url":"/docs/models/zen3-embedding-small","$ref":{"file":"models/zen3-embedding-small.mdx"}},{"$id":"models/zen3-embedding.mdx","type":"page","name":"zen3-embedding","description":"High-quality text embedding model with 3072 dimensions and 8K context.","url":"/docs/models/zen3-embedding","$ref":{"file":"models/zen3-embedding.mdx"}},{"$id":"models/zen3-guard.mdx","type":"page","name":"zen3-guard","description":"Content safety classifier with 4B dense architecture. 65K context.","url":"/docs/models/zen3-guard","$ref":{"file":"models/zen3-guard.mdx"}},{"$id":"models/zen3-image-dev.mdx","type":"page","name":"zen3-image-dev","description":"Development model for experimentation and iteration.","url":"/docs/models/zen3-image-dev","$ref":{"file":"models/zen3-image-dev.mdx"}},{"$id":"models/zen3-image-fast.mdx","type":"page","name":"zen3-image-fast","description":"Fastest image model for real-time generation.","url":"/docs/models/zen3-image-fast","$ref":{"file":"models/zen3-image-fast.mdx"}},{"$id":"models/zen3-image-jp.mdx","type":"page","name":"zen3-image-jp","description":"Japanese-specialized image generation model.","url":"/docs/models/zen3-image-jp","$ref":{"file":"models/zen3-image-jp.mdx"}},{"$id":"models/zen3-image-max.mdx","type":"page","name":"zen3-image-max","description":"Maximum quality image generation for professional creative work.","url":"/docs/models/zen3-image-max","$ref":{"file":"models/zen3-image-max.mdx"}},{"$id":"models/zen3-image-playground.mdx","type":"page","name":"zen3-image-playground","description":"Aesthetic model for artistic image generation.","url":"/docs/models/zen3-image-playground","$ref":{"file":"models/zen3-image-playground.mdx"}},{"$id":"models/zen3-image-sdxl.mdx","type":"page","name":"zen3-image-sdxl","description":"High-resolution image generation at 1024px.","url":"/docs/models/zen3-image-sdxl","$ref":{"file":"models/zen3-image-sdxl.mdx"}},{"$id":"models/zen3-image-ssd.mdx","type":"page","name":"zen3-image-ssd","description":"Fastest diffusion model for real-time generation.","url":"/docs/models/zen3-image-ssd","$ref":{"file":"models/zen3-image-ssd.mdx"}},{"$id":"models/zen3-image.mdx","type":"page","name":"zen3-image","description":"Best general-purpose image generation.","url":"/docs/models/zen3-image","$ref":{"file":"models/zen3-image.mdx"}},{"$id":"models/zen3-nano.mdx","type":"page","name":"zen3-nano","description":"Ultra-lightweight 4B dense model for edge deployment. 40K context.","url":"/docs/models/zen3-nano","$ref":{"file":"models/zen3-nano.mdx"}},{"$id":"models/zen3-omni.mdx","type":"page","name":"zen3-omni","description":"Hypermodal ~200B dense model supporting text, vision, and audio. 202K context.","url":"/docs/models/zen3-omni","$ref":{"file":"models/zen3-omni.mdx"}},{"$id":"models/zen3-reranker-medium.mdx","type":"page","name":"zen3-reranker-medium","description":"Balanced reranker for cost-effective retrieval quality improvement.","url":"/docs/models/zen3-reranker-medium","$ref":{"file":"models/zen3-reranker-medium.mdx"}},{"$id":"models/zen3-reranker-small.mdx","type":"page","name":"zen3-reranker-small","description":"Lightweight reranker for high-throughput reranking at minimal cost.","url":"/docs/models/zen3-reranker-small","$ref":{"file":"models/zen3-reranker-small.mdx"}},{"$id":"models/zen3-reranker.mdx","type":"page","name":"zen3-reranker","description":"High-quality reranker for improving retrieval accuracy in RAG pipelines.","url":"/docs/models/zen3-reranker","$ref":{"file":"models/zen3-reranker.mdx"}},{"$id":"models/zen3-tts-fast.mdx","type":"page","name":"zen3-tts-fast","description":"Low-latency text-to-speech for real-time voice agents and interactive applications.","url":"/docs/models/zen3-tts-fast","$ref":{"file":"models/zen3-tts-fast.mdx"}},{"$id":"models/zen3-tts-hd.mdx","type":"page","name":"zen3-tts-hd","description":"Maximum fidelity text-to-speech for broadcast-quality audio production.","url":"/docs/models/zen3-tts-hd","$ref":{"file":"models/zen3-tts-hd.mdx"}},{"$id":"models/zen3-tts.mdx","type":"page","name":"zen3-tts","description":"High-quality text-to-speech with natural prosody. 40+ voices, 8 languages.","url":"/docs/models/zen3-tts","$ref":{"file":"models/zen3-tts.mdx"}},{"$id":"models/zen3-vl.mdx","type":"page","name":"zen3-vl","description":"Vision-language model with 30B (3B active) MoE architecture. 262K context.","url":"/docs/models/zen3-vl","$ref":{"file":"models/zen3-vl.mdx"}},{"$id":"models/zen4-coder-flash.mdx","type":"page","name":"zen4-coder-flash","description":"Fast 30B (3B active) MoE code model with 262K context.","url":"/docs/models/zen4-coder-flash","$ref":{"file":"models/zen4-coder-flash.mdx"}},{"$id":"models/zen4-coder-pro.mdx","type":"page","name":"zen4-coder-pro","description":"Premium 480B full-precision BF16 code model with 131K context.","url":"/docs/models/zen4-coder-pro","$ref":{"file":"models/zen4-coder-pro.mdx"}},{"$id":"models/zen4-coder.mdx","type":"page","name":"zen4-coder","description":"Code generation model with 480B (35B active) MoE and 163K context.","url":"/docs/models/zen4-coder","$ref":{"file":"models/zen4-coder.mdx"}},{"$id":"models/zen4-max.mdx","type":"page","name":"zen4-max","description":"Trillion-parameter frontier MoE model. 1.04T (32B active) with 163K context.","url":"/docs/models/zen4-max","$ref":{"file":"models/zen4-max.mdx"}},{"$id":"models/zen4-mini.mdx","type":"page","name":"zen4-mini","description":"Fast and efficient 8B dense model with 40K context.","url":"/docs/models/zen4-mini","$ref":{"file":"models/zen4-mini.mdx"}},{"$id":"models/zen4-pro.mdx","type":"page","name":"zen4-pro","description":"High capability MoE model. 80B (3B active) with 131K context.","url":"/docs/models/zen4-pro","$ref":{"file":"models/zen4-pro.mdx"}},{"$id":"models/zen4-thinking.mdx","type":"page","name":"zen4-thinking","description":"Deep reasoning model with 80B (3B active) MoE + chain-of-thought. 131K context.","url":"/docs/models/zen4-thinking","$ref":{"file":"models/zen4-thinking.mdx"}},{"$id":"models/zen4-ultra.mdx","type":"page","name":"zen4-ultra","description":"Maximum reasoning model with 744B MoE (40B active) + extended chain-of-thought. 262K context.","url":"/docs/models/zen4-ultra","$ref":{"file":"models/zen4-ultra.mdx"}},{"$id":"models/zen4.1.mdx","type":"page","name":"zen4.1","description":"High-performance 1M context model for long-document analysis, large codebase reasoning, and agentic workflows. Best balance of intelligence and cost at million-token scale.","url":"/docs/models/zen4.1","$ref":{"file":"models/zen4.1.mdx"}},{"$id":"models/zen4.mdx","type":"page","name":"zen4","description":"Flagship 744B MoE model with 40B active parameters and 202K context window.","url":"/docs/models/zen4","$ref":{"file":"models/zen4.mdx"}},{"$id":"models/zen5-coder.mdx","type":"page","name":"zen5-coder","description":"Next-generation code model. Zen5 architecture optimized for agentic programming. Research Preview.","url":"/docs/models/zen5-coder","$ref":{"file":"models/zen5-coder.mdx"}},{"$id":"models/zen5-max.mdx","type":"page","name":"zen5-max","description":"Maximum-scale Zen5 model. Multi-trillion parameter frontier MoE. Research Preview.","url":"/docs/models/zen5-max","$ref":{"file":"models/zen5-max.mdx"}},{"$id":"models/zen5-mini.mdx","type":"page","name":"zen5-mini","description":"Efficient agentic model delivering zen5-class intelligence at a fraction of the cost.","url":"/docs/models/zen5-mini","$ref":{"file":"models/zen5-mini.mdx"}},{"$id":"models/zen5-pro.mdx","type":"page","name":"zen5-pro","description":"High-throughput agentic model for demanding production workloads. Trained on real-world development patterns with deep chain-of-thought reasoning.","url":"/docs/models/zen5-pro","$ref":{"file":"models/zen5-pro.mdx"}},{"$id":"models/zen5-ultra.mdx","type":"page","name":"zen5-ultra","description":"Deepest reasoning model in the Zen family. Multi-pass chain-of-thought with self-verification.","url":"/docs/models/zen5-ultra","$ref":{"file":"models/zen5-ultra.mdx"}},{"$id":"models/zen5.mdx","type":"page","name":"zen5","description":"Next-generation flagship model. 2T+ MoE with on-chain training via NVIDIA TEE. Research Preview.","url":"/docs/models/zen5","$ref":{"file":"models/zen5.mdx"}}],"$id":"models"},{"type":"folder","name":"Training","children":[{"$id":"training/cloud.mdx","type":"page","name":"Cloud Training","description":"Full-scale training on 8x H200 GPUs","url":"/docs/training/cloud","$ref":{"file":"training/cloud.mdx"}},{"$id":"training/cuda.mdx","type":"page","name":"CUDA Training","description":"Train locally with NVIDIA GPUs","url":"/docs/training/cuda","$ref":{"file":"training/cuda.mdx"}},{"$id":"training/mlx.mdx","type":"page","name":"MLX Training","description":"Train on Apple Silicon with MLX","url":"/docs/training/mlx","$ref":{"file":"training/mlx.mdx"}},{"$id":"training/overview.mdx","type":"page","name":"Training Overview","description":"Train Zen models with multiple backend options","url":"/docs/training/overview","$ref":{"file":"training/overview.mdx"}}],"$id":"training"},{"$id":"datasets.mdx","type":"page","name":"Zen Agentic Dataset","description":"Zen Agentic Dataset - 8.47 billion tokens of real-world agentic programming","url":"/docs/datasets","$ref":{"file":"datasets.mdx"}}],"fallback":{"$id":"fallback:root","name":"Docs","children":[{"$id":"fallback:models.mdx","type":"page","name":"Models","description":"Complete Zen LM model family â€” 80+ models across text, code, vision, audio, image, video, 3D, embedding, safety, and agent modalities","url":"/docs/models","$ref":{"file":"models.mdx"}},{"$id":"fallback:training.mdx","type":"page","name":"Fine-tuning Guide","description":"Fine-tune Zen4 models with MLX, Unsloth, or DeepSpeed","url":"/docs/training","$ref":{"file":"training.mdx"}}]}},"children":"$L3"}]]}],"loading":null,"isPartial":false}
3:["$","$L4",null,{"children":["$","$L5",null,{"children":["$","$L6",null,{"children":[["$","$L7",null,{"id":"nd-subnav","className":"[grid-area:header] sticky top-(--fd-docs-row-1) z-30 flex items-center ps-4 pe-2.5 border-b transition-colors backdrop-blur-sm h-(--fd-header-height) md:hidden max-md:layout:[--fd-header-height:--spacing(14)] data-[transparent=false]:bg-fd-background/80","children":[["$","$L8",null,{"href":"/","className":"inline-flex items-center gap-2.5 font-semibold","children":"ðŸª· Zen LM"}],["$","div",null,{"className":"flex-1"}],["$","$L9",null,{"className":"p-2","hideIfDisabled":true}],["$","$La",null,{"className":"inline-flex items-center justify-center rounded-md text-sm font-medium transition-colors duration-100 disabled:pointer-events-none disabled:opacity-50 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-fd-ring hover:bg-fd-accent hover:text-fd-accent-foreground [&_svg]:size-4.5 p-2","children":["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-panel-left","aria-hidden":"true","children":[["$","rect","afitv7",{"width":"18","height":"18","x":"3","y":"3","rx":"2"}],["$","path","fh3hqa",{"d":"M9 3v18"}],"$undefined"]}]}]]}],[["$","$Lb",null,{"children":[["$","div",null,{"className":"flex flex-col gap-3 p-4 pb-2","children":[["$","div",null,{"className":"flex","children":[["$","$L8",null,{"href":"/","className":"inline-flex text-[0.9375rem] items-center gap-2.5 font-medium me-auto","children":"ðŸª· Zen LM"}],"$undefined",["$","$Lc",null,{"className":"inline-flex items-center justify-center rounded-md text-sm font-medium transition-colors duration-100 disabled:pointer-events-none disabled:opacity-50 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-fd-ring hover:bg-fd-accent hover:text-fd-accent-foreground p-1.5 [&_svg]:size-4.5 mb-auto text-fd-muted-foreground","children":["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-panel-left","aria-hidden":"true","children":[["$","rect","afitv7",{"width":"18","height":"18","x":"3","y":"3","rx":"2"}],["$","path","fh3hqa",{"d":"M9 3v18"}],"$undefined"]}]}]]}],["$","$Ld",null,{"hideIfDisabled":true}],false,["$","div",null,{"className":"p-3 rounded-lg bg-primary/10 text-sm","children":[["$","strong",null,{"children":"zen4-max"}]," â€” 1T+ MoE frontier model"]}]]}],["$","$Le",null,{"children":[[["$","$Lf","0",{"item":{"text":"HuggingFace","url":"https://huggingface.co/zenlm"},"className":""}],["$","$Lf","1",{"item":{"text":"GitHub","url":"https://github.com/zenlm"},"className":"mb-4"}]],["$","$L10",null,{}]]}],["$","div",null,{"className":"flex flex-col border-t p-4 pt-2 empty:hidden","children":[["$","div",null,{"className":"flex text-fd-muted-foreground items-center empty:hidden","children":[false,[],["$","$L11",null,{"className":"ms-auto p-0"}]]}],"$undefined"]}]]}],["$","$L12",null,{"children":[["$","div",null,{"className":"flex flex-col gap-3 p-4 pb-2","children":[["$","div",null,{"className":"flex text-fd-muted-foreground items-center gap-1.5","children":[["$","div",null,{"className":"flex flex-1","children":[]}],false,["$","$L11",null,{"className":"p-0"}],["$","$La",null,{"className":"inline-flex items-center justify-center rounded-md text-sm font-medium transition-colors duration-100 disabled:pointer-events-none disabled:opacity-50 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-fd-ring hover:bg-fd-accent hover:text-fd-accent-foreground [&_svg]:size-4.5 p-2","children":["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-panel-left","aria-hidden":"true","children":[["$","rect","afitv7",{"width":"18","height":"18","x":"3","y":"3","rx":"2"}],["$","path","fh3hqa",{"d":"M9 3v18"}],"$undefined"]}]}]]}],false,"$3:props:children:props:children:props:children:1:0:props:children:0:props:children:3"]}],"$3:props:children:props:children:props:children:1:0:props:children:1",["$","div",null,{"className":"flex flex-col border-t p-4 pt-2 empty:hidden"}]]}]],false,["$","$L13",null,{"parallelRouterKey":"children","template":["$","$L14",null,{}]}]]}]}]}]
