1:"$Sreact.fragment"
2:I[106,["/_next/static/chunks/8d687ef04ae593a8.js"],"RootProvider"]
3:I[53113,["/_next/static/chunks/4d80e004cf4896dd.js","/_next/static/chunks/350ee4303b732916.js"],"default"]
4:I[73211,["/_next/static/chunks/4d80e004cf4896dd.js","/_next/static/chunks/350ee4303b732916.js"],"default"]
5:I[25838,["/_next/static/chunks/8d687ef04ae593a8.js","/_next/static/chunks/c366ecc986f0489d.js","/_next/static/chunks/d5ccaca76587500f.js","/_next/static/chunks/4032bb4803b95120.js","/_next/static/chunks/4b7dc5f6fcc90c00.js"],"TreeContextProvider"]
a:I[6998,["/_next/static/chunks/4d80e004cf4896dd.js","/_next/static/chunks/350ee4303b732916.js"],"default"]
:HL["/_next/static/chunks/3358adf4e9412038.css","style"]
0:{"P":null,"b":"5PR8qvycLiJu7j2WseH8O","c":["","docs","models","zen3-tts-fast"],"q":"","i":false,"f":[[["",{"children":["docs",{"children":[["slug","models/zen3-tts-fast","oc"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],[["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/chunks/3358adf4e9412038.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","script","script-0",{"src":"/_next/static/chunks/8d687ef04ae593a8.js","async":true,"nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"dark","suppressHydrationWarning":true,"children":["$","body",null,{"className":"antialiased","children":["$","$L2",null,{"children":["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]}]]}],{"children":[["$","$1","c",{"children":[[["$","script","script-0",{"src":"/_next/static/chunks/c366ecc986f0489d.js","async":true,"nonce":"$undefined"}],["$","script","script-1",{"src":"/_next/static/chunks/d5ccaca76587500f.js","async":true,"nonce":"$undefined"}],["$","script","script-2",{"src":"/_next/static/chunks/4032bb4803b95120.js","async":true,"nonce":"$undefined"}],["$","script","script-3",{"src":"/_next/static/chunks/4b7dc5f6fcc90c00.js","async":true,"nonce":"$undefined"}]],["$","$L5",null,{"tree":{"$id":"root","name":"Documentation","children":[{"$id":"index.mdx","type":"page","name":"Introduction","description":"Zen LM by Hanzo AI -- frontier models for code, reasoning, vision, multimodal, embeddings, and safety","icon":"$undefined","url":"/docs","$ref":{"file":"index.mdx"}},{"$id":"_0","type":"separator","icon":"$undefined","name":"Getting Started"},{"type":"folder","name":"Getting started","root":"$undefined","defaultOpen":"$undefined","description":"$undefined","collapsible":"$undefined","children":[{"$id":"getting-started/installation.mdx","type":"page","name":"Installation","description":"Install dependencies for using Zen models","icon":"$undefined","url":"/docs/getting-started/installation","$ref":{"file":"getting-started/installation.mdx"}},{"$id":"getting-started/quickstart.mdx","type":"page","name":"Quickstart","description":"Get started with Zen models in minutes","icon":"$undefined","url":"/docs/getting-started/quickstart","$ref":{"file":"getting-started/quickstart.mdx"}}],"$id":"getting-started","$ref":"$undefined","icon":"$undefined"},{"$id":"_1","type":"separator","icon":"$undefined","name":"API"},{"type":"folder","name":"API Reference","root":"$undefined","defaultOpen":"$undefined","description":"$undefined","collapsible":"$undefined","children":[{"$id":"api/chat-completions.mdx","type":"page","name":"Chat Completions","description":"Generate text with any Zen model using the OpenAI-compatible chat completions endpoint","icon":"$undefined","url":"/docs/api/chat-completions","$ref":{"file":"api/chat-completions.mdx"}},{"$id":"api/embeddings.mdx","type":"page","name":"Embeddings","description":"Generate 3072-dimensional vector embeddings with zen3-embedding","icon":"$undefined","url":"/docs/api/embeddings","$ref":{"file":"api/embeddings.mdx"}},{"$id":"api/models.mdx","type":"page","name":"Models","description":"All Zen models -- capabilities, pricing, and recommended use cases","icon":"$undefined","url":"/docs/api/models","$ref":{"file":"api/models.mdx"}},{"$id":"api/pricing.mdx","type":"page","name":"Pricing","description":"Zen LM API pricing -- transparent at 3x upstream inference cost","icon":"$undefined","url":"/docs/api/pricing","$ref":{"file":"api/pricing.mdx"}}],"$id":"api","$ref":"$undefined","index":{"$id":"api/index.mdx","type":"page","name":"API Reference","description":"Zen LM Cloud API -- OpenAI-compatible endpoints for all Zen models","icon":"$undefined","url":"/docs/api","$ref":{"file":"api/index.mdx"}},"icon":"$undefined"},{"$id":"_2","type":"separator","icon":"$undefined","name":"Models"},{"type":"folder","name":"Models","root":"$undefined","defaultOpen":"$undefined","description":"$undefined","collapsible":"$undefined","children":[{"$id":"models/zen-3d.mdx","type":"page","name":"zen-3d","description":"3D generation model for text-to-3D and image-to-3D asset creation.","icon":"$undefined","url":"/docs/models/zen-3d","$ref":{"file":"models/zen-3d.mdx"}},{"$id":"models/zen-agent.mdx","type":"page","name":"zen-agent","description":"32B dense model with tool use and planning for agentic AI workflows.","icon":"$undefined","url":"/docs/models/zen-agent","$ref":{"file":"models/zen-agent.mdx"}},{"$id":"models/zen-artist-edit.mdx","type":"page","name":"zen-artist-edit","description":"Image editing model for inpainting, outpainting, and edit-by-instruction.","icon":"$undefined","url":"/docs/models/zen-artist-edit","$ref":{"file":"models/zen-artist-edit.mdx"}},{"$id":"models/zen-artist.mdx","type":"page","name":"zen-artist","description":"Image generation model supporting multiple styles and high-resolution output.","icon":"$undefined","url":"/docs/models/zen-artist","$ref":{"file":"models/zen-artist.mdx"}},{"$id":"models/zen-code.mdx","type":"page","name":"zen-code","description":"Legacy 14B dense code model for general programming tasks.","icon":"$undefined","url":"/docs/models/zen-code","$ref":{"file":"models/zen-code.mdx"}},{"$id":"models/zen-coder-flash.mdx","type":"page","name":"zen-coder-flash","description":"Lightweight 7B dense model for low-latency code completions.","icon":"$undefined","url":"/docs/models/zen-coder-flash","$ref":{"file":"models/zen-coder-flash.mdx"}},{"$id":"models/zen-coder.mdx","type":"page","name":"zen-coder","description":"32B dense code model with 131K context for multi-language development.","icon":"$undefined","url":"/docs/models/zen-coder","$ref":{"file":"models/zen-coder.mdx"}},{"$id":"models/zen-designer.mdx","type":"page","name":"zen-designer","description":"Design generation model for UI/UX, graphics, and visual layouts.","icon":"$undefined","url":"/docs/models/zen-designer","$ref":{"file":"models/zen-designer.mdx"}},{"$id":"models/zen-director.mdx","type":"page","name":"zen-director","description":"Text-to-video generation model with cinematic quality output.","icon":"$undefined","url":"/docs/models/zen-director","$ref":{"file":"models/zen-director.mdx"}},{"$id":"models/zen-dub-live.mdx","type":"page","name":"zen-dub-live","description":"Real-time voice synthesis with ultra-low latency for live applications.","icon":"$undefined","url":"/docs/models/zen-dub-live","$ref":{"file":"models/zen-dub-live.mdx"}},{"$id":"models/zen-dub.mdx","type":"page","name":"zen-dub","description":"Voice synthesis and multi-language dubbing model.","icon":"$undefined","url":"/docs/models/zen-dub","$ref":{"file":"models/zen-dub.mdx"}},{"$id":"models/zen-eco.mdx","type":"page","name":"zen-eco","description":"Efficient 4B dense model balancing capability and cost for general-purpose tasks.","icon":"$undefined","url":"/docs/models/zen-eco","$ref":{"file":"models/zen-eco.mdx"}},{"$id":"models/zen-embedding.mdx","type":"page","name":"zen-embedding","description":"Foundation embedding model for search and retrieval.","icon":"$undefined","url":"/docs/models/zen-embedding","$ref":{"file":"models/zen-embedding.mdx"}},{"$id":"models/zen-foley.mdx","type":"page","name":"zen-foley","description":"Sound effects generation model for text-to-SFX production.","icon":"$undefined","url":"/docs/models/zen-foley","$ref":{"file":"models/zen-foley.mdx"}},{"$id":"models/zen-guard-gen.mdx","type":"page","name":"zen-guard-gen","description":"8B dense model for safe text generation with built-in guardrails.","icon":"$undefined","url":"/docs/models/zen-guard-gen","$ref":{"file":"models/zen-guard-gen.mdx"}},{"$id":"models/zen-guard-stream.mdx","type":"page","name":"zen-guard-stream","description":"4B dense model for low-latency streaming content moderation.","icon":"$undefined","url":"/docs/models/zen-guard-stream","$ref":{"file":"models/zen-guard-stream.mdx"}},{"$id":"models/zen-guard.mdx","type":"page","name":"zen-guard","description":"Content safety and moderation classifier.","icon":"$undefined","url":"/docs/models/zen-guard","$ref":{"file":"models/zen-guard.mdx"}},{"$id":"models/zen-live.mdx","type":"page","name":"zen-live","description":"Real-time bidirectional speech translation with ultra-low latency.","icon":"$undefined","url":"/docs/models/zen-live","$ref":{"file":"models/zen-live.mdx"}},{"$id":"models/zen-max.mdx","type":"page","name":"zen-max","description":"Trillion-parameter 1.04T MoE open-weights frontier model. Same model as zen4-max.","icon":"$undefined","url":"/docs/models/zen-max","$ref":{"file":"models/zen-max.mdx"}},{"$id":"models/zen-musician.mdx","type":"page","name":"zen-musician","description":"Music generation model with multi-instrument composition and style control.","icon":"$undefined","url":"/docs/models/zen-musician","$ref":{"file":"models/zen-musician.mdx"}},{"$id":"models/zen-nano.mdx","type":"page","name":"zen-nano","description":"Ultra-compact 0.6B dense model for edge inference at 44K tokens/sec.","icon":"$undefined","url":"/docs/models/zen-nano","$ref":{"file":"models/zen-nano.mdx"}},{"$id":"models/zen-next.mdx","type":"page","name":"zen-next","description":"Next-generation preview model with cutting-edge capabilities.","icon":"$undefined","url":"/docs/models/zen-next","$ref":{"file":"models/zen-next.mdx"}},{"$id":"models/zen-omni.mdx","type":"page","name":"zen-omni","description":"72B dense hypermodal model supporting text, vision, audio, and code.","icon":"$undefined","url":"/docs/models/zen-omni","$ref":{"file":"models/zen-omni.mdx"}},{"$id":"models/zen-pro.mdx","type":"page","name":"zen-pro","description":"Professional-grade 32B dense model with 19K tokens/sec throughput.","icon":"$undefined","url":"/docs/models/zen-pro","$ref":{"file":"models/zen-pro.mdx"}},{"$id":"models/zen-reranker.mdx","type":"page","name":"zen-reranker","description":"568M dense cross-encoder model for search result reranking.","icon":"$undefined","url":"/docs/models/zen-reranker","$ref":{"file":"models/zen-reranker.mdx"}},{"$id":"models/zen-scribe.mdx","type":"page","name":"zen-scribe","description":"Speech-to-text transcription model with multi-language support.","icon":"$undefined","url":"/docs/models/zen-scribe","$ref":{"file":"models/zen-scribe.mdx"}},{"$id":"models/zen-translator.mdx","type":"page","name":"zen-translator","description":"Context-aware translation model supporting 100+ languages.","icon":"$undefined","url":"/docs/models/zen-translator","$ref":{"file":"models/zen-translator.mdx"}},{"$id":"models/zen-video-i2v.mdx","type":"page","name":"zen-video-i2v","description":"Image-to-video animation model that brings still images to life.","icon":"$undefined","url":"/docs/models/zen-video-i2v","$ref":{"file":"models/zen-video-i2v.mdx"}},{"$id":"models/zen-video.mdx","type":"page","name":"zen-video","description":"Video understanding model for frame analysis, captioning, and temporal reasoning.","icon":"$undefined","url":"/docs/models/zen-video","$ref":{"file":"models/zen-video.mdx"}},{"$id":"models/zen-vl.mdx","type":"page","name":"zen-vl","description":"32B dense multimodal model for vision-language understanding.","icon":"$undefined","url":"/docs/models/zen-vl","$ref":{"file":"models/zen-vl.mdx"}},{"$id":"models/zen-voyager.mdx","type":"page","name":"zen-voyager","description":"World model for spatial reasoning and 3D scene understanding.","icon":"$undefined","url":"/docs/models/zen-voyager","$ref":{"file":"models/zen-voyager.mdx"}},{"$id":"models/zen-world.mdx","type":"page","name":"zen-world","description":"World simulation model for spatial reasoning and environment generation.","icon":"$undefined","url":"/docs/models/zen-world","$ref":{"file":"models/zen-world.mdx"}},{"$id":"models/zen.mdx","type":"page","name":"zen","description":"Standard 8-32B dense foundation model for general-purpose AI tasks.","icon":"$undefined","url":"/docs/models/zen","$ref":{"file":"models/zen.mdx"}},{"$id":"models/zen3-asr-v1.mdx","type":"page","name":"zen3-asr-v1","description":"First-generation streaming ASR for legacy compatibility.","icon":"$undefined","url":"/docs/models/zen3-asr-v1","$ref":{"file":"models/zen3-asr-v1.mdx"}},{"$id":"models/zen3-asr.mdx","type":"page","name":"zen3-asr","description":"Real-time streaming speech recognition for live transcription and voice agents.","icon":"$undefined","url":"/docs/models/zen3-asr","$ref":{"file":"models/zen3-asr.mdx"}},{"$id":"models/zen3-audio-fast.mdx","type":"page","name":"zen3-audio-fast","description":"Fastest speech-to-text transcription for high-throughput workloads.","icon":"$undefined","url":"/docs/models/zen3-audio-fast","$ref":{"file":"models/zen3-audio-fast.mdx"}},{"$id":"models/zen3-audio.mdx","type":"page","name":"zen3-audio","description":"Best quality speech-to-text transcription. 100+ languages.","icon":"$undefined","url":"/docs/models/zen3-audio","$ref":{"file":"models/zen3-audio.mdx"}},{"$id":"models/zen3-embedding-medium.mdx","type":"page","name":"zen3-embedding-medium","description":"Balanced embedding model for cost-effective retrieval workloads.","icon":"$undefined","url":"/docs/models/zen3-embedding-medium","$ref":{"file":"models/zen3-embedding-medium.mdx"}},{"$id":"models/zen3-embedding-small.mdx","type":"page","name":"zen3-embedding-small","description":"Lightweight embedding model for high-throughput, low-cost applications.","icon":"$undefined","url":"/docs/models/zen3-embedding-small","$ref":{"file":"models/zen3-embedding-small.mdx"}},{"$id":"models/zen3-embedding.mdx","type":"page","name":"zen3-embedding","description":"High-quality text embedding model with 3072 dimensions and 8K context.","icon":"$undefined","url":"/docs/models/zen3-embedding","$ref":{"file":"models/zen3-embedding.mdx"}},{"$id":"models/zen3-guard.mdx","type":"page","name":"zen3-guard","description":"Content safety classifier with 4B dense architecture. 65K context.","icon":"$undefined","url":"/docs/models/zen3-guard","$ref":{"file":"models/zen3-guard.mdx"}},{"$id":"models/zen3-image-dev.mdx","type":"page","name":"zen3-image-dev","description":"Development model for experimentation and iteration.","icon":"$undefined","url":"/docs/models/zen3-image-dev","$ref":{"file":"models/zen3-image-dev.mdx"}},{"$id":"models/zen3-image-fast.mdx","type":"page","name":"zen3-image-fast","description":"Fastest image model for real-time generation.","icon":"$undefined","url":"/docs/models/zen3-image-fast","$ref":{"file":"models/zen3-image-fast.mdx"}},{"$id":"models/zen3-image-jp.mdx","type":"page","name":"zen3-image-jp","description":"Japanese-specialized image generation model.","icon":"$undefined","url":"/docs/models/zen3-image-jp","$ref":{"file":"models/zen3-image-jp.mdx"}},{"$id":"models/zen3-image-max.mdx","type":"page","name":"zen3-image-max","description":"Maximum quality image generation for professional creative work.","icon":"$undefined","url":"/docs/models/zen3-image-max","$ref":{"file":"models/zen3-image-max.mdx"}},{"$id":"models/zen3-image-playground.mdx","type":"page","name":"zen3-image-playground","description":"Aesthetic model for artistic image generation.","icon":"$undefined","url":"/docs/models/zen3-image-playground","$ref":{"file":"models/zen3-image-playground.mdx"}},{"$id":"models/zen3-image-sdxl.mdx","type":"page","name":"zen3-image-sdxl","description":"High-resolution image generation at 1024px.","icon":"$undefined","url":"/docs/models/zen3-image-sdxl","$ref":{"file":"models/zen3-image-sdxl.mdx"}},{"$id":"models/zen3-image-ssd.mdx","type":"page","name":"zen3-image-ssd","description":"Fastest diffusion model for real-time generation.","icon":"$undefined","url":"/docs/models/zen3-image-ssd","$ref":{"file":"models/zen3-image-ssd.mdx"}},{"$id":"models/zen3-image.mdx","type":"page","name":"zen3-image","description":"Best general-purpose image generation.","icon":"$undefined","url":"/docs/models/zen3-image","$ref":{"file":"models/zen3-image.mdx"}},{"$id":"models/zen3-nano.mdx","type":"page","name":"zen3-nano","description":"Ultra-lightweight 4B dense model for edge deployment. 40K context.","icon":"$undefined","url":"/docs/models/zen3-nano","$ref":{"file":"models/zen3-nano.mdx"}},{"$id":"models/zen3-omni.mdx","type":"page","name":"zen3-omni","description":"Hypermodal ~200B dense model supporting text, vision, and audio. 202K context.","icon":"$undefined","url":"/docs/models/zen3-omni","$ref":{"file":"models/zen3-omni.mdx"}},{"$id":"models/zen3-reranker-medium.mdx","type":"page","name":"zen3-reranker-medium","description":"Balanced reranker for cost-effective retrieval quality improvement.","icon":"$undefined","url":"/docs/models/zen3-reranker-medium","$ref":{"file":"models/zen3-reranker-medium.mdx"}},{"$id":"models/zen3-reranker-small.mdx","type":"page","name":"zen3-reranker-small","description":"Lightweight reranker for high-throughput reranking at minimal cost.","icon":"$undefined","url":"/docs/models/zen3-reranker-small","$ref":{"file":"models/zen3-reranker-small.mdx"}},{"$id":"models/zen3-reranker.mdx","type":"page","name":"zen3-reranker","description":"High-quality reranker for improving retrieval accuracy in RAG pipelines.","icon":"$undefined","url":"/docs/models/zen3-reranker","$ref":{"file":"models/zen3-reranker.mdx"}},{"$id":"models/zen3-tts-fast.mdx","type":"page","name":"zen3-tts-fast","description":"Low-latency text-to-speech for real-time voice agents and interactive applications.","icon":"$undefined","url":"/docs/models/zen3-tts-fast","$ref":{"file":"models/zen3-tts-fast.mdx"}},{"$id":"models/zen3-tts-hd.mdx","type":"page","name":"zen3-tts-hd","description":"Maximum fidelity text-to-speech for broadcast-quality audio production.","icon":"$undefined","url":"/docs/models/zen3-tts-hd","$ref":{"file":"models/zen3-tts-hd.mdx"}},{"$id":"models/zen3-tts.mdx","type":"page","name":"zen3-tts","description":"High-quality text-to-speech with natural prosody. 40+ voices, 8 languages.","icon":"$undefined","url":"/docs/models/zen3-tts","$ref":{"file":"models/zen3-tts.mdx"}},{"$id":"models/zen3-vl.mdx","type":"page","name":"zen3-vl","description":"Vision-language model with 30B (3B active) MoE architecture. 262K context.","icon":"$undefined","url":"/docs/models/zen3-vl","$ref":{"file":"models/zen3-vl.mdx"}},{"$id":"models/zen4-coder-flash.mdx","type":"page","name":"zen4-coder-flash","description":"Fast 30B (3B active) MoE code model with 262K context.","icon":"$undefined","url":"/docs/models/zen4-coder-flash","$ref":{"file":"models/zen4-coder-flash.mdx"}},{"$id":"models/zen4-coder-pro.mdx","type":"page","name":"zen4-coder-pro","description":"Premium 480B full-precision BF16 code model with 131K context.","icon":"$undefined","url":"/docs/models/zen4-coder-pro","$ref":{"file":"models/zen4-coder-pro.mdx"}},{"$id":"models/zen4-coder.mdx","type":"page","name":"zen4-coder","description":"Code generation model with 480B (35B active) MoE and 163K context.","icon":"$undefined","url":"/docs/models/zen4-coder","$ref":{"file":"models/zen4-coder.mdx"}},{"$id":"models/zen4-max.mdx","type":"page","name":"zen4-max","description":"Trillion-parameter frontier MoE model. 1.04T (32B active) with 163K context.","icon":"$undefined","url":"/docs/models/zen4-max","$ref":{"file":"models/zen4-max.mdx"}},{"$id":"models/zen4-mini.mdx","type":"page","name":"zen4-mini","description":"Fast and efficient 8B dense model with 40K context.","icon":"$undefined","url":"/docs/models/zen4-mini","$ref":{"file":"models/zen4-mini.mdx"}},{"$id":"models/zen4-pro.mdx","type":"page","name":"zen4-pro","description":"High capability MoE model. 80B (3B active) with 131K context.","icon":"$undefined","url":"/docs/models/zen4-pro","$ref":{"file":"models/zen4-pro.mdx"}},{"$id":"models/zen4-thinking.mdx","type":"page","name":"zen4-thinking","description":"Deep reasoning model with 80B (3B active) MoE + chain-of-thought. 131K context.","icon":"$undefined","url":"/docs/models/zen4-thinking","$ref":{"file":"models/zen4-thinking.mdx"}},{"$id":"models/zen4-ultra.mdx","type":"page","name":"zen4-ultra","description":"Maximum reasoning model with 744B MoE (40B active) + extended chain-of-thought. 262K context.","icon":"$undefined","url":"/docs/models/zen4-ultra","$ref":{"file":"models/zen4-ultra.mdx"}},{"$id":"models/zen4.1.mdx","type":"page","name":"zen4.1","description":"High-performance 1M context model for long-document analysis, large codebase reasoning, and agentic workflows. Best balance of intelligence and cost at million-token scale.","icon":"$undefined","url":"/docs/models/zen4.1","$ref":{"file":"models/zen4.1.mdx"}},{"$id":"models/zen4.mdx","type":"page","name":"zen4","description":"Flagship 744B MoE model with 40B active parameters and 202K context window.","icon":"$undefined","url":"/docs/models/zen4","$ref":{"file":"models/zen4.mdx"}},{"$id":"models/zen5-coder.mdx","type":"page","name":"zen5-coder","description":"Next-generation code model. Zen5 architecture optimized for agentic programming. Research Preview.","icon":"$undefined","url":"/docs/models/zen5-coder","$ref":{"file":"models/zen5-coder.mdx"}},{"$id":"models/zen5-max.mdx","type":"page","name":"zen5-max","description":"Maximum-scale Zen5 model. Multi-trillion parameter frontier MoE. Research Preview.","icon":"$undefined","url":"/docs/models/zen5-max","$ref":{"file":"models/zen5-max.mdx"}},{"$id":"models/zen5-mini.mdx","type":"page","name":"zen5-mini","description":"Efficient agentic model delivering zen5-class intelligence at a fraction of the cost.","icon":"$undefined","url":"/docs/models/zen5-mini","$ref":{"file":"models/zen5-mini.mdx"}},{"$id":"models/zen5-pro.mdx","type":"page","name":"zen5-pro","description":"High-throughput agentic model for demanding production workloads. Trained on real-world development patterns with deep chain-of-thought reasoning.","icon":"$undefined","url":"/docs/models/zen5-pro","$ref":{"file":"models/zen5-pro.mdx"}},{"$id":"models/zen5-ultra.mdx","type":"page","name":"zen5-ultra","description":"Deepest reasoning model in the Zen family. Multi-pass chain-of-thought with self-verification.","icon":"$undefined","url":"/docs/models/zen5-ultra","$ref":{"file":"models/zen5-ultra.mdx"}},{"$id":"models/zen5.mdx","type":"page","name":"zen5","description":"Next-generation flagship model. 2T+ MoE with on-chain training via NVIDIA TEE. Research Preview.","icon":"$undefined","url":"/docs/models/zen5","$ref":{"file":"models/zen5.mdx"}}],"$id":"models","$ref":"$undefined","icon":"$undefined"},{"type":"folder","name":"Training","root":"$undefined","defaultOpen":"$undefined","description":"$undefined","collapsible":"$undefined","children":[{"$id":"training/cloud.mdx","type":"page","name":"Cloud Training","description":"Full-scale training on 8x H200 GPUs","icon":"$undefined","url":"/docs/training/cloud","$ref":{"file":"training/cloud.mdx"}},{"$id":"training/cuda.mdx","type":"page","name":"CUDA Training","description":"Train locally with NVIDIA GPUs","icon":"$undefined","url":"/docs/training/cuda","$ref":{"file":"training/cuda.mdx"}},{"$id":"training/mlx.mdx","type":"page","name":"MLX Training","description":"Train on Apple Silicon with MLX","icon":"$undefined","url":"/docs/training/mlx","$ref":{"file":"training/mlx.mdx"}},{"$id":"training/overview.mdx","type":"page","name":"Training Overview","description":"Train Zen models with multiple backend options","icon":"$undefined","url":"/docs/training/overview","$ref":{"file":"training/overview.mdx"}}],"$id":"training","$ref":"$undefined","icon":"$undefined"},{"$id":"datasets.mdx","type":"page","name":"Zen Agentic Dataset","description":"Zen Agentic Dataset - 8.47 billion tokens of real-world agentic programming","icon":"$undefined","url":"/docs/datasets","$ref":{"file":"datasets.mdx"}}],"fallback":{"$id":"fallback:root","name":"Docs","children":[{"$id":"fallback:models.mdx","type":"page","name":"Models","description":"Complete Zen LM model family -- 49 models across 10 modalities","icon":"$undefined","url":"/docs/models","$ref":{"file":"models.mdx"}},{"$id":"fallback:training.mdx","type":"page","name":"Fine-tuning Guide","description":"Fine-tune Zen4 models with MLX, Unsloth, or DeepSpeed","icon":"$undefined","url":"/docs/training","$ref":{"file":"training.mdx"}}]}},"children":"$L6"}]]}],{"children":["$L7",{"children":["$L8",{},null,false,false]},null,false,false]},null,false,false]},null,false,false],"$L9",false]],"m":"$undefined","G":["$a",[]],"S":true}
b:I[99924,["/_next/static/chunks/8d687ef04ae593a8.js","/_next/static/chunks/c366ecc986f0489d.js","/_next/static/chunks/d5ccaca76587500f.js","/_next/static/chunks/4032bb4803b95120.js","/_next/static/chunks/4b7dc5f6fcc90c00.js"],"LayoutContextProvider"]
c:I[32824,["/_next/static/chunks/8d687ef04ae593a8.js","/_next/static/chunks/c366ecc986f0489d.js","/_next/static/chunks/d5ccaca76587500f.js","/_next/static/chunks/4032bb4803b95120.js","/_next/static/chunks/4b7dc5f6fcc90c00.js"],"SidebarProvider"]
d:I[99924,["/_next/static/chunks/8d687ef04ae593a8.js","/_next/static/chunks/c366ecc986f0489d.js","/_next/static/chunks/d5ccaca76587500f.js","/_next/static/chunks/4032bb4803b95120.js","/_next/static/chunks/4b7dc5f6fcc90c00.js"],"LayoutBody"]
e:I[99924,["/_next/static/chunks/8d687ef04ae593a8.js","/_next/static/chunks/c366ecc986f0489d.js","/_next/static/chunks/d5ccaca76587500f.js","/_next/static/chunks/4032bb4803b95120.js","/_next/static/chunks/4b7dc5f6fcc90c00.js"],"LayoutHeader"]
f:I[48068,["/_next/static/chunks/8d687ef04ae593a8.js","/_next/static/chunks/c366ecc986f0489d.js","/_next/static/chunks/d5ccaca76587500f.js","/_next/static/chunks/4032bb4803b95120.js","/_next/static/chunks/4b7dc5f6fcc90c00.js","/_next/static/chunks/ad34b5d9cdfd228f.js","/_next/static/chunks/bb54b9a11eec46b3.js"],"default"]
10:I[80761,["/_next/static/chunks/8d687ef04ae593a8.js","/_next/static/chunks/c366ecc986f0489d.js","/_next/static/chunks/d5ccaca76587500f.js","/_next/static/chunks/4032bb4803b95120.js","/_next/static/chunks/4b7dc5f6fcc90c00.js"],"SearchToggle"]
11:I[32824,["/_next/static/chunks/8d687ef04ae593a8.js","/_next/static/chunks/c366ecc986f0489d.js","/_next/static/chunks/d5ccaca76587500f.js","/_next/static/chunks/4032bb4803b95120.js","/_next/static/chunks/4b7dc5f6fcc90c00.js"],"SidebarTrigger"]
12:I[80157,["/_next/static/chunks/8d687ef04ae593a8.js","/_next/static/chunks/c366ecc986f0489d.js","/_next/static/chunks/d5ccaca76587500f.js","/_next/static/chunks/4032bb4803b95120.js","/_next/static/chunks/4b7dc5f6fcc90c00.js"],"SidebarContent"]
13:I[32824,["/_next/static/chunks/8d687ef04ae593a8.js","/_next/static/chunks/c366ecc986f0489d.js","/_next/static/chunks/d5ccaca76587500f.js","/_next/static/chunks/4032bb4803b95120.js","/_next/static/chunks/4b7dc5f6fcc90c00.js"],"SidebarCollapseTrigger"]
14:I[80761,["/_next/static/chunks/8d687ef04ae593a8.js","/_next/static/chunks/c366ecc986f0489d.js","/_next/static/chunks/d5ccaca76587500f.js","/_next/static/chunks/4032bb4803b95120.js","/_next/static/chunks/4b7dc5f6fcc90c00.js"],"LargeSearchToggle"]
15:I[32824,["/_next/static/chunks/8d687ef04ae593a8.js","/_next/static/chunks/c366ecc986f0489d.js","/_next/static/chunks/d5ccaca76587500f.js","/_next/static/chunks/4032bb4803b95120.js","/_next/static/chunks/4b7dc5f6fcc90c00.js"],"SidebarViewport"]
16:I[80157,["/_next/static/chunks/8d687ef04ae593a8.js","/_next/static/chunks/c366ecc986f0489d.js","/_next/static/chunks/d5ccaca76587500f.js","/_next/static/chunks/4032bb4803b95120.js","/_next/static/chunks/4b7dc5f6fcc90c00.js"],"SidebarLinkItem"]
17:I[80157,["/_next/static/chunks/8d687ef04ae593a8.js","/_next/static/chunks/c366ecc986f0489d.js","/_next/static/chunks/d5ccaca76587500f.js","/_next/static/chunks/4032bb4803b95120.js","/_next/static/chunks/4b7dc5f6fcc90c00.js"],"SidebarPageTree"]
18:I[73332,["/_next/static/chunks/8d687ef04ae593a8.js","/_next/static/chunks/c366ecc986f0489d.js","/_next/static/chunks/d5ccaca76587500f.js","/_next/static/chunks/4032bb4803b95120.js","/_next/static/chunks/4b7dc5f6fcc90c00.js"],"ThemeToggle"]
19:I[80157,["/_next/static/chunks/8d687ef04ae593a8.js","/_next/static/chunks/c366ecc986f0489d.js","/_next/static/chunks/d5ccaca76587500f.js","/_next/static/chunks/4032bb4803b95120.js","/_next/static/chunks/4b7dc5f6fcc90c00.js"],"SidebarDrawer"]
1c:I[89923,["/_next/static/chunks/4d80e004cf4896dd.js","/_next/static/chunks/350ee4303b732916.js"],"OutletBoundary"]
1d:"$Sreact.suspense"
1f:I[89923,["/_next/static/chunks/4d80e004cf4896dd.js","/_next/static/chunks/350ee4303b732916.js"],"ViewportBoundary"]
21:I[89923,["/_next/static/chunks/4d80e004cf4896dd.js","/_next/static/chunks/350ee4303b732916.js"],"MetadataBoundary"]
6:["$","$Lb",null,{"navTransparentMode":"$undefined","children":["$","$Lc",null,{"defaultOpenLevel":"$undefined","prefetch":"$undefined","children":["$","$Ld",null,{"children":[["$","$Le",null,{"id":"nd-subnav","className":"[grid-area:header] sticky top-(--fd-docs-row-1) z-30 flex items-center ps-4 pe-2.5 border-b transition-colors backdrop-blur-sm h-(--fd-header-height) md:hidden max-md:layout:[--fd-header-height:--spacing(14)] data-[transparent=false]:bg-fd-background/80","children":[["$","$Lf",null,{"href":"/","className":"inline-flex items-center gap-2.5 font-semibold","children":"ðŸª· Zen LM"}],["$","div",null,{"className":"flex-1","children":"$undefined"}],["$","$L10",null,{"className":"p-2","hideIfDisabled":true}],["$","$L11",null,{"className":"inline-flex items-center justify-center rounded-md text-sm font-medium transition-colors duration-100 disabled:pointer-events-none disabled:opacity-50 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-fd-ring hover:bg-fd-accent hover:text-fd-accent-foreground [&_svg]:size-4.5 p-2","children":["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-panel-left","aria-hidden":"true","children":[["$","rect","afitv7",{"width":"18","height":"18","x":"3","y":"3","rx":"2"}],["$","path","fh3hqa",{"d":"M9 3v18"}],"$undefined"]}]}]]}],[["$","$L12",null,{"children":[["$","div",null,{"className":"flex flex-col gap-3 p-4 pb-2","children":[["$","div",null,{"className":"flex","children":[["$","$Lf",null,{"href":"/","className":"inline-flex text-[0.9375rem] items-center gap-2.5 font-medium me-auto","children":"ðŸª· Zen LM"}],"$undefined",["$","$L13",null,{"className":"inline-flex items-center justify-center rounded-md text-sm font-medium transition-colors duration-100 disabled:pointer-events-none disabled:opacity-50 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-fd-ring hover:bg-fd-accent hover:text-fd-accent-foreground p-1.5 [&_svg]:size-4.5 mb-auto text-fd-muted-foreground","children":["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-panel-left","aria-hidden":"true","children":[["$","rect","afitv7",{"width":"18","height":"18","x":"3","y":"3","rx":"2"}],["$","path","fh3hqa",{"d":"M9 3v18"}],"$undefined"]}]}]]}],["$","$L14",null,{"hideIfDisabled":true}],false,["$","div",null,{"className":"p-3 rounded-lg bg-primary/10 text-sm","children":[["$","strong",null,{"children":"zen4-max"}]," â€” 1T+ MoE frontier model"]}]]}],["$","$L15",null,{"children":[[["$","$L16","0",{"item":{"text":"HuggingFace","url":"https://huggingface.co/zenlm"},"className":""}],["$","$L16","1",{"item":{"text":"GitHub","url":"https://github.com/zenlm"},"className":"mb-4"}]],["$","$L17",null,{}]]}],["$","div",null,{"className":"flex flex-col border-t p-4 pt-2 empty:hidden","children":[["$","div",null,{"className":"flex text-fd-muted-foreground items-center empty:hidden","children":[false,[],["$","$L18",null,{"className":"ms-auto p-0","mode":"$undefined"}]]}],"$undefined"]}]]}],["$","$L19",null,{"children":[["$","div",null,{"className":"flex flex-col gap-3 p-4 pb-2","children":[["$","div",null,{"className":"flex text-fd-muted-foreground items-center gap-1.5","children":[["$","div",null,{"className":"flex flex-1","children":[]}],false,["$","$L18",null,{"className":"p-0","mode":"$undefined"}],["$","$L11",null,{"className":"inline-flex items-center justify-center rounded-md text-sm font-medium transition-colors duration-100 disabled:pointer-events-none disabled:opacity-50 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-fd-ring hover:bg-fd-accent hover:text-fd-accent-foreground [&_svg]:size-4.5 p-2","children":["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-panel-left","aria-hidden":"true","children":[["$","rect","afitv7",{"width":"18","height":"18","x":"3","y":"3","rx":"2"}],["$","path","fh3hqa",{"d":"M9 3v18"}],"$undefined"]}]}]]}],false,"$6:props:children:props:children:props:children:1:0:props:children:0:props:children:3"]}],"$6:props:children:props:children:props:children:1:0:props:children:1",["$","div",null,{"className":"flex flex-col border-t p-4 pt-2 empty:hidden","children":"$undefined"}]]}]],false,"$L1a"]}]}]}]
7:["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}]
8:["$","$1","c",{"children":["$L1b",[["$","script","script-0",{"src":"/_next/static/chunks/ad34b5d9cdfd228f.js","async":true,"nonce":"$undefined"}],["$","script","script-1",{"src":"/_next/static/chunks/bb54b9a11eec46b3.js","async":true,"nonce":"$undefined"}]],["$","$L1c",null,{"children":["$","$1d",null,{"name":"Next.MetadataOutlet","children":"$@1e"}]}]]}]
9:["$","$1","h",{"children":[null,["$","$L1f",null,{"children":"$L20"}],["$","div",null,{"hidden":true,"children":["$","$L21",null,{"children":["$","$1d",null,{"name":"Next.Metadata","children":"$L22"}]}]}],null]}]
1a:["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]
23:I[70258,["/_next/static/chunks/8d687ef04ae593a8.js","/_next/static/chunks/c366ecc986f0489d.js","/_next/static/chunks/d5ccaca76587500f.js","/_next/static/chunks/4032bb4803b95120.js","/_next/static/chunks/4b7dc5f6fcc90c00.js","/_next/static/chunks/ad34b5d9cdfd228f.js","/_next/static/chunks/bb54b9a11eec46b3.js"],"DocsPage"]
24:I[70258,["/_next/static/chunks/8d687ef04ae593a8.js","/_next/static/chunks/c366ecc986f0489d.js","/_next/static/chunks/d5ccaca76587500f.js","/_next/static/chunks/4032bb4803b95120.js","/_next/static/chunks/4b7dc5f6fcc90c00.js","/_next/static/chunks/ad34b5d9cdfd228f.js","/_next/static/chunks/bb54b9a11eec46b3.js"],"DocsTitle"]
25:I[70258,["/_next/static/chunks/8d687ef04ae593a8.js","/_next/static/chunks/c366ecc986f0489d.js","/_next/static/chunks/d5ccaca76587500f.js","/_next/static/chunks/4032bb4803b95120.js","/_next/static/chunks/4b7dc5f6fcc90c00.js","/_next/static/chunks/ad34b5d9cdfd228f.js","/_next/static/chunks/bb54b9a11eec46b3.js"],"DocsDescription"]
26:I[70258,["/_next/static/chunks/8d687ef04ae593a8.js","/_next/static/chunks/c366ecc986f0489d.js","/_next/static/chunks/d5ccaca76587500f.js","/_next/static/chunks/4032bb4803b95120.js","/_next/static/chunks/4b7dc5f6fcc90c00.js","/_next/static/chunks/ad34b5d9cdfd228f.js","/_next/static/chunks/bb54b9a11eec46b3.js"],"DocsBody"]
1b:["$","$L23",null,{"toc":[{"depth":1,"url":"#zen3-tts-fast","title":"zen3-tts-fast"},{"depth":2,"url":"#specifications","title":"Specifications"},{"depth":2,"url":"#capabilities","title":"Capabilities"},{"depth":2,"url":"#api-usage","title":"API Usage"},{"depth":2,"url":"#try-it","title":"Try It"},{"depth":2,"url":"#resources","title":"Resources"},{"depth":2,"url":"#see-also","title":"See Also"}],"children":[["$","$L24",null,{"children":"zen3-tts-fast"}],["$","$L25",null,{"children":"Low-latency text-to-speech for real-time voice agents and interactive applications."}],["$","$L26",null,{"children":[["$","h1",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"zen3-tts-fast","children":[["$","a",null,{"data-card":"","href":"#zen3-tts-fast","className":"peer","children":"zen3-tts-fast"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}],"\n",["$","p",null,{"children":["$","strong",null,{"children":"Low-Latency Voice Synthesis"}]}],"\n",["$","p",null,{"children":"An 82M parameter low-latency text-to-speech model built for real-time voice agents and interactive applications. Streams audio output with minimal first-byte latency, enabling fluid conversational AI experiences."}],"\n",["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"specifications","children":[["$","a",null,{"data-card":"","href":"#specifications","className":"peer","children":"Specifications"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}],"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",["$","div",null,{"className":"relative overflow-auto prose-no-margin my-6","children":["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"Property"}],["$","th",null,{"children":"Value"}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"Model ID"}]}],["$","td",null,{"children":["$","code",null,{"children":"zen3-tts-fast"}]}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"Parameters"}]}],["$","td",null,{"children":"82M"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"Architecture"}]}],["$","td",null,{"children":"TTS"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"Output"}]}],["$","td",null,{"children":"Audio (MP3, WAV, OPUS)"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"Tier"}]}],["$","td",null,{"children":"pro"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"Status"}]}],["$","td",null,{"children":"Available"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","strong",null,{"children":"Deployment"}]}],["$","td",null,{"children":"API only"}]]}]]}]]}]}],"\n",["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"capabilities","children":[["$","a",null,{"data-card":"","href":"#capabilities","className":"peer","children":"Capabilities"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}],"\n","$L27","\n","$L28","\n","$L29","\n","$L2a","\n","$L2b","\n","$L2c","\n","$L2d","\n","$L2e","\n","$L2f","\n","$L30"]}]]}]
31:I[51504,["/_next/static/chunks/8d687ef04ae593a8.js","/_next/static/chunks/c366ecc986f0489d.js","/_next/static/chunks/d5ccaca76587500f.js","/_next/static/chunks/4032bb4803b95120.js","/_next/static/chunks/4b7dc5f6fcc90c00.js","/_next/static/chunks/ad34b5d9cdfd228f.js","/_next/static/chunks/bb54b9a11eec46b3.js"],"CodeBlock"]
32:I[51504,["/_next/static/chunks/8d687ef04ae593a8.js","/_next/static/chunks/c366ecc986f0489d.js","/_next/static/chunks/d5ccaca76587500f.js","/_next/static/chunks/4032bb4803b95120.js","/_next/static/chunks/4b7dc5f6fcc90c00.js","/_next/static/chunks/ad34b5d9cdfd228f.js","/_next/static/chunks/bb54b9a11eec46b3.js"],"Pre"]
27:["$","ul",null,{"children":["\n",["$","li",null,{"children":"Low first-byte latency for conversational responsiveness"}],"\n",["$","li",null,{"children":"Streaming audio output for real-time playback"}],"\n",["$","li",null,{"children":"Voice agent and chatbot integration"}],"\n",["$","li",null,{"children":"Interactive voice response (IVR) systems"}],"\n",["$","li",null,{"children":"Real-time narration and announcements"}],"\n",["$","li",null,{"children":"High-throughput TTS for cost-efficient scale"}],"\n"]}]
28:["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"api-usage","children":[["$","a",null,{"data-card":"","href":"#api-usage","className":"peer","children":"API Usage"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
29:["$","$L31",null,{"className":"shiki shiki-themes github-light github-dark","style":{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},"tabIndex":"0","icon":"<svg viewBox=\"0 0 24 24\"><path d=\"m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z\" fill=\"currentColor\" /></svg>","children":["$","$L32",null,{"children":["$","code",null,{"children":[["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},"children":"curl"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":" https://api.hanzo.ai/v1/audio/speech"}],["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":" \\"}]]}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":"  -H"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":" \"Authorization: Bearer "}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"$$HANZO_API_KEY"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\""}],["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":" \\"}]]}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":"  -H"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":" \"Content-Type: application/json\""}],["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":" \\"}]]}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":"  -d"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":" '{"}]]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"    \"model\": \"zen3-tts-fast\","}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"    \"input\": \"Your order has been confirmed. It will arrive in 3 to 5 business days.\","}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"    \"voice\": \"alloy\","}]}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"    \"response_format\": \"opus\""}]}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"  }'"}],["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":" \\"}]]}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":"  --output"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":" response.opus"}]]}]]}]}]}]
33:T5c0,<svg viewBox="0 0 24 24"><path d="M14.25.18l.9.2.73.26.59.3.45.32.34.34.25.34.16.33.1.3.04.26.02.2-.01.13V8.5l-.05.63-.13.55-.21.46-.26.38-.3.31-.33.25-.35.19-.35.14-.33.1-.3.07-.26.04-.21.02H8.77l-.69.05-.59.14-.5.22-.41.27-.33.32-.27.35-.2.36-.15.37-.1.35-.07.32-.04.27-.02.21v3.06H3.17l-.21-.03-.28-.07-.32-.12-.35-.18-.36-.26-.36-.36-.35-.46-.32-.59-.28-.73-.21-.88-.14-1.05-.05-1.23.06-1.22.16-1.04.24-.87.32-.71.36-.57.4-.44.42-.33.42-.24.4-.16.36-.1.32-.05.24-.01h.16l.06.01h8.16v-.83H6.18l-.01-2.75-.02-.37.05-.34.11-.31.17-.28.25-.26.31-.23.38-.2.44-.18.51-.15.58-.12.64-.1.71-.06.77-.04.84-.02 1.27.05zm-6.3 1.98l-.23.33-.08.41.08.41.23.34.33.22.41.09.41-.09.33-.22.23-.34.08-.41-.08-.41-.23-.33-.33-.22-.41-.09-.41.09zm13.09 3.95l.28.06.32.12.35.18.36.27.36.35.35.47.32.59.28.73.21.88.14 1.04.05 1.23-.06 1.23-.16 1.04-.24.86-.32.71-.36.57-.4.45-.42.33-.42.24-.4.16-.36.09-.32.05-.24.02-.16-.01h-8.22v.82h5.84l.01 2.76.02.36-.05.34-.11.31-.17.29-.25.25-.31.24-.38.2-.44.17-.51.15-.58.13-.64.09-.71.07-.77.04-.84.01-1.27-.04-1.07-.14-.9-.2-.73-.25-.59-.3-.45-.33-.34-.34-.25-.34-.16-.33-.1-.3-.04-.25-.02-.2.01-.13v-5.34l.05-.64.13-.54.21-.46.26-.38.3-.32.33-.24.35-.2.35-.14.33-.1.3-.06.26-.04.21-.02.13-.01h5.84l.69-.05.59-.14.5-.21.41-.28.33-.32.27-.35.2-.36.15-.36.1-.35.07-.32.04-.28.02-.21V6.07h2.09l.14.01zm-6.47 14.25l-.23.33-.08.41.08.41.23.33.33.23.41.08.41-.08.33-.23.23-.33.08-.41-.08-.41-.23-.33-.33-.23-.41-.08-.41.08z" fill="currentColor" /></svg>2a:["$","$L31",null,{"className":"shiki shiki-themes github-light github-dark","style":{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},"tabIndex":"0","icon":"$33","children":["$","$L32",null,{"children":["$","code",null,{"children":[["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"from"}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":" hanzoai "}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"import"}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":" Hanzo"}]]}],"\n",["$","span",null,{"className":"line"}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"client "}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"="}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":" Hanzo("}],["$","span",null,{"style":{"--shiki-light":"#E36209","--shiki-dark":"#FFAB70"},"children":"api_key"}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"="}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\"hk-your-api-key\""}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":")"}]]}],"\n",["$","span",null,{"className":"line"}],"\n",["$","span",null,{"className":"line","children":["$","span",null,{"style":{"--shiki-light":"#6A737D","--shiki-dark":"#6A737D"},"children":"# Streaming for real-time playback"}]}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"with"}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":" client.audio.speech.with_streaming_response.create("}]]}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#E36209","--shiki-dark":"#FFAB70"},"children":"    model"}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"="}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\"zen3-tts-fast\""}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":","}]]}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#E36209","--shiki-dark":"#FFAB70"},"children":"    input"}],"$L34","$L35","$L36"]}],"\n","$L37","\n","$L38","\n","$L39","\n","$L3a"]}]}]}]
2b:["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"try-it","children":[["$","a",null,{"data-card":"","href":"#try-it","className":"peer","children":"Try It"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
2c:["$","p",null,{"children":["$","$Lf",null,{"href":"https://hanzo.chat?model=zen3-tts-fast","children":"Open in Hanzo Chat"}]}]
2d:["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"resources","children":[["$","a",null,{"data-card":"","href":"#resources","className":"peer","children":"Resources"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
2e:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","$Lf",null,{"href":"/docs/api/audio","children":"Audio API"}]," -- Endpoint documentation"]}],"\n",["$","li",null,{"children":["$","$Lf",null,{"href":"https://github.com/zenlm/zen/blob/main/paper/zen-technical-report.pdf","children":"Technical Report"}]}],"\n"]}]
2f:["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"see-also","children":[["$","a",null,{"data-card":"","href":"#see-also","className":"peer","children":"See Also"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-link size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-hidden":true,"children":[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}],"$undefined"]}]]}]
30:["$","ul",null,{"children":["\n",["$","li",null,{"children":[["$","$Lf",null,{"href":"/docs/models/zen3-tts","children":"zen3-tts"}]," -- High-quality TTS with 40+ voices"]}],"\n",["$","li",null,{"children":[["$","$Lf",null,{"href":"/docs/models/zen3-tts-hd","children":"zen3-tts-hd"}]," -- Broadcast-quality audio production"]}],"\n",["$","li",null,{"children":[["$","$Lf",null,{"href":"/docs/models/zen3-asr","children":"zen3-asr"}]," -- Real-time streaming speech recognition"]}],"\n"]}]
34:["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"="}]
35:["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\"Your order has been confirmed.\""}]
36:["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":","}]
37:["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#E36209","--shiki-dark":"#FFAB70"},"children":"    voice"}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"="}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\"alloy\""}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":","}]]}]
38:["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#E36209","--shiki-dark":"#FFAB70"},"children":"    response_format"}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"="}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\"opus\""}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":","}]]}]
39:["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":") "}],["$","span",null,{"style":{"--shiki-light":"#D73A49","--shiki-dark":"#F97583"},"children":"as"}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":" response:"}]]}]
3a:["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"    response.stream_to_file("}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":"\"response.opus\""}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":")"}]]}]
20:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","2",{"name":"theme-color","media":"(prefers-color-scheme: dark)","content":"#0A0A0A"}],["$","meta","3",{"name":"theme-color","media":"(prefers-color-scheme: light)","content":"#fff"}]]
1e:null
22:[["$","title","0",{"children":"zen3-tts-fast | Zen LM"}],["$","meta","1",{"name":"description","content":"Low-latency text-to-speech for real-time voice agents and interactive applications."}],["$","meta","2",{"property":"og:title","content":"Zen LM - Open Foundation Models"}],["$","meta","3",{"property":"og:description","content":"Zen AI model family by Hanzo AI. 14 frontier models from 4B to 1T+ parameters for code, reasoning, vision, and multimodal tasks."}],["$","meta","4",{"property":"og:url","content":"https://zenlm.org"}],["$","meta","5",{"property":"og:site_name","content":"Zen LM"}],["$","meta","6",{"property":"og:type","content":"website"}],["$","meta","7",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","8",{"name":"twitter:site","content":"@zenlmorg"}],["$","meta","9",{"name":"twitter:creator","content":"@zenlmorg"}],["$","meta","10",{"name":"twitter:title","content":"Zen LM - Open Foundation Models"}],["$","meta","11",{"name":"twitter:description","content":"Zen AI model family by Hanzo AI. 14 frontier models from 4B to 1T+ parameters for code, reasoning, vision, and multimodal tasks."}]]
