<!DOCTYPE html><!--o_J3cFnAZ1mdL_cv2bxKY--><html lang="en" class="dark"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/chunks/f2332aac77592f9d.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/e83606e8fa9cc796.js"/><script src="/_next/static/chunks/36d6595f0156cd7e.js" async=""></script><script src="/_next/static/chunks/040e9cea20a8d9c7.js" async=""></script><script src="/_next/static/chunks/c19fcbf6bf086438.js" async=""></script><script src="/_next/static/chunks/turbopack-7419f7f4f6b062de.js" async=""></script><script src="/_next/static/chunks/59d0ad1b64f8544e.js" async=""></script><script src="/_next/static/chunks/4d80e004cf4896dd.js" async=""></script><script src="/_next/static/chunks/350ee4303b732916.js" async=""></script><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#0A0A0A"/><meta name="theme-color" media="(prefers-color-scheme: light)" content="#fff"/><title>Blog — Zen LM | Zen LM</title><meta name="description" content="Research, releases, and perspectives from the Zen LM team."/><meta property="og:title" content="Zen LM - Open Foundation Models"/><meta property="og:description" content="Zen AI model family by Hanzo AI. 14 frontier models from 4B to 1T+ parameters for code, reasoning, vision, and multimodal tasks."/><meta property="og:url" content="https://zenlm.org"/><meta property="og:site_name" content="Zen LM"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="@zenlmorg"/><meta name="twitter:creator" content="@zenlmorg"/><meta name="twitter:title" content="Zen LM - Open Foundation Models"/><meta name="twitter:description" content="Zen AI model family by Hanzo AI. 14 frontier models from 4B to 1T+ parameters for code, reasoning, vision, and multimodal tasks."/><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="antialiased"><div hidden=""><!--$--><!--/$--></div><script>((a,b,c,d,e,f,g,h)=>{let i=document.documentElement,j=["light","dark"];function k(b){var c;(Array.isArray(a)?a:[a]).forEach(a=>{let c="class"===a,d=c&&f?e.map(a=>f[a]||a):e;c?(i.classList.remove(...d),i.classList.add(f&&f[b]?f[b]:b)):i.setAttribute(a,b)}),c=b,h&&j.includes(c)&&(i.style.colorScheme=c)}if(d)k(d);else try{let a=localStorage.getItem(b)||c,d=g&&"system"===a?window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":a;k(d)}catch(a){}})("class","theme","system",null,["light","dark"],null,true,true)</script><!--$--><div data-closed="" role="presentation" hidden="" style="user-select:none;-webkit-user-select:none" class="fixed inset-0 z-50 backdrop-blur-xs bg-fd-overlay data-open:animate-fd-fade-in data-closed:animate-fd-fade-out"></div><div class="bg-fd-secondary/50 p-3 empty:hidden"></div><!--/$--><main class="mx-auto w-full max-w-3xl px-4 py-16"><div class="mb-12"><h1 class="text-4xl font-bold mb-3">Blog</h1><p class="text-fd-muted-foreground text-lg">Research, releases, and perspectives from the Zen LM team.</p></div><div class="flex flex-col gap-0"><article><a class="block group py-8" href="/blog/zen5-release"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">February 28, 2026</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Introducing Zen 5 — Our Most Capable Model Yet</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">Zen 5 brings 1T+ parameter scale, 2M context windows, and state-of-the-art performance across reasoning, code, and multimodal tasks.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">release</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">zen5</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">flagship</span></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/bitdelta-behavioral-compression"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">February 27, 2026</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">BitDelta: 1-Bit Behavioral Compression Across the Zen Model Family</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">How BitDelta (arXiv:2402.10193) compresses fine-tuned behavioral deltas to 1-bit precision, enabling the full Zen model family — nano through ultra — to share a single GPU cluster.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Research</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Quantization</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">BitDelta</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Model Serving</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Delta Compression</span></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/continual-learning-sure-opcm"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">February 27, 2026</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">SuRe + OPCM: Production-Grade Continual Learning for Open Models</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">Deep dive on Surprise-Driven Prioritized Replay (SuRe) and Orthogonal Projection Continual Merging (OPCM) — the two SOTA techniques we use for catastrophic-forgetting-free LLM adaptation in the Zen model family.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Research</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Continual Learning</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">SuRe</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">OPCM</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">OPLoRA</span></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/drop-upcycling-zen-mode"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">February 27, 2026</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Drop-Upcycling and the Birth of Zen MoDE Architecture</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">How Drop-Upcycling (arXiv:2502.19261) transforms dense checkpoints into MoE models at 1/4 training cost, and how it shapes Zen MoDE — our Mixture of Distilled Experts architecture.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Research</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">MoE</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Architecture</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Drop-Upcycling</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Zen MoDE</span></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/gt-qlora-moe-abliteration"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">February 27, 2026</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">GT-QLoRA: Uncensoring Trillion-Parameter MoE Models</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">Why standard abliteration techniques fail on Mixture-of-Experts models, and how Gate-Targeted QLoRA solves the expert routing problem at 1 trillion parameters.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Research</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Abliteration</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">GT-QLoRA</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">MoE</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">zen4-ultra</span></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/zen-mode-architecture"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">February 14, 2026</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Zen MoDE — How We Build Frontier Models Through Distillation</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">An inside look at the Mixture of Distilled Experts architecture that powers the Zen model family.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">architecture</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">research</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">distillation</span></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/open-weights-philosophy"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">January 19, 2026</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Why We Open-Source Everything</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">The case for radical openness in AI — why releasing weights is the right thing, strategically and ethically.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">open-source</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">philosophy</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">community</span></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/zen4-ultra"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">January 19, 2026</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Zen4 Ultra: 480B Parameters, 1M Token Context</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">Zen4 Ultra is our most capable model: 480B total parameters, 35B active per token, 1M token context window. Benchmark results and use cases.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Models</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Zen4</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Flagship</span></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/zen-lm-launch"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">January 14, 2026</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Introducing Zen LM: Open Frontier Models from Hanzo AI and Zoo Labs</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">Announcing the Zen model family: 94+ open models built on Zen MoDE architecture, co-developed by Hanzo AI and Zoo Labs Foundation.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Announcement</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Models</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Zen</span></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen3guard"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">September 22, 2025</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Qwen3Guard: Real-time Safety for Your Token Stream</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">Tech Report GitHub Hugging Face ModelScope DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen-image-edit"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">August 18, 2025</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Qwen-Image-Edit: Image Editing with Higher Quality and Efficiency</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">QWEN CHAT GITHUB HUGGING FACE MODELSCOPE DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen-image"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">August 3, 2025</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Qwen-Image: Crafting with Native Text Rendering</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/gspo"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">July 26, 2025</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">GSPO: Towards Scalable Reinforcement Learning for Language Models</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">PAPER DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen-mt"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">July 23, 2025</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Qwen-MT: Where Speed Meets Smart Translation</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">DEMO API DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen3-coder"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">July 21, 2025</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Qwen3-Coder: Agentic Coding in the World</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">GITHUB HUGGING FACE MODELSCOPE DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen-tts"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">June 26, 2025</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Time to Speak Some Dialects, Qwen-TTS!</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">API DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen-vlo"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">June 25, 2025</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Qwen VLo: From \&quot;Understanding\&quot; the World to \&quot;Depicting\&quot; It</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">QWEN CHAT DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen3-embedding"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">June 4, 2025</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">GITHUB HUGGING FACE MODELSCOPE DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen3"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">April 28, 2025</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Qwen3: Think Deeper, Act Faster</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">QWEN CHAT GitHub Hugging Face ModelScope Kaggle DEMO DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qvq-max-preview"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">March 27, 2025</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">QVQ-Max: Think with Evidence</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">QWEN CHAT GITHUB HUGGING FACE MODELSCOPE DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen2.5-omni"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">March 26, 2025</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">zen Omni: See, Hear, Talk, Write, Do It All!</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">QWEN CHAT HUGGING FACE MODELSCOPE DASHSCOPE GITHUB PAPER DEMO DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen2.5-vl-32b"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">March 23, 2025</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">zen-VL-32B: Smarter and Lighter</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">QWEN CHAT GITHUB HUGGING FACE MODELSCOPE DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwq-32b"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">March 5, 2025</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">QwQ-32B: Embracing the Power of Reinforcement Learning</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">QWEN CHAT Hugging Face ModelScope DEMO DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwq-max-preview"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">February 24, 2025</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">... QwQ-Max-Preview</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">QWEN CHAT DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen2.5-max"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">January 27, 2025</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">zen-Max: Exploring the Intelligence of Large-scale MoE Model</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">QWEN CHAT API DEMO DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen2.5-1m"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">January 26, 2025</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">zen-1M: Deploy Your Own Qwen with Context Length up to 1M Tokens</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">Tech Report HuggingFace ModelScope Qwen Chat HuggingFace Demo ModelScope Demo DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen2.5-vl"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">January 25, 2025</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">zen VL! zen VL! zen VL!</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">We release zen-VL, the new flagship vision-language model with enhanced visual understanding, OCR, agentic capabilities, and long video comprehension.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/global-load-balance"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">January 20, 2025</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Global-batch load balance almost free lunch to improve your MoE LLM training</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">GITHUB HUGGING FACE MODELSCOPE DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen2.5-math-prm"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">January 13, 2025</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Towards Effective Process Supervision in Mathematical Reasoning</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">GITHUB HUGGING FACE MODELSCOPE DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/zen-3"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">January 12, 2025</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Zen 3.0: The Next Generation of Open AI</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">Announcing Zen 3.0, our most capable open model family yet.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zach Kelling</span><div class="flex gap-1.5 ml-auto"><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Announcement</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Models</span></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qvq-72b-preview"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">December 24, 2024</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">QVQ: To See the World with Wisdom</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">GITHUB HUGGING FACE MODELSCOPE KAGGLE DEMO DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwq-32b-preview"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">November 27, 2024</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">QwQ: Reflect Deeply on the Boundaries of the Unknown</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen2.5-turbo"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">November 14, 2024</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Extending the Context Length to 1M Tokens!</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">API Documentation (Chinese) HuggingFace Demo ModelScope Demo</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen2.5-coder-family"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">November 11, 2024</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">zen-Coder Series: Powerful, Diverse, Practical.</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">GITHUB HUGGING FACE MODELSCOPE KAGGLE DEMO DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/future-of-open-ai"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">November 10, 2024</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">The Future of Open AI</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">Reflections on where open AI development is heading and what it will take to get there.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zach Kelling</span><div class="flex gap-1.5 ml-auto"><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Vision</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Open Source</span></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen2.5-coder"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">September 18, 2024</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">zen-Coder: Code More, Learn More!</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen2.5-llm"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">September 18, 2024</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">zen-LLM: Extending the boundary of LLMs</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen2.5-math"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">September 18, 2024</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">zen-Math: The world&#x27;s leading open-sourced mathematical LLMs</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">GITHUB HUGGING FACE MODELSCOPE DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen2.5"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">September 18, 2024</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">zen: A Party of Foundation Models!</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen2-vl"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">August 28, 2024</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">zen-VL: To See the World More Clearly</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">DEMO GITHUB HUGGING FACE MODELSCOPE API DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen2-audio"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">August 8, 2024</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">zen-Audio: Chat with Your Voice!</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">DEMO PAPER GITHUB HUGGING FACE MODELSCOPE DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen2-math"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">August 7, 2024</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Introducing zen-Math</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">GITHUB HUGGING FACE MODELSCOPE DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/decentralized-compute"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">August 4, 2024</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Decentralized Compute for AI Training</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">How we&#x27;re building a decentralized compute network for training large AI models.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zach Kelling</span><div class="flex gap-1.5 ml-auto"><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Infrastructure</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Training</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Decentralization</span></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen2"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">June 6, 2024</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Hello zen</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen-agent-2405"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">June 5, 2024</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Generalizing an LLM from 8k to 1M Context using Qwen-Agent</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">We&amp;rsquo;ve created an agent using zen models with an 8k context size to understand documents with 1M tokens, surpassing RAG and native long-context models. This agent was also used to generate data for training new long-context Qwen models.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/zips-governance"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">May 19, 2024</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">ZIPs: Decentralized Governance for Open AI</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">How Zoo Improvement Proposals enable community-driven governance of open AI development.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zach Kelling</span><div class="flex gap-1.5 ml-auto"><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Governance</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">ZIPs</span></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen-max-0428"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">May 10, 2024</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Notes on Qwen-Max-0428</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">API DEMO DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen1.5-110b"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">April 24, 2024</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Qwen1.5-110B: The First 100B+ Model of the Qwen1.5 Series</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/codeqwen1.5"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">April 15, 2024</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Code with CodeQwen1.5</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen1.5-32b"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">April 1, 2024</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Qwen1.5-32B: Fitting the Capstone of the Qwen1.5 Language Model Series</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen-moe"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">March 27, 2024</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated Parameters</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/zoo-foundation-launch"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">February 11, 2024</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Announcing the Zoo Labs Foundation</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">Today we formally launch Zoo Labs Foundation, an open research network dedicated to decentralized AI and decentralized science.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zach Kelling</span><div class="flex gap-1.5 ml-auto"><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Announcement</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Foundation</span></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen1.5"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">February 3, 2024</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Introducing Qwen1.5</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen-vl"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">January 24, 2024</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Introducing Qwen-VL</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">Along with the rapid development of our large language model Qwen, we leveraged Qwen’s capabilities and unified multimodal pretraining to address the limitations of multimodal models in generalization, and we opensourced multimodal model Qwen-VL in Sep. 2023. Recently, the Qwen-VL series has undergone a significant upgrade with the launch of two enhanced versions, Qwen-VL-Plus and Qwen-VL-Max. The key technical advancements in these versions include:</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/qwen"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">January 22, 2024</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Introducing Qwen</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">4 months after our first release of Qwen-7B, which is the starting point of our opensource journey of large language models (LLM), we now provide an introduction to the Qwen series to give you a whole picture of our work as well as our objectives. Below are important links to our opensource projects and community.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/agent-nfts"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">December 17, 2023</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Agent NFTs: Ownership and Identity for AI Agents</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">Introducing Agent NFTs, a framework for giving AI agents persistent identity and enabling ownership of their capabilities.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zach Kelling</span><div class="flex gap-1.5 ml-auto"><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Agents</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">NFTs</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Blockchain</span></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/training-gym"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">September 10, 2023</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Training Gym: A Platform for Open Model Development</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">Announcing Training Gym, our open platform for collaborative large model training.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zach Kelling</span><div class="flex gap-1.5 ml-auto"><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Infrastructure</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Training</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Open Source</span></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/proof-of-ai"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">June 25, 2023</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Proof of AI: Verifiable Machine Learning on Chain</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">How we&#x27;re bringing cryptographic verification to AI inference, enabling trustless machine learning.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zach Kelling</span><div class="flex gap-1.5 ml-auto"><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Research</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Blockchain</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Verification</span></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/zen-reranker"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">March 12, 2023</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Zen Reranker: Two-Stage Retrieval Done Right</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">Introducing the Zen Reranker, a cross-encoder model that dramatically improves retrieval quality in two-stage pipelines.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zach Kelling</span><div class="flex gap-1.5 ml-auto"><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Research</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Retrieval</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Reranking</span></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/ofasys"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">December 27, 2022</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">OFASys: Enabling Multitask Learning with One Line of Code! </h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">Intro Generalist Models are hot! We all see an opportunity towards a real generalist model by multimodal multitask learning. We previously release an opensourced unified multimodal pretrained model OFA for this goal. However, we actually met a lot of difficulties in our implementation. For example, it is hard to set up multiple tasks concerning multiple modalities, and it is hard to organize multitask learning, e.g., how to batchify your data and how to make your training stable.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/chinese-clip"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">December 23, 2022</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">CLIP1 is a phenomenal playmaker in vision and multimodal representation learning. It plays not only as a foundation model but also a bridge between vision and language. It has triggered a series of research in different fields, especially text-to-image generation. However, we find that there is a necessity for a language-specific CLIP for applications, especially cross-modal retrieval, and there is no opensourced Chinese CLIP with good performance. We therefore launched this project to promote the Chinese multimodal representation learning.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/7680-dim-embeddings"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">December 4, 2022</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">7680-Dimensional Embeddings: More Dimensions, Better Retrieval</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">Why we trained embedding models with 7680 dimensions and what we learned about the relationship between dimensionality and retrieval quality.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zach Kelling</span><div class="flex gap-1.5 ml-auto"><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Research</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Embeddings</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Retrieval</span></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/embedding-spaces-7680-dimensions"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">December 4, 2022</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Embedding Spaces at 7680 Dimensions</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">Exploring high-dimensional embedding spaces for semantic search and retrieval.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zach Kelling</span><div class="flex gap-1.5 ml-auto"><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Research</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Embeddings</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Retrieval</span></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/ofa"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">November 13, 2022</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">OFA: Towards Building a One-For-All Model</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">2022 is a year of generalist models! With the bloom of multimodal pretraining, especially the unified model, we have witnessed the opportunity to building a generalist model that is capable of processing tasks of different modalities or multi-modalities! Thus, we propose OFA1, namely One-For-All, a unified multimodal pretrained model that unifies understanding and generation tasks concerning modalities into a single framework, and we pretrain OFA with the instruction-based multitask-pretraining that endows it with multiple capabilities.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zen LM Team</span><div class="flex gap-1.5 ml-auto"></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/grpo"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">September 18, 2022</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">GRPO: Group Relative Policy Optimization</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">Introducing GRPO, a new approach to reinforcement learning from human feedback that improves sample efficiency and alignment stability.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zach Kelling</span><div class="flex gap-1.5 ml-auto"><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Research</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Alignment</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Training</span></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/grpo-group-relative-policy-optimization"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">September 17, 2022</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">GRPO: Group Relative Policy Optimization</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">A companion post to our GRPO paper, explaining group relative policy optimization for language model alignment.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zach Kelling</span><div class="flex gap-1.5 ml-auto"><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Research</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">RLHF</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">GRPO</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Alignment</span></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/federated-learning-without-compromise"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">May 29, 2022</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Federated Learning Without Compromise</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">Privacy-preserving machine learning that maintains model quality through novel aggregation protocols.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zach Kelling</span><div class="flex gap-1.5 ml-auto"><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Research</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Federated Learning</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Privacy</span></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/federated-learning"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">May 8, 2022</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Federated Learning for Open AI</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">How federated learning enables collaborative model training while preserving data privacy.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zach Kelling</span><div class="flex gap-1.5 ml-auto"><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Research</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Training</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Privacy</span></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/experience-ledgers"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">February 13, 2022</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Experience Ledgers: Persistent Memory for AI Agents</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">Introducing experience ledgers, a framework for giving AI agents persistent, verifiable memory.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zach Kelling</span><div class="flex gap-1.5 ml-auto"><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Research</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Agents</span></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/case-for-decentralized-science"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">November 7, 2021</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">The Case for Decentralized Science</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">A manifesto for decentralized science (DeSci) and its application to AI research.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zach Kelling</span><div class="flex gap-1.5 ml-auto"><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">DeSci</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Vision</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Governance</span></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/case-for-desci"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">November 7, 2021</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">The Case for Decentralized Science</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">Why scientific research needs decentralization, and how blockchain can help.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zach Kelling</span><div class="flex gap-1.5 ml-auto"><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">DeSci</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Vision</span></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/training-llms-collective-intelligence"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">July 21, 2021</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Training LLMs on Collective Intelligence</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">How we&#x27;re approaching training data curation to capture humanity&#x27;s collective intelligence.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zach Kelling</span><div class="flex gap-1.5 ml-auto"><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Research</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Training</span></div></div></a><div class="border-b border-fd-border"></div></article><article><a class="block group py-8" href="/blog/introducing-zen"><time class="text-xs font-mono text-fd-muted-foreground uppercase tracking-wider">March 14, 2021</time><h2 class="text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors">Introducing Zen: Open AI for the Open Web</h2><p class="text-fd-muted-foreground text-sm leading-relaxed mb-3">We&#x27;re launching Zen, an open research initiative to build AI that serves everyone.</p><div class="flex items-center gap-3"><span class="text-xs text-fd-muted-foreground">Zach Kelling</span><div class="flex gap-1.5 ml-auto"><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Announcement</span><span class="text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded">Vision</span></div></div></a></article></div></main><!--$--><!--/$--><script src="/_next/static/chunks/e83606e8fa9cc796.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[106,[\"/_next/static/chunks/59d0ad1b64f8544e.js\"],\"RootProvider\"]\n3:I[53113,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"default\"]\n4:I[73211,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"default\"]\n5:I[10086,[\"/_next/static/chunks/59d0ad1b64f8544e.js\"],\"\"]\n52:I[6998,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"default\"]\n:HL[\"/_next/static/chunks/f2332aac77592f9d.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"o_J3cFnAZ1mdL_cv2bxKY\",\"c\":[\"\",\"blog\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"blog\",{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/f2332aac77592f9d.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/59d0ad1b64f8544e.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"dark\",\"suppressHydrationWarning\":true,\"children\":[\"$\",\"body\",null,{\"className\":\"antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"main\",null,{\"className\":\"flex min-h-screen flex-col items-center justify-center px-4 text-center\",\"children\":[[\"$\",\"div\",null,{\"className\":\"mb-8 opacity-20\",\"children\":[\"$\",\"svg\",null,{\"width\":\"120\",\"height\":\"120\",\"viewBox\":\"0 0 120 120\",\"fill\":\"none\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"circle\",null,{\"cx\":\"60\",\"cy\":\"60\",\"r\":\"50\",\"stroke\":\"currentColor\",\"strokeWidth\":\"3\",\"strokeLinecap\":\"round\",\"strokeDasharray\":\"280 40\"}]}]}],[\"$\",\"p\",null,{\"className\":\"text-xs font-mono uppercase tracking-[0.3em] text-fd-muted-foreground mb-4\",\"children\":\"404\"}],[\"$\",\"h1\",null,{\"className\":\"text-3xl font-semibold mb-3\",\"children\":\"Page not found\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground max-w-sm mb-10\",\"children\":\"This page doesn't exist, or it may have moved. Try the documentation or head home.\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-3 justify-center\",\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/\",\"className\":\"inline-flex items-center gap-2 rounded-lg bg-fd-primary text-fd-primary-foreground px-4 py-2 text-sm font-medium hover:opacity-90 transition\",\"children\":\"Go home\"}],[\"$\",\"$L5\",null,{\"href\":\"/docs\",\"className\":\"inline-flex items-center gap-2 rounded-lg border border-fd-border bg-fd-background px-4 py-2 text-sm font-medium hover:bg-fd-muted transition\",\"children\":\"Documentation\"}],[\"$\",\"$L5\",null,{\"href\":\"/docs/models\",\"className\":\"inline-flex items-center gap-2 rounded-lg border border-fd-border bg-fd-background px-4 py-2 text-sm font-medium hover:bg-fd-muted transition\",\"children\":\"Browse models\"}]]}],[\"$\",\"p\",null,{\"className\":\"mt-16 text-xs font-mono text-fd-muted-foreground opacity-50\",\"children\":\"zenlm.org\"}]]}],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]}]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"main\",null,{\"className\":\"mx-auto w-full max-w-3xl px-4 py-16\",\"children\":[[\"$\",\"div\",null,{\"className\":\"mb-12\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-4xl font-bold mb-3\",\"children\":\"Blog\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-lg\",\"children\":\"Research, releases, and perspectives from the Zen LM team.\"}]]}],[\"$\",\"div\",null,{\"className\":\"flex flex-col gap-0\",\"children\":[[\"$\",\"article\",\"zen5-release\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/zen5-release\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"February 28, 2026\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Introducing Zen 5 — Our Most Capable Model Yet\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"Zen 5 brings 1T+ parameter scale, 2M context windows, and state-of-the-art performance across reasoning, code, and multimodal tasks.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[[\"$\",\"span\",\"release\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"release\"}],[\"$\",\"span\",\"zen5\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"zen5\"}],\"$L6\"]}]]}]]}],\"$L7\"]}],\"$L8\",\"$L9\",\"$La\",\"$Lb\",\"$Lc\",\"$Ld\",\"$Le\",\"$Lf\",\"$L10\",\"$L11\",\"$L12\",\"$L13\",\"$L14\",\"$L15\",\"$L16\",\"$L17\",\"$L18\",\"$L19\",\"$L1a\",\"$L1b\",\"$L1c\",\"$L1d\",\"$L1e\",\"$L1f\",\"$L20\",\"$L21\",\"$L22\",\"$L23\",\"$L24\",\"$L25\",\"$L26\",\"$L27\",\"$L28\",\"$L29\",\"$L2a\",\"$L2b\",\"$L2c\",\"$L2d\",\"$L2e\",\"$L2f\",\"$L30\",\"$L31\",\"$L32\",\"$L33\",\"$L34\",\"$L35\",\"$L36\",\"$L37\",\"$L38\",\"$L39\",\"$L3a\",\"$L3b\",\"$L3c\",\"$L3d\",\"$L3e\",\"$L3f\",\"$L40\",\"$L41\",\"$L42\",\"$L43\",\"$L44\",\"$L45\",\"$L46\",\"$L47\",\"$L48\",\"$L49\",\"$L4a\",\"$L4b\",\"$L4c\",\"$L4d\",\"$L4e\",\"$L4f\"]}]]}],null,\"$L50\"]}],{},null,false,false]},null,false,false]},null,false,false],\"$L51\",false]],\"m\":\"$undefined\",\"G\":[\"$52\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"53:I[89923,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"OutletBoundary\"]\n54:\"$Sreact.suspense\"\n56:I[89923,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"ViewportBoundary\"]\n58:I[89923,[\"/_next/static/chunks/4d80e004cf4896dd.js\",\"/_next/static/chunks/350ee4303b732916.js\"],\"MetadataBoundary\"]\n6:[\"$\",\"span\",\"flagship\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"flagship\"}]\n7:[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]\n"])</script><script>self.__next_f.push([1,"8:[\"$\",\"article\",\"bitdelta-behavioral-compression\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/bitdelta-behavioral-compression\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"February 27, 2026\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"BitDelta: 1-Bit Behavioral Compression Across the Zen Model Family\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"How BitDelta (arXiv:2402.10193) compresses fine-tuned behavioral deltas to 1-bit precision, enabling the full Zen model family — nano through ultra — to share a single GPU cluster.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[[\"$\",\"span\",\"Research\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Research\"}],[\"$\",\"span\",\"Quantization\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Quantization\"}],[\"$\",\"span\",\"BitDelta\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"BitDelta\"}],[\"$\",\"span\",\"Model Serving\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Model Serving\"}],[\"$\",\"span\",\"Delta Compression\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Delta Compression\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n"])</script><script>self.__next_f.push([1,"9:[\"$\",\"article\",\"continual-learning-sure-opcm\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/continual-learning-sure-opcm\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"February 27, 2026\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"SuRe + OPCM: Production-Grade Continual Learning for Open Models\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"Deep dive on Surprise-Driven Prioritized Replay (SuRe) and Orthogonal Projection Continual Merging (OPCM) — the two SOTA techniques we use for catastrophic-forgetting-free LLM adaptation in the Zen model family.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[[\"$\",\"span\",\"Research\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Research\"}],[\"$\",\"span\",\"Continual Learning\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Continual Learning\"}],[\"$\",\"span\",\"SuRe\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"SuRe\"}],[\"$\",\"span\",\"OPCM\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"OPCM\"}],[\"$\",\"span\",\"OPLoRA\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"OPLoRA\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n"])</script><script>self.__next_f.push([1,"a:[\"$\",\"article\",\"drop-upcycling-zen-mode\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/drop-upcycling-zen-mode\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"February 27, 2026\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Drop-Upcycling and the Birth of Zen MoDE Architecture\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"How Drop-Upcycling (arXiv:2502.19261) transforms dense checkpoints into MoE models at 1/4 training cost, and how it shapes Zen MoDE — our Mixture of Distilled Experts architecture.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[[\"$\",\"span\",\"Research\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Research\"}],[\"$\",\"span\",\"MoE\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"MoE\"}],[\"$\",\"span\",\"Architecture\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Architecture\"}],[\"$\",\"span\",\"Drop-Upcycling\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Drop-Upcycling\"}],[\"$\",\"span\",\"Zen MoDE\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Zen MoDE\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n"])</script><script>self.__next_f.push([1,"b:[\"$\",\"article\",\"gt-qlora-moe-abliteration\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/gt-qlora-moe-abliteration\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"February 27, 2026\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"GT-QLoRA: Uncensoring Trillion-Parameter MoE Models\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"Why standard abliteration techniques fail on Mixture-of-Experts models, and how Gate-Targeted QLoRA solves the expert routing problem at 1 trillion parameters.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[[\"$\",\"span\",\"Research\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Research\"}],[\"$\",\"span\",\"Abliteration\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Abliteration\"}],[\"$\",\"span\",\"GT-QLoRA\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"GT-QLoRA\"}],[\"$\",\"span\",\"MoE\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"MoE\"}],[\"$\",\"span\",\"zen4-ultra\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"zen4-ultra\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n"])</script><script>self.__next_f.push([1,"c:[\"$\",\"article\",\"zen-mode-architecture\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/zen-mode-architecture\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"February 14, 2026\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Zen MoDE — How We Build Frontier Models Through Distillation\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"An inside look at the Mixture of Distilled Experts architecture that powers the Zen model family.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[[\"$\",\"span\",\"architecture\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"architecture\"}],[\"$\",\"span\",\"research\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"research\"}],[\"$\",\"span\",\"distillation\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"distillation\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n"])</script><script>self.__next_f.push([1,"d:[\"$\",\"article\",\"open-weights-philosophy\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/open-weights-philosophy\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"January 19, 2026\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Why We Open-Source Everything\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"The case for radical openness in AI — why releasing weights is the right thing, strategically and ethically.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[[\"$\",\"span\",\"open-source\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"open-source\"}],[\"$\",\"span\",\"philosophy\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"philosophy\"}],[\"$\",\"span\",\"community\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"community\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n"])</script><script>self.__next_f.push([1,"e:[\"$\",\"article\",\"zen4-ultra\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/zen4-ultra\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"January 19, 2026\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Zen4 Ultra: 480B Parameters, 1M Token Context\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"Zen4 Ultra is our most capable model: 480B total parameters, 35B active per token, 1M token context window. Benchmark results and use cases.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[[\"$\",\"span\",\"Models\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Models\"}],[\"$\",\"span\",\"Zen4\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Zen4\"}],[\"$\",\"span\",\"Flagship\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Flagship\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n"])</script><script>self.__next_f.push([1,"f:[\"$\",\"article\",\"zen-lm-launch\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/zen-lm-launch\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"January 14, 2026\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Introducing Zen LM: Open Frontier Models from Hanzo AI and Zoo Labs\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"Announcing the Zen model family: 94+ open models built on Zen MoDE architecture, co-developed by Hanzo AI and Zoo Labs Foundation.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[[\"$\",\"span\",\"Announcement\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Announcement\"}],[\"$\",\"span\",\"Models\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Models\"}],[\"$\",\"span\",\"Zen\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Zen\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n"])</script><script>self.__next_f.push([1,"10:[\"$\",\"article\",\"qwen3guard\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen3guard\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"September 22, 2025\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Qwen3Guard: Real-time Safety for Your Token Stream\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"Tech Report GitHub Hugging Face ModelScope DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n11:[\"$\",\"article\",\"qwen-image-edit\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen-image-edit\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"August 18, 2025\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Qwen-Image-Edit: Image Editing with Higher Quality and Efficiency\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"QWEN CHAT GITHUB HUGGING FACE MODELSCOPE DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n12:[\"$\",\"article\",\"qwen-image\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen-image\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"August 3, 2025\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Qwen-Image: Crafting with Native Text Rendering\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n13:[\"$\",\"article\",\"gspo\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/gspo\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"July 26, 2025\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"GSPO: Towards Scalable Reinforcement Learning for Language Models\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"PAPER DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n14:[\"$\",\"article\",\"qwen-mt\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen-mt\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"July 23, 2025\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Qwen-MT: Where Speed Meets Smart Translation\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"DEMO API DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\""])</script><script>self.__next_f.push([1,":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n15:[\"$\",\"article\",\"qwen3-coder\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen3-coder\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"July 21, 2025\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Qwen3-Coder: Agentic Coding in the World\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"GITHUB HUGGING FACE MODELSCOPE DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n16:[\"$\",\"article\",\"qwen-tts\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen-tts\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"June 26, 2025\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Time to Speak Some Dialects, Qwen-TTS!\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"API DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n17:[\"$\",\"article\",\"qwen-vlo\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen-vlo\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"June 25, 2025\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Qwen VLo: From \\\\\\\"Understanding\\\\\\\" the World to \\\\\\\"Depicting\\\\\\\" It\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"QWEN CHAT DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n18:[\"$\",\"article\",\"qwen3-embedding\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen3-embedding\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"June 4, 2025\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"GITHUB HUGGING FACE MODELSCOPE DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n19:[\"$\",\"article\",\"qwen3\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen3\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"April 28, 2025\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Qwen3: Think Deeper, Act Faster\"}],[\"$\",\"p\",null,{\"c"])</script><script>self.__next_f.push([1,"lassName\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"QWEN CHAT GitHub Hugging Face ModelScope Kaggle DEMO DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n1a:[\"$\",\"article\",\"qvq-max-preview\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qvq-max-preview\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"March 27, 2025\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"QVQ-Max: Think with Evidence\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"QWEN CHAT GITHUB HUGGING FACE MODELSCOPE DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n1b:[\"$\",\"article\",\"qwen2.5-omni\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen2.5-omni\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"March 26, 2025\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"zen Omni: See, Hear, Talk, Write, Do It All!\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"QWEN CHAT HUGGING FACE MODELSCOPE DASHSCOPE GITHUB PAPER DEMO DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n1c:[\"$\",\"article\",\"qwen2.5-vl-32b\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen2.5-vl-32b\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"March 23, 2025\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"zen-VL-32B: Smarter and Lighter\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"QWEN CHAT GITHUB HUGGING FACE MODELSCOPE DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n1d:[\"$\",\"article\",\"qwq-32b\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwq-32b\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"March 5, 2025\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"QwQ-32B: Embracing the Power of Reinforcement Learning\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"QWEN CHAT Hugging Face ModelScope DEMO DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n1e:[\"$\",\"article\",\"qwq-max-preview\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwq-max-preview\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"class"])</script><script>self.__next_f.push([1,"Name\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"February 24, 2025\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"... QwQ-Max-Preview\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"QWEN CHAT DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n1f:[\"$\",\"article\",\"qwen2.5-max\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen2.5-max\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"January 27, 2025\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"zen-Max: Exploring the Intelligence of Large-scale MoE Model\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"QWEN CHAT API DEMO DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n20:[\"$\",\"article\",\"qwen2.5-1m\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen2.5-1m\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"January 26, 2025\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"zen-1M: Deploy Your Own Qwen with Context Length up to 1M Tokens\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"Tech Report HuggingFace ModelScope Qwen Chat HuggingFace Demo ModelScope Demo DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n21:[\"$\",\"article\",\"qwen2.5-vl\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen2.5-vl\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"January 25, 2025\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"zen VL! zen VL! zen VL!\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"We release zen-VL, the new flagship vision-language model with enhanced visual understanding, OCR, agentic capabilities, and long video comprehension.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n22:[\"$\",\"article\",\"global-load-balance\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/global-load-balance\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"January 20, 2025\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Global-batch load balance almost free lunch to improve your MoE LLM training\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"GITHUB HUGGING FACE MODELSCOPE DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":"])</script><script>self.__next_f.push([1,"[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n23:[\"$\",\"article\",\"qwen2.5-math-prm\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen2.5-math-prm\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"January 13, 2025\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Towards Effective Process Supervision in Mathematical Reasoning\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"GITHUB HUGGING FACE MODELSCOPE DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n24:[\"$\",\"article\",\"zen-3\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/zen-3\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"January 12, 2025\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Zen 3.0: The Next Generation of Open AI\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"Announcing Zen 3.0, our most capable open model family yet.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zach Kelling\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[[\"$\",\"span\",\"Announcement\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Announcement\"}],[\"$\",\"span\",\"Models\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Models\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n25:[\"$\",\"article\",\"qvq-72b-preview\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qvq-72b-preview\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"December 24, 2024\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"QVQ: To See the World with Wisdom\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"GITHUB HUGGING FACE MODELSCOPE KAGGLE DEMO DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n26:[\"$\",\"article\",\"qwq-32b-preview\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwq-32b-preview\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"November 27, 2024\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"QwQ: Reflect Deeply on the Boundaries of the Unknown\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n27:[\"$"])</script><script>self.__next_f.push([1,"\",\"article\",\"qwen2.5-turbo\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen2.5-turbo\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"November 14, 2024\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Extending the Context Length to 1M Tokens!\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"API Documentation (Chinese) HuggingFace Demo ModelScope Demo\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n28:[\"$\",\"article\",\"qwen2.5-coder-family\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen2.5-coder-family\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"November 11, 2024\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"zen-Coder Series: Powerful, Diverse, Practical.\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"GITHUB HUGGING FACE MODELSCOPE KAGGLE DEMO DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n29:[\"$\",\"article\",\"future-of-open-ai\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/future-of-open-ai\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"November 10, 2024\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"The Future of Open AI\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"Reflections on where open AI development is heading and what it will take to get there.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zach Kelling\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[[\"$\",\"span\",\"Vision\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Vision\"}],[\"$\",\"span\",\"Open Source\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Open Source\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n2a:[\"$\",\"article\",\"qwen2.5-coder\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen2.5-coder\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"September 18, 2024\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"zen-Coder: Code More, Learn More!\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n2b:[\"$\",\"article\",\"qwen2.5-llm\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen2.5-llm\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppe"])</script><script>self.__next_f.push([1,"rcase tracking-wider\",\"children\":\"September 18, 2024\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"zen-LLM: Extending the boundary of LLMs\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n2c:[\"$\",\"article\",\"qwen2.5-math\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen2.5-math\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"September 18, 2024\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"zen-Math: The world's leading open-sourced mathematical LLMs\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"GITHUB HUGGING FACE MODELSCOPE DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n2d:[\"$\",\"article\",\"qwen2.5\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen2.5\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"September 18, 2024\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"zen: A Party of Foundation Models!\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n2e:[\"$\",\"article\",\"qwen2-vl\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen2-vl\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"August 28, 2024\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"zen-VL: To See the World More Clearly\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"DEMO GITHUB HUGGING FACE MODELSCOPE API DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n2f:[\"$\",\"article\",\"qwen2-audio\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen2-audio\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"August 8, 2024\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"zen-Audio: Chat with Your Voice!\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"DEMO PAPER GITHUB HUGGING FACE MODELSCOPE DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border"])</script><script>self.__next_f.push([1,"-fd-border\"}]]}]\n30:[\"$\",\"article\",\"qwen2-math\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen2-math\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"August 7, 2024\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Introducing zen-Math\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"GITHUB HUGGING FACE MODELSCOPE DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n"])</script><script>self.__next_f.push([1,"31:[\"$\",\"article\",\"decentralized-compute\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/decentralized-compute\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"August 4, 2024\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Decentralized Compute for AI Training\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"How we're building a decentralized compute network for training large AI models.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zach Kelling\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[[\"$\",\"span\",\"Infrastructure\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Infrastructure\"}],[\"$\",\"span\",\"Training\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Training\"}],[\"$\",\"span\",\"Decentralization\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Decentralization\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n"])</script><script>self.__next_f.push([1,"32:[\"$\",\"article\",\"qwen2\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen2\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"June 6, 2024\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Hello zen\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n33:[\"$\",\"article\",\"qwen-agent-2405\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen-agent-2405\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"June 5, 2024\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Generalizing an LLM from 8k to 1M Context using Qwen-Agent\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"We\u0026rsquo;ve created an agent using zen models with an 8k context size to understand documents with 1M tokens, surpassing RAG and native long-context models. This agent was also used to generate data for training new long-context Qwen models.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n34:[\"$\",\"article\",\"zips-governance\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/zips-governance\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"May 19, 2024\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"ZIPs: Decentralized Governance for Open AI\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"How Zoo Improvement Proposals enable community-driven governance of open AI development.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zach Kelling\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[[\"$\",\"span\",\"Governance\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Governance\"}],[\"$\",\"span\",\"ZIPs\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"ZIPs\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n35:[\"$\",\"article\",\"qwen-max-0428\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen-max-0428\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"May 10, 2024\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Notes on Qwen-Max-0428\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"API DEMO DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n36:[\"$\",\"article\",\"qwen1.5-110b\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen1.5-110b\",\"className\":\"block group py-8\",\"ch"])</script><script>self.__next_f.push([1,"ildren\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"April 24, 2024\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Qwen1.5-110B: The First 100B+ Model of the Qwen1.5 Series\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n37:[\"$\",\"article\",\"codeqwen1.5\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/codeqwen1.5\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"April 15, 2024\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Code with CodeQwen1.5\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n38:[\"$\",\"article\",\"qwen1.5-32b\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen1.5-32b\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"April 1, 2024\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Qwen1.5-32B: Fitting the Capstone of the Qwen1.5 Language Model Series\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n39:[\"$\",\"article\",\"qwen-moe\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen-moe\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"March 27, 2024\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated Parameters\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n3a:[\"$\",\"article\",\"zoo-foundation-launch\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/zoo-foundation-launch\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"February 11, 2024\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Announcing the Zoo Labs Foundation\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"Today we formally launch Zoo Labs Foundation, an open research network dedicated to decentralized AI and decentralized science.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-cent"])</script><script>self.__next_f.push([1,"er gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zach Kelling\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[[\"$\",\"span\",\"Announcement\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Announcement\"}],[\"$\",\"span\",\"Foundation\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Foundation\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n3b:[\"$\",\"article\",\"qwen1.5\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen1.5\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"February 3, 2024\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Introducing Qwen1.5\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n3c:[\"$\",\"article\",\"qwen-vl\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen-vl\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"January 24, 2024\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Introducing Qwen-VL\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"Along with the rapid development of our large language model Qwen, we leveraged Qwen’s capabilities and unified multimodal pretraining to address the limitations of multimodal models in generalization, and we opensourced multimodal model Qwen-VL in Sep. 2023. Recently, the Qwen-VL series has undergone a significant upgrade with the launch of two enhanced versions, Qwen-VL-Plus and Qwen-VL-Max. The key technical advancements in these versions include:\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n3d:[\"$\",\"article\",\"qwen\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/qwen\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"January 22, 2024\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Introducing Qwen\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"4 months after our first release of Qwen-7B, which is the starting point of our opensource journey of large language models (LLM), we now provide an introduction to the Qwen series to give you a whole picture of our work as well as our objectives. Below are important links to our opensource projects and community.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n"])</script><script>self.__next_f.push([1,"3e:[\"$\",\"article\",\"agent-nfts\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/agent-nfts\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"December 17, 2023\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Agent NFTs: Ownership and Identity for AI Agents\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"Introducing Agent NFTs, a framework for giving AI agents persistent identity and enabling ownership of their capabilities.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zach Kelling\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[[\"$\",\"span\",\"Agents\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Agents\"}],[\"$\",\"span\",\"NFTs\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"NFTs\"}],[\"$\",\"span\",\"Blockchain\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Blockchain\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n"])</script><script>self.__next_f.push([1,"3f:[\"$\",\"article\",\"training-gym\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/training-gym\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"September 10, 2023\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Training Gym: A Platform for Open Model Development\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"Announcing Training Gym, our open platform for collaborative large model training.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zach Kelling\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[[\"$\",\"span\",\"Infrastructure\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Infrastructure\"}],[\"$\",\"span\",\"Training\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Training\"}],[\"$\",\"span\",\"Open Source\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Open Source\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n"])</script><script>self.__next_f.push([1,"40:[\"$\",\"article\",\"proof-of-ai\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/proof-of-ai\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"June 25, 2023\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Proof of AI: Verifiable Machine Learning on Chain\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"How we're bringing cryptographic verification to AI inference, enabling trustless machine learning.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zach Kelling\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[[\"$\",\"span\",\"Research\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Research\"}],[\"$\",\"span\",\"Blockchain\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Blockchain\"}],[\"$\",\"span\",\"Verification\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Verification\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n"])</script><script>self.__next_f.push([1,"41:[\"$\",\"article\",\"zen-reranker\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/zen-reranker\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"March 12, 2023\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Zen Reranker: Two-Stage Retrieval Done Right\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"Introducing the Zen Reranker, a cross-encoder model that dramatically improves retrieval quality in two-stage pipelines.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zach Kelling\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[[\"$\",\"span\",\"Research\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Research\"}],[\"$\",\"span\",\"Retrieval\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Retrieval\"}],[\"$\",\"span\",\"Reranking\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Reranking\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n"])</script><script>self.__next_f.push([1,"42:[\"$\",\"article\",\"ofasys\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/ofasys\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"December 27, 2022\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"OFASys: Enabling Multitask Learning with One Line of Code! \"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"Intro Generalist Models are hot! We all see an opportunity towards a real generalist model by multimodal multitask learning. We previously release an opensourced unified multimodal pretrained model OFA for this goal. However, we actually met a lot of difficulties in our implementation. For example, it is hard to set up multiple tasks concerning multiple modalities, and it is hard to organize multitask learning, e.g., how to batchify your data and how to make your training stable.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n"])</script><script>self.__next_f.push([1,"43:[\"$\",\"article\",\"chinese-clip\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/chinese-clip\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"December 23, 2022\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"CLIP1 is a phenomenal playmaker in vision and multimodal representation learning. It plays not only as a foundation model but also a bridge between vision and language. It has triggered a series of research in different fields, especially text-to-image generation. However, we find that there is a necessity for a language-specific CLIP for applications, especially cross-modal retrieval, and there is no opensourced Chinese CLIP with good performance. We therefore launched this project to promote the Chinese multimodal representation learning.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n"])</script><script>self.__next_f.push([1,"44:[\"$\",\"article\",\"7680-dim-embeddings\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/7680-dim-embeddings\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"December 4, 2022\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"7680-Dimensional Embeddings: More Dimensions, Better Retrieval\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"Why we trained embedding models with 7680 dimensions and what we learned about the relationship between dimensionality and retrieval quality.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zach Kelling\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[[\"$\",\"span\",\"Research\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Research\"}],[\"$\",\"span\",\"Embeddings\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Embeddings\"}],[\"$\",\"span\",\"Retrieval\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Retrieval\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n"])</script><script>self.__next_f.push([1,"45:[\"$\",\"article\",\"embedding-spaces-7680-dimensions\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/embedding-spaces-7680-dimensions\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"December 4, 2022\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Embedding Spaces at 7680 Dimensions\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"Exploring high-dimensional embedding spaces for semantic search and retrieval.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zach Kelling\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[[\"$\",\"span\",\"Research\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Research\"}],[\"$\",\"span\",\"Embeddings\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Embeddings\"}],[\"$\",\"span\",\"Retrieval\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Retrieval\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n"])</script><script>self.__next_f.push([1,"46:[\"$\",\"article\",\"ofa\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/ofa\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"November 13, 2022\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"OFA: Towards Building a One-For-All Model\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"2022 is a year of generalist models! With the bloom of multimodal pretraining, especially the unified model, we have witnessed the opportunity to building a generalist model that is capable of processing tasks of different modalities or multi-modalities! Thus, we propose OFA1, namely One-For-All, a unified multimodal pretrained model that unifies understanding and generation tasks concerning modalities into a single framework, and we pretrain OFA with the instruction-based multitask-pretraining that endows it with multiple capabilities.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zen LM Team\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n"])</script><script>self.__next_f.push([1,"47:[\"$\",\"article\",\"grpo\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/grpo\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"September 18, 2022\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"GRPO: Group Relative Policy Optimization\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"Introducing GRPO, a new approach to reinforcement learning from human feedback that improves sample efficiency and alignment stability.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zach Kelling\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[[\"$\",\"span\",\"Research\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Research\"}],[\"$\",\"span\",\"Alignment\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Alignment\"}],[\"$\",\"span\",\"Training\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Training\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n"])</script><script>self.__next_f.push([1,"48:[\"$\",\"article\",\"grpo-group-relative-policy-optimization\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/grpo-group-relative-policy-optimization\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"September 17, 2022\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"GRPO: Group Relative Policy Optimization\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"A companion post to our GRPO paper, explaining group relative policy optimization for language model alignment.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zach Kelling\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[[\"$\",\"span\",\"Research\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Research\"}],[\"$\",\"span\",\"RLHF\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"RLHF\"}],[\"$\",\"span\",\"GRPO\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"GRPO\"}],[\"$\",\"span\",\"Alignment\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Alignment\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n"])</script><script>self.__next_f.push([1,"49:[\"$\",\"article\",\"federated-learning-without-compromise\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/federated-learning-without-compromise\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"May 29, 2022\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Federated Learning Without Compromise\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"Privacy-preserving machine learning that maintains model quality through novel aggregation protocols.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zach Kelling\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[[\"$\",\"span\",\"Research\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Research\"}],[\"$\",\"span\",\"Federated Learning\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Federated Learning\"}],[\"$\",\"span\",\"Privacy\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Privacy\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n"])</script><script>self.__next_f.push([1,"4a:[\"$\",\"article\",\"federated-learning\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/federated-learning\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"May 8, 2022\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Federated Learning for Open AI\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"How federated learning enables collaborative model training while preserving data privacy.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zach Kelling\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[[\"$\",\"span\",\"Research\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Research\"}],[\"$\",\"span\",\"Training\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Training\"}],[\"$\",\"span\",\"Privacy\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Privacy\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n"])</script><script>self.__next_f.push([1,"4b:[\"$\",\"article\",\"experience-ledgers\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/experience-ledgers\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"February 13, 2022\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Experience Ledgers: Persistent Memory for AI Agents\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"Introducing experience ledgers, a framework for giving AI agents persistent, verifiable memory.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zach Kelling\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[[\"$\",\"span\",\"Research\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Research\"}],[\"$\",\"span\",\"Agents\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Agents\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n"])</script><script>self.__next_f.push([1,"4c:[\"$\",\"article\",\"case-for-decentralized-science\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/case-for-decentralized-science\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"November 7, 2021\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"The Case for Decentralized Science\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"A manifesto for decentralized science (DeSci) and its application to AI research.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zach Kelling\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[[\"$\",\"span\",\"DeSci\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"DeSci\"}],[\"$\",\"span\",\"Vision\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Vision\"}],[\"$\",\"span\",\"Governance\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Governance\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n"])</script><script>self.__next_f.push([1,"4d:[\"$\",\"article\",\"case-for-desci\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/case-for-desci\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"November 7, 2021\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"The Case for Decentralized Science\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"Why scientific research needs decentralization, and how blockchain can help.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zach Kelling\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[[\"$\",\"span\",\"DeSci\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"DeSci\"}],[\"$\",\"span\",\"Vision\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Vision\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n4e:[\"$\",\"article\",\"training-llms-collective-intelligence\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/training-llms-collective-intelligence\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"July 21, 2021\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Training LLMs on Collective Intelligence\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"How we're approaching training data curation to capture humanity's collective intelligence.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zach Kelling\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[[\"$\",\"span\",\"Research\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Research\"}],[\"$\",\"span\",\"Training\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Training\"}]]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"border-b border-fd-border\"}]]}]\n4f:[\"$\",\"article\",\"introducing-zen\",{\"children\":[[\"$\",\"$L5\",null,{\"href\":\"/blog/introducing-zen\",\"className\":\"block group py-8\",\"children\":[[\"$\",\"time\",null,{\"className\":\"text-xs font-mono text-fd-muted-foreground uppercase tracking-wider\",\"children\":\"March 14, 2021\"}],[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mt-1 mb-2 group-hover:text-fd-primary transition-colors\",\"children\":\"Introducing Zen: Open AI for the Open Web\"}],[\"$\",\"p\",null,{\"className\":\"text-fd-muted-foreground text-sm leading-relaxed mb-3\",\"children\":\"We're launching Zen, an open research initiative to build AI that serves everyone.\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-3\",\"children\":[[\"$\",\"span\",null,{\"className\":\"text-xs text-fd-muted-foreground\",\"children\":\"Zach Kelling\"}],[\"$\",\"div\",null,{\"className\":\"flex gap-1.5 ml-auto\",\"children\":[[\"$\",\"span\",\"Announcement\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Announcement\"}],[\"$\",\"span\",\"Vision\",{\"className\":\"text-[10px] font-mono uppercase tracking-wider bg-fd-muted text-fd-muted-foreground px-1.5 py-0.5 rounded\",\"children\":\"Vision\"}]]}]]}]]}],false]}]\n50:[\"$\",\"$L53\",null,{\"children\":[\"$\",\"$54\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@55\"}]}]\n51:[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$L56\",null,{\"children\":\"$L57\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$L58\",null,{\"children\":[\"$\",\"$54\",null,{\"name\":\"Next.Metadata\",\"children\":\"$L59\"}]}]}],null]}]\n"])</script><script>self.__next_f.push([1,"57:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"2\",{\"name\":\"theme-color\",\"media\":\"(prefers-color-scheme: dark)\",\"content\":\"#0A0A0A\"}],[\"$\",\"meta\",\"3\",{\"name\":\"theme-color\",\"media\":\"(prefers-color-scheme: light)\",\"content\":\"#fff\"}]]\n"])</script><script>self.__next_f.push([1,"55:null\n59:[[\"$\",\"title\",\"0\",{\"children\":\"Blog — Zen LM | Zen LM\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Research, releases, and perspectives from the Zen LM team.\"}],[\"$\",\"meta\",\"2\",{\"property\":\"og:title\",\"content\":\"Zen LM - Open Foundation Models\"}],[\"$\",\"meta\",\"3\",{\"property\":\"og:description\",\"content\":\"Zen AI model family by Hanzo AI. 14 frontier models from 4B to 1T+ parameters for code, reasoning, vision, and multimodal tasks.\"}],[\"$\",\"meta\",\"4\",{\"property\":\"og:url\",\"content\":\"https://zenlm.org\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:site_name\",\"content\":\"Zen LM\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"7\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"8\",{\"name\":\"twitter:site\",\"content\":\"@zenlmorg\"}],[\"$\",\"meta\",\"9\",{\"name\":\"twitter:creator\",\"content\":\"@zenlmorg\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:title\",\"content\":\"Zen LM - Open Foundation Models\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:description\",\"content\":\"Zen AI model family by Hanzo AI. 14 frontier models from 4B to 1T+ parameters for code, reasoning, vision, and multimodal tasks.\"}]]\n"])</script></body></html>