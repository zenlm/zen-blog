<!doctype html><html lang=en dir=auto><head><meta name=generator content="Hugo 0.106.0"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Zen LM</title><meta name=keywords content="Zen LM,Zen MoDE,open weights,large language models,machine learning,AI"><meta name=description content="Open frontier models from Hanzo AI and Zoo Labs Foundation."><meta name=author content="Zen LM Team"><link rel=canonical href=https://zenlm.org/><link crossorigin=anonymous href=/assets/css/stylesheet.6c0865a4bc8dbcbd27d78777eb33b404568f7fbf3185cca7b978e5275a4dd049.css integrity="sha256-bAhlpLyNvL0n14d36zO0BFaPf78xhcynuXjlJ1pN0Ek=" rel="preload stylesheet" as=style><link rel=icon href=https://zenlm.org/favicon.png><link rel=apple-touch-icon href=https://zenlm.org/favicon.png><link rel=manifest href=https://zenlm.org/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate type=application/json href=https://zenlm.org/index.json><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.df2a5734071a3a99040f5e88e6d16d78358fbdef9a5e7389874ac5f2aa2ca86f.js integrity="sha256-3ypXNAcaOpkED16I5tFteDWPve+aXnOJh0rF8qosqG8="></script><meta property="og:title" content="Zen LM"><meta property="og:description" content="Open frontier models from Hanzo AI and Zoo Labs Foundation."><meta property="og:type" content="website"><meta property="og:url" content="https://zenlm.org/"><meta property="og:image" content="https://zenlm.org/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="Zen LM"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://zenlm.org/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Zen LM"><meta name=twitter:description content="Open frontier models from Hanzo AI and Zoo Labs Foundation."><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"Zen LM","url":"https://zenlm.org/","description":"Zen LM — Frontier open models from Hanzo AI and Zoo Labs Foundation","thumbnailUrl":"https://zenlm.org/favicon.png","sameAs":[]}</script></head><body class=list id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="Zen LM (Alt + H)"><img src=https://zenlm.org/img/logo.png alt aria-label=logo height=30></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://zeekay.blog title=zeekay><span>zeekay</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://hanzo.blog title=hanzo.blog><span>hanzo.blog</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://hanzo.ai/chat title="Try Zen Chat"><span>Try Zen Chat</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://blog.zoo.ngo title="zoo blog"><span>zoo blog</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container style=min-height:100vh;display:flex;justify-content:center;background-color:#000><img class=hero-background style=opacity:0 onload="this.style.opacity=1" src=https://cdn.jsdelivr.net/gh/hanzoai/zen-blog@main/static/img/background.webp width=100%><div class=hero-gradient></div><div class=hero-blur></div><div class=mouse-hint><div class=mouse-point></div></div><style>body{-ms-overflow-style:none;scrollbar-width:none}body::-webkit-scrollbar{display:none}.mouse-hint{position:absolute;height:36px;width:24px;border:1px solid #fff;border-radius:12px;bottom:20%;left:50% - calc(12px);opacity:1;transition:opacity .3s;animation:1s ease-out 0s 1 slideBelow}.mouse-hint .mouse-point{height:4px;width:4px;background-color:#fff;position:absolute;left:50%;bottom:40%;border-radius:4px;transform-origin:50% 100%;transform:translate(-50%);animation:2.2s ease-in-out infinite jump;will-change:transform}@keyframes slideBelow{0%{transform:translateY(50px);opacity:0}100%{transform:translateX(0);opacity:1}}@keyframes jump{0%,20%,60%,to{transform:translate(-50%)translateY(0);height:4px;animation-timing-function:ease-in}40%,80%{transform:translate(-50%)translateY(8px);height:8px;animation-timing-function:ease-out}}</style><div class="hero text-light text-fade-in"><div class=hero-header><h1>Zen LM</h1></div><div class=hero-content>Open frontier models from Hanzo AI and Zoo Labs Foundation.</div><div class=hero-footer><div class=social-icons></div></div></div></div><main class="main home"><article class=post-entry><header class=entry-header><h2>Training Gym: A Platform for Open Model Development</h2></header><div class=entry-content><p>Training large language models requires more than algorithms. It requires infrastructure: distributed training frameworks, data pipelines, experiment tracking, and evaluation harnesses. Today we open source Training Gym, our complete platform for model development.
Why Training Gym? Open AI development faces an infrastructure gap. Publishing model weights is valuable, but it’s not enough. Researchers need:
Reproducible training pipelines Scalable distributed training Standardized evaluation Experiment management Data processing tools Training Gym provides all of this in an integrated, open source package....</p></div><footer class=entry-footer><span title='2023-09-11 10:00:00 -0800 -0800'>September 11, 2023</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;622 words&nbsp;·&nbsp;Zach Kelling</footer><a class=entry-link aria-label="post link to Training Gym: A Platform for Open Model Development" href=https://zenlm.org/blog/training-gym/></a></article><article class=post-entry><header class=entry-header><h2>Proof of AI: Verifiable Machine Learning on Chain</h2></header><div class=entry-content><p>When an AI system makes a prediction, how do you know it actually ran the model it claims? In centralized systems, you trust the operator. Decentralized AI needs cryptographic proof.
Today we introduce Proof of AI (PoAI), a framework for verifiable machine learning inference.
The Trust Problem Consider a decentralized AI service:
User submits input and payment Compute provider runs inference Provider returns output User receives result What prevents the provider from:...</p></div><footer class=entry-footer><span title='2023-06-26 09:00:00 -0800 -0800'>June 26, 2023</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;614 words&nbsp;·&nbsp;Zach Kelling</footer><a class=entry-link aria-label="post link to Proof of AI: Verifiable Machine Learning on Chain" href=https://zenlm.org/blog/proof-of-ai/></a></article><article class=post-entry><header class=entry-header><h2>Zen Reranker: Two-Stage Retrieval Done Right</h2></header><div class=entry-content><p>Embedding-based retrieval is fast but imprecise. Cross-encoder reranking is precise but slow. The combination unlocks the best of both. Today we release the Zen Reranker, purpose-built for two-stage retrieval.
Two-Stage Retrieval Modern retrieval pipelines typically operate in two stages:
Query -> [Embedding Retrieval] -> Top-K Candidates -> [Reranker] -> Final Results (fast, approximate) (slow, precise) Stage 1: Bi-encoder embeddings enable fast approximate search over millions of documents. Retrieve top-100 to top-1000 candidates....</p></div><footer class=entry-footer><span title='2023-03-13 09:00:00 -0800 -0800'>March 13, 2023</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;573 words&nbsp;·&nbsp;Zach Kelling</footer><a class=entry-link aria-label="post link to Zen Reranker: Two-Stage Retrieval Done Right" href=https://zenlm.org/blog/zen-reranker/></a></article><article class=post-entry><header class=entry-header><h2>OFASys: Enabling Multitask Learning with One Line of Code!</h2></header><div class=entry-content><p>Intro Generalist Models are hot! We all see an opportunity towards a real generalist model by multimodal multitask learning. We previously release an opensourced unified multimodal pretrained model OFA for this goal. However, we actually met a lot of difficulties in our implementation. For example, it is hard to set up multiple tasks concerning multiple modalities, and it is hard to organize multitask learning, e.g., how to batchify your data and how to make your training stable....</p></div><footer class=entry-footer><span title='2022-12-28 18:01:21 +0800 +0800'>December 28, 2022</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;1108 words&nbsp;·&nbsp;Zen LM Team</footer><a class=entry-link aria-label="post link to OFASys: Enabling Multitask Learning with One Line of Code! " href=https://zenlm.org/blog/ofasys/></a></article><article class=post-entry><header class=entry-header><h2>Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese</h2></header><div class=entry-content><p>CLIP1 is a phenomenal playmaker in vision and multimodal representation learning. It plays not only as a foundation model but also a bridge between vision and language. It has triggered a series of research in different fields, especially text-to-image generation. However, we find that there is a necessity for a language-specific CLIP for applications, especially cross-modal retrieval, and there is no opensourced Chinese CLIP with good performance. We therefore launched this project to promote the Chinese multimodal representation learning....</p></div><footer class=entry-footer><span title='2022-12-24 14:54:19 +0800 +0800'>December 24, 2022</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;850 words&nbsp;·&nbsp;Zen LM Team</footer><a class=entry-link aria-label="post link to Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese" href=https://zenlm.org/blog/chinese-clip/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://zenlm.org/page/11/>«&nbsp;Prev&nbsp;</a>
<a class=next href=https://zenlm.org/page/13/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2026 <a href=https://zenlm.org/>Zen LM</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script></body></html>